%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                      %
%     File: Thesis_Conclusions.tex                                     %
%     Tex Master: Thesis.tex                                           %
%                                                                      %
%     Author: Andre C. Marta                                           %
%     Last modified : 21 Jan 2011                                      %
%                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusions}
\label{chapter:conclusions}

Insert your chapter material here...


% ----------------------------------------------------------------------
\section{Achievements}
\label{section:achievements}

The major achievements of the present work...


% ----------------------------------------------------------------------
\section{Future Work}
\label{section:future}

% other algorithms in the first and last phase
Much effort was put developing and testing co-association matrix building strategies.
The schemes presented here provide a solid framework to work with large data sets in this middle step of EAC.
As such, interesting directions for this work to take are testing how state of the art algorithms designed for Big Data would complement EAC in the first and last steps.

The programming model used for the GPU was CUDA, used through a Python library called Numba.
This library offers an interface to access most of CUDA's capabilities, but not all.
One of those capabilities is Dynamic Parallelism.
This offers the ability of having a host kernel call on other host kernel without intervention from the host.
This translates in the possibility of moving the control logic in the Bor≈Øvka variant (and also its the Connected Components Labeling variant) to the device, effectevely removing the memory transfer of values related with the control logic.

Adaptation of the present implementation to OpenCL. This brings major benefits in respect to portability since OpenCL supports most devices. Moreover, OpenCL's performance is catching in on that of CUDA's and since it's programming model was based on CUDA, it should be straightforward for developers to make the switch.

Application of EAC to the MapReduce framework will further expand the possibilities of application of EAC.

Study the integration of other clustering algorithms within the the EAC toolchain.

A good follow-up of the present work is to study the relationship between several metrics (e.g. sparsity, accuracy, maximum number of co-associations) and the complexity of the dataset. Some metrics for describing the complexity of the dataset exist (Tin Kam Ho) and it would be interesting to profile several datasets of different complexities and structures and search for the former relationship.
On a performance perspective it could prove useful to deduce better rules to set the maximum number of associations in the sparse matrix.
On a accuracy perspective it would be interesting to see if there are types of datasets that simply are not a good fit for EAC while other are. It would also be interesting to relate complexity with the threshold cut-off.

% cross with WEAC
The WEAC algorithm is focused on improving accuracy.
The underlying concept is to measure the quality of the partitions in the ensemble and allow the better ones to be more influential in the co-association matrix.
The concept of measuring the quality of the partitions may prove useful for further decreasing the memory complexity with the EAC CSR scheme without compromising too much accuracy.
The basic idea is to choose a $max\_assocs$ value that will likely be less than the number of associations many patterns will have, which will result in some associations being discarded.
The associations that will be kept are the ones from the first partitions that were processed.
With this in mind, one could order the partitions by quality and start the processing from those with better quality.

% More efficient external sorting
% It is believed that a significant speedup may be obtained by using GPUs for external sorting in the argsort step of the cluster recovery of EAC.
% Even without using GPU's further speedup may be possible simply by using more memory. After the co-association matrix is stored it can be deleted from main memory. This means that almostthe entire memory is available for the computation of the argsort array. Currently the PyTables CSI algorithm uses a insignificant amount of memory.

\cleardoublepage

