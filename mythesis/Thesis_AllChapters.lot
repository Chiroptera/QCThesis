\select@language {english}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Memory used for different matrix types for the generic case and a real example of 100000 samples. The relative reduction (R.R.) refers to the memory reduction relative to the type of matrix above, the absolute reduction (A.R.) refers to the reduction relative to the full complete matrix.\relax }}{43}{table.caption.37}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {6.1}{\ignorespaces \textbf {Alpha} machine specifications.\relax }}{52}{table.6.1}
\contentsline {table}{\numberline {6.2}{\ignorespaces \textbf {Bravo} machine specifications.\relax }}{52}{table.6.2}
\contentsline {table}{\numberline {6.3}{\ignorespaces \textbf {Charlie} machine specifications.\relax }}{53}{table.6.3}
\contentsline {table}{\numberline {6.4}{\ignorespaces Maximum theoretical speed-up for the labeling and update phases of K-Means based on experimental data. The data sets used range from $1000$ to $500 \tmspace +\medmuskip {.2222em} 000$ patterns, from $2$ to $1000$ dimensions and from $2$ to $2048$ centroids.\relax }}{54}{table.caption.48}
\contentsline {table}{\numberline {6.5}{\ignorespaces Effective bandwidth and computational throughput of labeling phase computed from results taken from running K-Means over data sets whose complexity ranged from $100$ to $10 \tmspace +\medmuskip {.2222em} 000 \tmspace +\medmuskip {.2222em} 000$ patterns, from $2$ to $1000$ dimensions and from $2$ to $2048$ centroids.\relax }}{56}{table.caption.51}
\contentsline {table}{\numberline {6.6}{\ignorespaces Average speed-up of the GPU MST algorithm for different data sets, sorted by number of edges.\relax }}{57}{table.6.6}
\contentsline {table}{\numberline {6.7}{\ignorespaces Cross-correlation between several characteristics of the graphs and the average speed-up.\relax }}{57}{table.caption.52}
\contentsline {table}{\numberline {6.8}{\ignorespaces Execution times for computing the condensed co-association matrix using different matrix strategies.\relax }}{59}{table.caption.53}
\contentsline {table}{\numberline {6.9}{\ignorespaces Difference between accuracies from the two implementations of EAC, using the same ensemble. Accuracy was measured using the H-index.\relax }}{61}{table.caption.55}
\contentsline {table}{\numberline {6.10}{\ignorespaces Speed-ups obtained in the different phases of EAC, with independent production of ensembles.\relax }}{61}{table.caption.56}
\contentsline {table}{\numberline {6.11}{\ignorespaces Different rules for computing $K_{min}$ and $K_{max}$. $n$ is the number of patterns in the data set and $sk$ is the number of samples per cluster.\relax }}{61}{table.caption.57}
\contentsline {table}{\numberline {6.12}{\ignorespaces Timing results for the different algorithms in the different tests. Fitness time refers to the time that took to compute the DB index of each solution of classical K-Means. All time values are the average over 20 rounds and are displayed in seconds.\relax }}{66}{table.caption.67}
\contentsline {table}{\numberline {6.13}{\ignorespaces All values displayed are the average over 20 rounds, except for the Overall best which shows the best result in any round. The values represent the Davies-Bouldin fitness index (low is better).\relax }}{67}{table.caption.69}
\contentsline {table}{\numberline {6.14}{\ignorespaces The values represent generations.\relax }}{67}{table.caption.75}
\contentsline {table}{\numberline {6.15}{\ignorespaces Time of computation of \citet {Horn2001b} algorithm for a mixture of 4 Gaussians of different cardinality and dimensionality.\relax }}{70}{table.caption.76}
\addvspace {10\p@ }
\addvspace {10\p@ }
