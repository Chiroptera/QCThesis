%!TEX root = thesis.tex
% \rscpath = contains the path to the resources folder

\chapter{Results and Discussion}
\label{chapter:results}

The present chapter is dedicated to the results relevant to the work produced and their associated interpretation and subsequent discussion.
% Different results are presented, not always directly related to the EAC method.
% Still, the set-up of the tests and experiments was always done with EAC in mind and how one could improve it with different perspectives.

%

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%
%  					Experimental environment
%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{Experimental environment}
\label{sec:system configs}

All experiments were carried out in one (or more) of three distinct machines what will be referred to as \textbf{Alpha}, \textbf{Bravo} and \textbf{Charlie}.
Their CPU and GPU hardware configurations are described in Tables \ref{tab:alpha}, \ref{tab:bravo} and \ref{tab:charlie}, respectively.
Besides, Charlie has a \emph{Seagate ST2000DM001} 7200 RPM spinning disk and a \emph{Samsung 840 EVO} Solid State Drive, informations relevant for the third phase of EAC.% with sequential read/write speeds of up to 540/410 MB/s.

Software wise, all machines are running Linux based operating systems.
Alpha and Bravo are using the Ubuntu 14.04 and 12.04, respectively, with a graphical interface.
Whether the machine is running a graphical interface or not is important because, in the case that it only has one GPU (as is the case with all the machines here presented) the available memory for computation is less than total and there is a limit to how long a CUDA kernel can be executed.
Charlie is running Fedora 21 without a user interface.

% SAMSUNG HM641JI
% http://www.farnell.com/datasheets/841934.pdf
% 5400 RPM
% 209.3IOPS


% Mighty4 SSD : http://hexus.net/tech/reviews/storage/58229-samsung-ssd-840-evo-120gb/
\input{\rscpath/results/hardware/laptop}
\input{\rscpath/results/hardware/mariana}
\input{\rscpath/results/hardware/mighty4}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%
%  					KMEANS RESULTS
%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{Parallel K-Means}
\label{sec:parallel kmeans}

To test the time efficiency 

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%
%  					GPU MST RESULTS
%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{GPU MST}
\label{sec:gpu mst}

% From Sousa dissertation - the graphs he used and where he took them from
% The graph collection supplied provided by 9th DIMACS Implementation Challeng \footnote{http://www.dis.uniroma1.it/~challenge9/} include sparse graphs
% that depict the United States road network. These graphs are seen frequently in the recent literature. Furthermore, the OpenStreetMapâ€™s \footnote{http://www.openstreetmap.org/} Portuguese road-network, provided by Geofabrik \footnote{http://download.geofabrik.de/europe/portugal.html} is included.

To test the performance of the GPU MST algorithm, several graphs were used.
Most of the graphs are United Stated road network graphs taken from the 9th DIMACS Implementation Challenge \footnote{http://www.dis.uniroma1.it/~challenge9/}.
Furthermore, graphs taken from co-association matrix of the second step of EAC were used.
This is important because, as will become clear, the graphs within the EAC paradigm have different characteristics.
All the tests were performed on machine Bravo.
The average speed-up obtained by using the GPU version over the sequential one is presented in Table \ref{tab:mst speedup}.
Characteristics of the different graphs are also shown so as to illustrate what variables influence the speed-up obtained.
It should be noted that a speed-up below 0 is actually a slow-down and its absolute value corresponds to the speed-up of the sequential version relative its GPU counterpart.

\input{\rscpath/results/mst_gpu/mst_speedup_table}

% note on the memory usage
Although all the graphs presented in Table \ref{tab:mst speedup} occupy significantly less memory than the available in the used machine, the processing of bigger graphs is not possible.
The reason for this is that between in each iteration two graphs have to be held in memory: the initial and the contracted.
Moreover, the space occupied by the contracted graph will depend on the characteristics of the graph.

% speed-ups are possible, contrast with original paper
The results clearly show that it is possible to obtain speed-ups for computing a MST.
This speed-up seems to increase with the size of the graph, with the notable exception of the graph from the EAC context.
A note should be made here to bring to attention the contrast between these results and those presented by \citet{Sousa2015}.
The speed-ups observed here are less than those reported by \citet{Sousa2015}.
This is believed to be related with the technology stack used and this topic has been discussed in more depth in chapter \ref{chapter:methodology}.
To understand how different parameters affect the speed-up of the algorithm, Table \ref{tab:mst corr} presents the correlation matrix of these variables.

\input{\rscpath/results/mst_gpu/corr_table}

%TODO run MST algorithm with more graphs of different characteristics
% what are the variables that influence speed-up
The row corresponding to the average speed-up is of special relevance.
One can observe that the parameters most correlated with the speed-up are the number of vertices and the number of edges per vertex (EPV).
The correlation matrix suggests that as one increases the number of vertices, the speed-up will also increase.
In fact, if no graphs from the EAC context were present in the results, the same would apply to the number of edges, since the EPV would very similar.
The reason for this is that speed-ups from parallelism are more salient when applied to big data sets, so that the speed-up of the computation itself outweighs the overhead associated with communication between host and device.
The EPV is the other parameter that shows has highest (inverse) correlation with the speed-up.
This suggests that the relationship between the number of edges and the number of vertices in the graph actually plays a big role in deciding if there will be a speed-up.

% why, programatically, there is slow-down
The underlying reason for the poor performance of graphs with high EPV ratio is believed to be that, since the parallel computation is anchored to vertices, the workload per vertex is higher than if the graph had a low ratio.
Accordingly, the workload per vertex is higher from the beginning and can increase significantly as the algorithm progresses.
Besides, the workload can become highly unbalanced with some threads having to process hundreds of thousands of edges while others only a few thousands, which translates threads not doing any computation when waiting for the others.

% connection with original paper
The original source \cite{Sousa2015} of the algorithm doesn't address graphs with a EPV as high as presented here.
In that sense, the results here complement those of the original source and suggest an increase in EPC translates in the decrease in speed-up.
Still, more in-depth studies should be made.

% connection with EAC
Within EAC paradigm, this algorithm is of little contribution.
The most obvious reason is that the EAC method would actually be slower if this algorithm was used.
Still, even considering that speed-ups like those reported in literature were possible for EAC co-association graphs, the algorithm requires a double redundancy of edges (which effectively doubles the necessary memory to hold a graph) and at any iteration the device must be able to hold two distinct graphs (the initial and the contracted).
For these reasons, the device memory (which typically is smaller than the host memory) would confine the EAC method to small input data sets. 

% The underlying reason for this is believed to be that the number of edges to node ratio of these graphs is low compared to that typically seen in co-associations matrix, even when using a prototype subset of the original matrix.
% The parallel version of the final step of EAC showed a slowdown relative to its sequential counterpart.
% This slowdown is related with the performance of the MST algorithm.
% The implementation of the algorithm was tested in some of the same graphs as those reported in \cite{Sousa2015} and revealed a speedup.
% However, when using the this MST solver on the target graphs (the co-association matrices) not only there was no speedup, but a significantly slowdown was observed, reaching up to nine times slower.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%
%  					SPARSE BUILDNG RESUTLS 
%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{Building the co-association matrix with different sparse formats}
\label{sec:spare building}

The purpose of this section is presenting brief results concerning the time that took to build a co-association matrix for different types of matrices.
The ensemble from which the co-association matrices are built has 100 partitions and was produced from a mixture of 6 Gaussians with 5000 patterns.
Only the upper triangular (condensed) matrix was built.
The types of matrices under test are: fully allocated (a "normal" matrix), LIL, DOK, CSR, an optimized fully allocated and the proposed EAC CSR.
The SciPy's LIL, DOK and CSR implementations were used.

The time that took to update the first partition and the total time were recorded for the different types of matrix.
The results can are presented in Table \ref{tab:coassoc build sparse} and also in Fig. \ref{fig:coassoc build sparse}.
For the CSR format only the first partition was updated, since it took a very long time to update just the first partition.
A rough estimate for the time it would take to update the whole matrix is around 15h, 100 times the time it took to update the first partition.
Observing the other timings, and for the exception of the EAC CSR matrix, this estimate should not be too far off.
The reason that the first partition update of the EAC CSR matrix was so much faster is that it only requires a simple copy of the partition to the data structure.

It is clear from the results that the optimized versions are much faster than any of the others.
These results focus on providing a justification for the design and implementation of a novel method of building the co-association matrix: a fully allocated matrix consumes too much memory but available sparse implementations are too slow.
For this purpose a small data set as the one used suffices to demonstrate this point.
The difference between the two optimized versions will become clearer on future sections, where a more thorough study covering a wider spectrum of data sets is presented. 


\begin{table}[h]
\centering
 \caption{Times for computing the condensed co-association matrix using different matrix strategies.}
\begin{tabular}{crr}
\toprule
  Matrix type (condensed) &  Time 1st partition [s] &  Time ensemble [s] \\
\midrule
 Optimzed Fully allocated &                 0.00170 &              0.139 \\
                  EAC CSR &                 0.00481 &              1.470 \\
          Fully allocated &                 0.85500 &             96.000 \\
                      LIL &                 5.39000 &            614.000 \\
                      DOK &                12.50000 &           1535.000 \\
                      CSR &               548.00000 &                - \\
\bottomrule
\end{tabular}
\label{tab:coassoc build sparse}
\end{table}



\begin{figure}[hbtp]
\centering
\includegraphics[scale=0.5]{results/eac_sparse_build/coassoc_build}
\caption{sd}
\label{fig:coassoc build sparse}
\end{figure}


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%
%  					EAC VALIDATION RESUTLS
%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{EAC Validation}
\label{sec:eac validation}

The present section aims to provide results showing that the proposed methods do not alter the overall quality of the results.
With this in mind, the results of original version of EAC, implemented in Matlab, are compared with those of the proposed solution.
Several small data sets, chosen from the data sets used by \citet{Lourenco2010}, were processed by the two versions of EAC.
All these data sets were taken from the UCI Machine Learning Repository. %TODO add ref
Furthermore, since the generation of the ensemble is probabilistic and can change the results between runs, the proposed version is processed with the ensembles created by the original version as well.
This guarantees that the combination and recovery phases of EAC, which are deterministic when using SL, are equivalent to the original.

Table \ref{tab:validation error acc} presents the difference between the accuracies of the two versions.
Analyzing these results, it is apparent that the difference is minimal.
It should be noted at this point that the original implementation always maps the dissimilarities of the co-association matrix to the range $\left [ 0 , 1 \right ]$.
This forces the co-association matrix to have a floating point data type.
However, since the number of partitions used is usually less than 255, the proposed version uses unsigned integers of 1 byte to reduce the used memory considerably.
The differences in accuracy are thought to come from rounding differences of the two frameworks used and from this difference in data type.



% \begin{table}[h]
% \centering
% \caption{Times for computing the condensed co-association matrix using different matrix strategies.}

% \label{tab:coassoc build sparse}
% \end{table}


\begin{table}[h]
\centering
\caption{Difference between accuracies from the two implementations of EAC, using the same ensemble. Accuracy was measured using the H-index.}

\begin{tabular}{lll}
\toprule
{} &      accuracy & lifetime accuracy \\
\midrule
breast\_cancer &  4.948755e-06 &      2.825769e-06 \\
ionosphere    &  1.652422e-06 &      1.452991e-06 \\
iris          &  3.333333e-06 &      3.333333e-06 \\
isolet        &  1.038861e-07 &      4.084904e-07 \\
optdigits     &  3.795449e-06 &      1.480513e-06 \\
pima          &  3.333333e-06 &      3.333333e-06 \\
pima\_norm     &  4.166667e-07 &      4.166667e-07 \\
wine\_norm     &  1.123596e-07 &      1.910112e-06 \\
\bottomrule
\end{tabular}

\label{tab:validation error acc}
\end{table}




% \begin{table}[h]
% \centering
% \caption{Speed-ups obtained in the combination and recovery phases of EAC, using the ensemble generated from the original EAC implementation.}

% \begin{tabular}{lll}
% \toprule
% Data set & Combination & Recovery \\
% \midrule
% breast\_cancer &   7.713564 &        15.22334 \\
% ionosphere    &   9.678288 &        20.12336 \\
% iris          &   14.25549 &         28.4751 \\
% isolet        &   5.500147 &        174.4283 \\
% optdigits     &   9.783604 &        53.21466 \\
% pima          &   85.21744 &        8.406726 \\
% pima\_norm     &   127.2274 &        12.89474 \\
% wine\_norm     &     8.0178 &        12.98206 \\
% \bottomrule
% \end{tabular}

% \label{tab:validation speedup comb rec}
% \end{table}

Table \ref{tab:validation speedup all} presents the speed-up of the proposed version over the original one.
It is clear that speed-up is obtained in all phases of EAC, often by an order of magnitude.
This result, combined with the demonstration that the differences in accuracy are negligible, show that the proposed algorithm performs well in small data sets.

\begin{table}[h]
\centering
\caption{Speed-ups obtained in the different phases of EAC, with independent production of ensembles.}

\begin{tabular}{lccllll}
\toprule
Data set &  No. patterns &  No. features & No. classes & Production & Combination & Recovery \\
\midrule
breast\_cancer &           683 &            10 &           2 &      50.43974 &   7.544247 &        15.83316 \\
ionosphere    &           351 &            34 &           2 &      21.86286 &   11.30883 &        19.97219 \\
iris          &           150 &             4 &           3 &      19.76525 &   14.49562 &        28.50479 \\
isolet        &          7797 &           617 &          26 &      7.010007 &   6.183124 &        206.2837 \\
optdigits     &          3823 &            64 &          10 &      17.30209 &    10.2096 &        53.02636 \\
pima          &           768 &             8 &           2 &      50.65624 &   141.4828 &        13.93502 \\
pima\_norm     &           768 &             8 &           2 &      54.25415 &   132.8632 &          14.355 \\
wine\_norm     &           178 &             4 &           3 &      22.92404 &   14.56994 &        25.27709 \\
\bottomrule
\end{tabular}

\label{tab:validation speedup all}
\end{table}




% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%
%  					EAC RESUTLS
%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{EAC}
\label{sec:eac results}

This section will present thorough results concerning several characteristics related to the EAC method.










% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%
%  					QUANTUM RESULTS
%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

% \input{results_quantum}
