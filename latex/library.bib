Automatically generated by Mendeley Desktop 1.13.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Vacaliuc2011,
abstract = {Design of data structures for high performance computing (HPC) is one of the principal challenges facing researchers looking to utilize heterogeneous computing machinery. Heterogeneous systems derive cost, power, and speed efficiency by being composed of the appropriate hardware for the task. Yet, each type of processor requires a specific organization of the application state in order to achieve peak performance. Discovering this and refactoring the code can be a challenging and time-consuming task for the researcher, as the data structures and the computational model must be co-designed. We present a methodology that uses Python as the environment for which to explore tradeoffs in both the data structure design as well as the code executing on the computation accelerator. Our method enables multi-dimensional arrays to be used effectively in any target environment. We have chosen to focus on OpenMP and CUDA environments, thus exploring the development of optimized kernels for the two most common classes of computing hardware available today: multi-core CPU and GPU. Python's large palette of file and network access routines, its associative indexing syntax and support for common HPC environments makes it relevant for diverse hardware ranging from laptops through computing clusters to the highest performance supercomputers. Our work enables researchers to accelerate the development of their codes on the computing hardware of their choice.},
author = {Vacaliuc, Bogdan and Patlolla, Dilip R. and D'Azevedo, Ed and Davidson, Greg G. and Munro, John K. and Evans, Thomas M. and Joubert, Wayne and Bell, Zane W.},
doi = {10.1109/SAAHPC.2011.26},
file = {:home/chiroptera/Dropbox/mendeley/Vacaliuc et al. - 2011 - Python for development of OpenMP and CUDA kernels for multidimensional data.pdf:pdf},
isbn = {9780769544489},
journal = {Proceedings - 2011 Symposium on Application Accelerators in High-Performance Computing, SAAHPC 2011},
keywords = {cuda,openMP,python},
mendeley-tags = {cuda,openMP,python},
pages = {159--167},
title = {{Python for development of OpenMP and CUDA kernels for multidimensional data}},
year = {2011}
}
@article{Herrero-Lopez2011,
abstract = {The uninterrupted growth of information repositories has progressively lead data-intensive applications, such as MapReduce-based systems, to the mainstream. The MapReduce paradigm has frequently proven to be a simple yet flexible and scalable technique to distribute algorithms across thousands of nodes and petabytes of information. Under these circumstances, classic data mining algorithms have been adapted to this model, in order to run in production environments. Unfortunately, the high latency nature of this architecture has relegated the applicability of these algorithms to batch-processing scenarios. In spite of this shortcoming, the emergence of massively threaded shared-memory multiprocessors, such as Graphics Processing Units (GPU), on the commodity computing market has enabled these algorithms to be executed orders of magnitude faster, while keeping the same MapReduce based model. In this paper, we propose the integration of massively threaded shared-memory multiprocessors into MapReduce-based clusters creating a unified heterogeneous architecture that enables executing Map and Reduce operators on thousands of threads across multiple GPU devices and nodes, while maintaining the built-in reliability of the baseline system. For this purpose, we created a programming model that facilitates the collaboration of multiple CPU cores and multiple GPU devices towards the resolution of a data intensive problem. In order to prove the potential of this hybrid system, we take a popular NP-Hard supervised learning algorithm, the Support Vector Machine (SVM) and show that a 36x \&\#x2013; 192x speedup can be achieved on large datasets without changing the model or leaving the commodity hardware paradigm.},
author = {Herrero-Lopez, Sergio},
doi = {10.1109/ICSMC.2011.6083839},
file = {:home/chiroptera/Dropbox/mendeley/Herrero-Lopez - 2011 - Accelerating SVMs by integrating GPUs into MapReduce clusters.pdf:pdf},
isbn = {9781457706523},
issn = {1062922X},
journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
keywords = {Multiprocessing,Parallel Algorithms,Pattern Classification,cluster,gpu,machine learning,mapreduce,svm,topnotch},
mendeley-tags = {cluster,gpu,machine learning,mapreduce,svm,topnotch},
pages = {1298--1305},
title = {{Accelerating SVMs by integrating GPUs into MapReduce clusters}},
year = {2011}
}
@inproceedings{DiBuccio2011,
abstract = {Dynamic Quantum Clustering is a recent clustering technique which makes use of Parzen window estimator to construct a potential function whose minima are related to the clusters to be found. The dynamic of the system is computed by means of the Schr\"{o}dinger differential equation. In this paper, we apply this technique in the context of Information Retrieval to explore its performance in terms of the quality of clusters and the efficiency of the computation. In particular, we want to analyze the clusters produced by using datasets of relevant and non-relevant documents given a topic. © 2011 Springer-Verlag.},
author = {{Di Buccio}, Emanuele and {Di Nunzio}, Giorgio Maria},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {360--363},
title = {{Distilling relevant documents by means of dynamic quantum clustering}},
volume = {6931 LNCS},
year = {2011}
}
@article{Horn2001a,
abstract = {We propose a novel clusteringmethod that is an extension of ideas inher- ent to scale-space clustering and support-vector clustering. Like the lat- ter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scale- space probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schr¨ odinger equation of which the probability function is a solution. This Schr¨ odinger equation contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. Themethod has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schr¨ odinger potential to the locations of data points, we can apply this method to problems in high dimensions. 1},
author = {Horn, David and Gottlieb, Assaf},
file = {:home/chiroptera/Dropbox/mendeley/Horn, Gottlieb - 2001 - The Method of Quantum Clustering.pdf:pdf},
journal = {NIPS},
number = {1},
title = {{The Method of Quantum Clustering.}},
url = {http://www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/AA08.ps.gz},
year = {2001}
}
@article{Nvidia2010,
author = {Nvidia},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia - 2010 - Introduction to CUDA C.pdf:pdf},
journal = {Siggraph Asia 2010},
title = {{Introduction to CUDA C}},
year = {2010}
}
@article{Weinstein2009a,
abstract = {Last year, in 2008, I gave a talk titled \{$\backslash$it Quantum Calisthenics\}. This year I am going to tell you about how the work I described then has spun off into a most unlikely direction. What I am going to talk about is how one maps the problem of finding clusters in a given data set into a problem in quantum mechanics. I will then use the tricks I described to let quantum evolution lets the clusters come together on their own.},
archivePrefix = {arXiv},
arxivId = {0911.0462},
author = {Weinstein, Marvin},
eprint = {0911.0462},
file = {:home/chiroptera/Dropbox/mendeley/Weinstein - 2009 - Strange Bedfellows Quantum Mechanics and Data Mining.pdf:pdf},
pages = {11},
title = {{Strange Bedfellows: Quantum Mechanics and Data Mining}},
url = {http://arxiv.org/abs/0911.0462},
year = {2009}
}
@article{Ji2013,
abstract = {Type Ia supernovae (SNe Ia) play a crucial role as standardizable cosmological candles, though the nature of their progenitors is a subject of active investigation. Recent observational and theoretical work has pointed to merging white dwarf binaries, referred to as the double-degenerate channel, as the possible progenitor systems for some SNe Ia. Additionally, recent theoretical work suggests that mergers which fail to detonate may produce magnetized, rapidly-rotating white dwarfs. In this paper, we present the first multidimensional simulations of the post-merger evolution of white dwarf binaries to include the effect of the magnetic field. In these systems, the two white dwarfs complete a final merger on a dynamical timescale, and are tidally disrupted, producing a rapidly-rotating white dwarf merger surrounded by a hot corona and a thick, differentially-rotating disk. The disk is strongly susceptible to the magnetorotational instability (MRI), and we demonstrate that this leads to the rapid growth of an initially dynamically weak magnetic field in the disk, the spin-down of the white dwarf merger, and to the subsequent central ignition of the white dwarf merger. Additionally, these magnetized models exhibit new features not present in prior hydrodynamic studies of white dwarf mergers, including the development of MRI turbulence in the hot disk, magnetized outflows carrying a significant fraction of the disk mass, and the magnetization of the white dwarf merger to field strengths \$\backslash sim 2 \backslash times 10\^{}8\$ G. We discuss the impact of our findings on the origin and observed properties of SNe Ia and magnetized white dwarfs.},
archivePrefix = {arXiv},
arxivId = {1302.5700},
author = {Bogert, F. Alexander and Smith, Nicholas and Holdener, John and Jong, Eric M. De and Hart, Andrew F. and Shamir, Lior and Allen, Alice and {Luca Cinquini}, Shakeh E. Khudikyan and Thompson, David R. and Mattmann, Chris A. and Wagstaff, Kiri and Lazio, Joseph and Jones, Dayton L. and Teuben, Peter},
doi = {10.1088/0004-637X/773/2/136},
eprint = {1302.5700},
file = {:home/chiroptera/Dropbox/mendeley/Bogert et al. - 2013 - Computing in Astronomy Applications and Examples.pdf:pdf},
issn = {0004-637X},
journal = {The Astrophysical Journal},
pages = {14},
title = {{Computing in Astronomy: Applications and Examples}},
url = {http://arxiv.org/abs/1302.5700},
volume = {submitted},
year = {2013}
}
@article{Elteir2011,
abstract = {MapReduce is a programming model from Google that facilitates parallel processing on a cluster of thousands of commodity computers. The success of MapReduce in cluster environments has motivated several studies of implementing MapReduce on a graphics processing unit (GPU), but generally focusing on the NVIDIA GPU. Our investigation reveals that the design and mapping of the MapReduce framework needs to be revisited for AMD GPUs due to their notable architectural differences from NVIDIA GPUs. For instance, current state-of-the-art MapReduce implementations employ atomic operations to coordinate the execution of different threads. However, atomic operations can implicitly cause inefficient memory access, and in turn, severely impact performance. In this paper, we propose Streamer, an OpenCL MapReduce framework optimized for AMD GPUs. With efficient atomic-free algorithms for output handling and intermediate result shuffling, Stream MR is superior to atomic-based MapReduce designs and can outperform existing atomic-free MapReduce implementations by nearly five-fold on an AMD Radeon HD 5870.},
author = {Elteir, Marwa and Lin, Heshan and Feng, Wu Chun and Scogland, Tom},
doi = {10.1109/ICPADS.2011.131},
file = {:home/chiroptera/Dropbox/mendeley/Elteir et al. - 2011 - StreamMR An optimized MapReduce framework for AMD GPUs.pdf:pdf},
isbn = {9780769545769},
issn = {15219097},
journal = {Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS},
keywords = {AMD GPU,Atomics,GPGPU,MapCG,MapReduce,Mars,OpenCL,Parallel computing},
pages = {364--371},
title = {{StreamMR: An optimized MapReduce framework for AMD GPUs}},
year = {2011}
}
@article{Blekas2009,
author = {Blekas, Konstantinos and Christodoulidou, K and Lagaris, I E},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Christodoulidou, Lagaris - 2009 - LNCS 5769 - Newtonian Spectral Clustering.pdf:pdf},
pages = {145--154},
title = {{LNCS 5769 - Newtonian Spectral Clustering}},
year = {2009}
}
@article{Mullner2013,
abstract = {The fastcluster package is a C++ library for hierarchical, agglomerative clustering. It provides a fast implementation of the most efficient, current algorithms when the input is a dissimilarity index. Moreover, it features memory-saving routines for hierarchical clustering of vector data. It improves both asymptotic time complexity (in most cases) and practical performance (in all cases) compared to the existing implementations in standard software: several R packages, MATLAB, Mathematica, Python with SciPy.},
archivePrefix = {arXiv},
arxivId = {1109.2378},
author = {M\"{u}llner, Daniel},
eprint = {1109.2378},
file = {:home/chiroptera/Dropbox/mendeley/M\"{u}llner - 2013 - fastcluster Fast Hierarchical , Agglomerative.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {age,agglomerative,algorithm,aver-,c,centroid,clustering,complete,hierarchical,linkage,mathematica,matlab,mcquitty,median,python,scipy,single,upgma,upgmc,ward,weighted,wpgma,wpgmc},
number = {9},
pages = {1--18},
title = {{fastcluster : Fast Hierarchical , Agglomerative}},
url = {http://www.jstatsoft.org/v53/i09},
volume = {53},
year = {2013}
}
@article{Kijsipongse2012,
abstract = {K-Means is the clustering algorithm which is widely used in many areas such as information retrieval, computer vision and pattern recognition. With the recent advance in General Purpose Graphics Processing Unit (GPGPU), we can use a modern GPU which is capable to do computation up to Tflops to calculate K-Means clustering on average problems. However, due to the exponential growth of data, the K-Means clustering on a single GPU will not be adequate for large datasets in the near future. In this paper, we present the design and implementation of an efficient large-scale parallel K-Means on GPU clusters. We utilize the massive parallelism in GPUs to speed up the most time consuming part of K-Means clustering in each node. We employ the dynamic load balancing to distribute workload equally on different GPUs installed in the clusters so as to improve the performance of the parallel K-Means at the inter-node level. We also take advantage from software distributed shared memory to simplify the communication and collaboration among nodes. The result of the evaluation shows the performance improvement of the parallel K-Means by maintaining load balance on GPU clusters.},
author = {Kijsipongse, Ekasit and U-Ruekolan, Suriya},
doi = {10.1109/JCSSE.2012.6261977},
file = {:home/chiroptera/Dropbox/mendeley/Kijsipongse, U-Ruekolan - 2012 - Dynamic load balancing on GPU clusters for large-scale K-Means clustering.pdf:pdf},
isbn = {9781467319218},
journal = {JCSSE 2012 - 9th International Joint Conference on Computer Science and Software Engineering},
keywords = {gpu,k-means},
mendeley-tags = {gpu,k-means},
pages = {346--350},
title = {{Dynamic load balancing on GPU clusters for large-scale K-Means clustering}},
year = {2012}
}
@article{Letham2013,
abstract = {It is easy to find expert knowledge on the Internet on almost any topic, but obtaining a complete overview of a given topic is not always easy: information can be scattered across many sources and must be aggregated to be useful. We introduce a method for intelligently growing a list of relevant items, starting from a small seed of examples. Our algorithm takes advantage of the wisdom of the crowd, in the sense that there are many experts who post lists of things on the Internet. We use a collection of simple machine learning components to find these experts and aggregate their lists to produce a single complete and meaningful list. We use experiments with gold standards and open-ended experiments without gold standards to show that our method significantly outperforms the state of the art. Our method uses the ranking algorithm Bayesian Sets even when its underlying independence assumption is violated, and we provide a theoretical generalization bound to motivate its use.},
author = {Letham, Benjamin and Rudin, Cynthia and Heller, Katherine a.},
doi = {10.1007/s10618-013-0329-7},
file = {:home/chiroptera/Dropbox/mendeley/Letham, Rudin, Heller - 2013 - Growing a list.pdf:pdf},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {Collective intelligence,Internet data mining,Ranking,Set completion},
number = {3},
pages = {372--395},
title = {{Growing a list}},
volume = {27},
year = {2013}
}
@article{Arbelaez2013,
author = {Arbelaez, Alejandro and Quesada, Luis},
file = {:home/chiroptera/Dropbox/mendeley/Arbelaez, Quesada - 2013 - Parallelising the k-Medoids Clustering Problem Using Space-Partitioning.pdf:pdf},
journal = {Sixth Annual Symposium on Combinatorial Search},
keywords = {Full Papers,k-medoids,parallel},
mendeley-tags = {k-medoids,parallel},
pages = {20--28},
title = {{Parallelising the k-Medoids Clustering Problem Using Space-Partitioning}},
url = {http://www.aaai.org/ocs/index.php/SOCS/SOCS13/paper/view/7225\&lt;/ee\&gt;},
year = {2013}
}
@article{Geras2011,
author = {Geras, Krzysztof Jerzy},
file = {:home/chiroptera/Dropbox/mendeley/Geras - 2011 - Prediction Markets for Machine Learning.pdf:pdf},
number = {248264},
title = {{Prediction Markets for Machine Learning}},
year = {2011}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, CJC Christopher J C},
doi = {10.1023/A:1009715923555},
editor = {Fayyad, Usama},
eprint = {1111.6189v1},
institution = {Bell Laboratories, Lucent Technologies},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
publisher = {Springer},
series = {NetGames '06},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf$\backslash$nhttp://link.springer.com/article/10.1023/A:1009715923555},
volume = {2},
year = {1998}
}
@article{Nadungodage2013,
abstract = {Recommendation systems are a popular marketing strategy for online service providers. These systems predict a customer's future preferences from the past behaviors of that customer and the other customers. Most of the popular online stores process millions of transactions per day; therefore, providing quick and quality recommendations using the large amount of data collected from past transactions can be challenging. Parallel processing power of GPUs can be used to accelerate the recommendation process. However, the amount of memory available on a GPU card is limited; thus, a number of passes may be required to completely process a large-scale dataset. This paper proposes two parallel, item-based recommendation algorithms implemented using the CUDA platform. Considering the high sparsity of the user-item data, we utilize two compression techniques to reduce the required number of passes and increase the speedup. The experimental results on synthetic and real-world datasets show that our algorithms outperform the respective CPU implementations and also the na\~{A}¯ve GPU implementation which does not use compression. \^{A}© 2013 IEEE.},
author = {Nadungodage, Chandima Hewa and Xia, Yuni and Lee, John Jaehwan and Lee, Myungcheol and Park, Choon Seo},
doi = {10.1109/BigData.2013.6691571},
file = {:home/chiroptera/Dropbox/mendeley/Nadungodage et al. - 2013 - GPU accelerated item-based collaborative filtering for big-data applications.pdf:pdf},
isbn = {9781479912926},
journal = {Proceedings - 2013 IEEE International Conference on Big Data, Big Data 2013},
keywords = {CUDA,big-data,collaborative filtering,recommendation systems GPU},
pages = {175--180},
title = {{GPU accelerated item-based collaborative filtering for big-data applications}},
year = {2013}
}
@inproceedings{Li2010,
abstract = {Based on the concepts and principles of quantum computing, a novel clustering algorithm, called a quantum-inspired immune clonal clustering algorithm based on watershed (QICW), is proposed to deal with the problem of image segmentation. In QICW, antibody is proliferated and divided into a set of subpopulation groups. Antibodies in a subpopulation group are represented by multi-state gene quantum bits. In the antibody's updating, the quantum mutation operator is applied to accelerate convergence. The quantum recombination realizes the information communication between the subpopulation groups so as to avoid premature convergences. In this paper, the segmentation problem is viewed as a combinatorial optimization problem, the original image is partitioned into small blocks by watershed algorithm, and the quantum-inspired immune clonal algorithm is used to search the optimal clustering centre, and make the sequence of maximum affinity function as clustering result, and finally obtain the segmentation result. Experimental results show that the proposed method is effective for texture image and SAR image segmentation, compared with the genetic clustering algorithm based on watershed (W-GAC), and the k-means algorithm based on watershed (W-KM).},
author = {Li, Yangyang and Wu, Nana and Ma, Jingjing and Jiao, Licheng},
booktitle = {IEEE Congress on Evolutionary Computation},
doi = {10.1109/CEC.2010.5586362},
file = {:home/chiroptera/Dropbox/mendeley/Li et al. - 2010 - Quantum-inspired immune clonal clustering algorithm based on watershed.pdf:pdf},
isbn = {978-1-4244-6909-3},
keywords = {Clustering algorithms,Error analysis,Feature extraction,Image segmentation,Partitioning algorithms,Radiative recombination,SAR image segmentation,Wavelet transforms,combinatorial mathematics,combinatorial optimization problem,genetic algorithms,genetic clustering algorithm,image segmentation,image texture,k-means algorithm,maximum affinity function,multistate gene quantum bits,pattern clustering,quantum computing,quantum mutation operator,quantum-inspired immune clonal clustering algorith,synthetic aperture radar,texture image,watershed algorithm},
month = jul,
pages = {1--7},
publisher = {IEEE},
shorttitle = {Evolutionary Computation (CEC), 2010 IEEE Congress},
title = {{Quantum-inspired immune clonal clustering algorithm based on watershed}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5586362},
year = {2010}
}
@book{Aggarwal2014,
author = {Aggarwal, Charu C and Reddy, Chandan K},
file = {:home/chiroptera/Dropbox/mendeley/Aggarwal, Reddy - Unknown - Data clustering algorithms and applications.pdf:pdf},
isbn = {9781466558229},
title = {{Data clustering algorithms and applications}}
}
@article{Dong2010,
abstract = {Building the quantum clustering model by quantum characteristic. It is proved by the Simulation experiment, that It can deal with exceptional, high-dimension complicated data and large-scale data set.},
author = {Dong, Yumin and Jia, Fanghua},
doi = {10.1109/ISIP.2010.111},
file = {:home/chiroptera/Dropbox/mendeley/Dong, Jia - 2010 - A new-style generalized quantum clustering model.pdf:pdf},
isbn = {9780769542614},
journal = {Proceedings - 3rd International Symposium on Information Processing, ISIP 2010},
keywords = {Clustering arithmetic,Quantum clustering model,Quantum entropy},
pages = {519--522},
title = {{A new-style generalized quantum clustering model}},
year = {2010}
}
@article{Xin2012,
author = {Xin, Miao and Li, Hao},
doi = {10.1109/IJCSS.2012.22},
file = {:home/chiroptera/Dropbox/mendeley/Xin, Li - 2012 - An implementation of GPU accelerated MapReduce Using Hadoop with OpenCL for data- and compute-intensive jobs.pdf:pdf},
isbn = {9780769547312},
journal = {Proceedings - 2012 International Joint Conference on Service Sciences, Service Innovation in Emerging Economy: Cross-Disciplinary and Cross-Cultural Perspective, IJCSS 2012},
keywords = {GPU acceleration,Hadoop,MapReduce,OpenCL},
pages = {6--11},
title = {{An implementation of GPU accelerated MapReduce: Using Hadoop with OpenCL for data- and compute-intensive jobs}},
year = {2012}
}
@article{DiMarco2013a,
abstract = {Discover and quantify the performance gains of dynamic parallelism for clustering algorithms on GPUs. Dynamic parallelism effectively eliminates the superfluous back and forth communication between the GPU and CPU through nested kernel computations. The change in performance is measured using two well-known clustering algorithms that exhibit data dependencies: the K-means clustering and the hierarchical clustering. K-means has a sequential data dependence wherein iterations occur in a linear fashion, while the hierarchical clustering has a tree-like dependence that produces split tasks. Analyzing the performance of these data-dependent algorithms gives us a better understanding of the benefits or potential drawbacks of CUDA 5's new dynamic parallelism feature.},
author = {DiMarco, Jeffrey and Taufer, Michela},
doi = {10.1117/12.2018069},
file = {:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms.pdf:pdf;:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms(2).pdf:pdf},
isbn = {9780819495433},
issn = {0277786X},
journal = {Spie},
keywords = {,0,cuda 5,divisive hierarchical clustering,k-means},
pages = {87520E},
title = {{Performance impact of dynamic parallelism on different clustering algorithms}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2018069},
year = {2013}
}
@article{Wang2013,
author = {Wang, Huaixiao and Liu, Jianyong and Zhi, Jun and Fu, Chengqun},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2013 - The Improvement of Quantum Genetic Algorithm and Its Application on Function Optimization.pdf:pdf},
number = {1},
title = {{The Improvement of Quantum Genetic Algorithm and Its Application on Function Optimization}},
volume = {2013},
year = {2013}
}
@article{Woong-KeeLoh2014,
author = {Woong-KeeLoh and Kim, Young-Kuk},
doi = {10.1109/BDCloud.2014.130},
file = {:home/chiroptera/Dropbox/mendeley/Woong-KeeLoh, Kim - 2014 - A GPU-accelerated Density-Based Clustering Algorithm.pdf:pdf},
isbn = {978-1-4799-6719-3},
journal = {2014 IEEE Fourth International Conference on Big Data and Cloud Computing},
keywords = {12,3,and there have been,bohm et al,clustering,cuda,density-based clustering,divide-and-conquer,gpu,many approaches to improve,parallel algorithm,performance,proposed an algorithm,their},
mendeley-tags = {clustering,gpu},
pages = {775--776},
title = {{A GPU-accelerated Density-Based Clustering Algorithm}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7034874},
year = {2014}
}
@article{Singh1977,
annote = {Absolute trash paper. Use only to get references for applications of GPGPU.},
author = {Singh, Sarabjeet},
file = {:home/chiroptera/Dropbox/mendeley/Singh - 1977 - CUDA for GPGPU Applications – A Survey.pdf:pdf},
keywords = {GPGPU,examples},
mendeley-tags = {examples,GPGPU},
pages = {1--4},
title = {{CUDA for GPGPU Applications – A Survey}},
year = {1977}
}
@misc{Horn2010,
author = {Horn, David and Aviv, Tel and Gottlieb, Assaf and HaSharon, Hod and Axel, Inon and Gan, Ramat},
file = {:home/chiroptera/Dropbox/mendeley/Horn et al. - 2010 - Method and Apparatus for Quantum Clustring.pdf:pdf},
number = {12},
title = {{Method and Apparatus for Quantum Clustring}},
volume = {2},
year = {2010}
}
@article{Jain2010,
abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Jain, Anil K},
doi = {10.1016/j.patrec.2009.09.011},
file = {:home/chiroptera/Dropbox/mendeley/Jain - 2010 - Data clustering 50 years beyond K-means.pdf:pdf},
isbn = {9781424417360},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Data clustering,Historical developments,King-Sun Fu prize,Perspectives on clustering,User's dilemma},
number = {8},
pages = {651--666},
publisher = {Elsevier B.V.},
title = {{Data clustering: 50 years beyond K-means}},
url = {http://dx.doi.org/10.1016/j.patrec.2009.09.011},
volume = {31},
year = {2010}
}
@article{Halkidi2001,
abstract = {Cluster analysis aims at identifying groups of similar objects and, therefore helps to discover distribution of patterns and interesting correlations in large data sets. It has been subject of wide research since it arises in many application domains in engineering, business and social sciences. Especially, in the last years the availability of huge transactional and experimental data sets and the arising requirements for data mining created needs for clustering algorithms that scale and can be applied in diverse domains.},
author = {Halkidi, Maria and Batistakis, Yannis and Vazirgiannis, Michalis},
doi = {10.1023/A:1012801612483},
file = {:home/chiroptera/Dropbox/mendeley/Halkidi, Batistakis, Vazirgiannis - 2001 - On clustering validation techniques.pdf:pdf},
isbn = {0925-9902},
issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Cluster validity,Clustering algorithms,Unsupervised learning,Validity indices},
number = {2-3},
pages = {107--145},
title = {{On clustering validation techniques}},
volume = {17},
year = {2001}
}
@article{Nvidia2014,
author = {Nvidia},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia - 2014 - Cuda c programming guide.pdf:pdf},
journal = {Programming Guides},
number = {August},
title = {{Cuda c programming guide}},
year = {2014}
}
@article{Kaldewey2011,
author = {Kaldewey, Tim},
file = {:home/chiroptera/Dropbox/mendeley/Kaldewey - 2011 - Large-Scale GPU programming.pdf:pdf},
keywords = {gpu},
mendeley-tags = {gpu},
title = {{Large-Scale GPU programming}},
year = {2011}
}
@article{Papenhausen2013,
abstract = {Clustering is an important preparation step in big data processing. It may even be used to detect redundant data points as well as outliers. Elimination of redundant data and duplicates can serve as a viable means for data reduction and it can also aid in sampling. Visual feedback is very valuable here to give users confidence in this process. Furthermore, big data preprocessing is seldom interactive, which stands at conflict with users who seek answers immediately. The best one can do is incremental preprocessing in which partial and hopefully quite accurate results become available relatively quickly and are then refined over time. We propose a correlation clustering framework which uses MDS for layout and GPU-acceleration to accomplish these goals. Our domain application is the correlation clustering of atmospheric mass spectrum data with 8 million data points of 450 dimensions each. © 2013 IEEE.},
author = {Papenhausen, Eric and Wang, Bing and Ha, Sungsoo and Zelenyuk, Alla and Imre, Dan and Mueller, Klaus},
doi = {10.1109/BigData.2013.6691716},
file = {:home/chiroptera/Dropbox/mendeley/Papenhausen et al. - 2013 - GPU-accelerated incremental correlation clustering of large data with visual feedback.pdf:pdf},
isbn = {9781479912926},
journal = {Proceedings - 2013 IEEE International Conference on Big Data, Big Data 2013},
keywords = {GPU,big data,clustering,correlation,visual analytics,visualization},
pages = {63--70},
title = {{GPU-accelerated incremental correlation clustering of large data with visual feedback}},
year = {2013}
}
@article{Sirotkovi2012,
author = {Sirotkovi, J and Dujmi, H and Papi, V},
file = {:home/chiroptera/Dropbox/mendeley/Sirotkovi, Dujmi, Papi - 2012 - K-Means Image Segmentation on Massively Parallel GPU Architecture.pdf:pdf},
isbn = {9789532330724},
pages = {489--494},
title = {{K-Means Image Segmentation on Massively Parallel GPU Architecture}},
year = {2012}
}
@article{Zechner2009,
abstract = {In this paper an optimized k-means implementation on the graphics processing unit (GPU) is presented. NVIDIApsilas compute unified device architecture (CUDA), available from the G80 GPU family onwards, is used as the programming environment. Emphasis is placed on optimizations directly targeted at this architecture to best exploit the computational capabilities available. Additionally drawbacks and limitations of previous related work, e.g. maximum instance, dimension and centroid count are addressed. The algorithm is realized in a hybrid manner, parallelizing distance calculations on the GPU while sequentially updating cluster centroids on the CPU based on the results from the GPU calculations. An empirical performance study on synthetic data is given, demonstrating a maximum 14times speed increase to a fully SIMD optimized CPU implementation.},
author = {Zechner, Mario and Granitzer, Michael},
doi = {10.1109/INTENSIVE.2009.19},
file = {:home/chiroptera/Dropbox/mendeley/Zechner, Granitzer - 2009 - Accelerating k-means on the graphics processor via CUDA.pdf:pdf},
isbn = {9780769535852},
journal = {Proceedings of the 1st International Conference on Intensive Applications and Services, INTENSIVE 2009},
pages = {7--15},
title = {{Accelerating k-means on the graphics processor via CUDA}},
year = {2009}
}
@article{Cs2003,
author = {Cs, Sael Lee and Biology, Computational},
file = {:home/chiroptera/Dropbox/mendeley/Cs, Biology - 2003 - Lecture 16 pca and svd.pdf:pdf},
title = {{Lecture 16: pca and svd}},
year = {2003}
}
@article{Bai2009,
abstract = {K-means algorithm is one of the most famous unsupervised clustering algorithms. Many theoretical improvements for the performance of original algorithms have been put forward, while almost all of them are based on single instruction single data (SISD) architecture processors (GPUs), which partly ignored the inherent paralleled characteristic of the algorithms. In this paper, a novel single instruction multiple data (SIMD) architecture processors (GPUs) based k-means algorithm is proposed. In this algorithm, in order to accelerate compute-intensive portions of traditional k-means, both data objects assignment and k-centroids recalculation are offloaded to the GPU in parallel. We have implemented this GPU-based k-means on the newest generation GPU with compute unified device architecture(CUDA). The numerical experiments demonstrated that the speed of GPU-based k-means could reach as high as 40 times of the CPU-based k-means.},
annote = {Doesn't say which tools were used (C, Matlab, Fortran???).},
author = {Bai, Hong Tao and He, Li Li and Ouyang, Dan Tong and Li, Zhan Shan and Li, He},
doi = {10.1109/CSIE.2009.491},
file = {:home/chiroptera/Dropbox/mendeley/Bai et al. - 2009 - K-means on commodity GPUs with CUDA.pdf:pdf},
isbn = {9780769535074},
journal = {2009 WRI World Congress on Computer Science and Information Engineering, CSIE 2009},
pages = {651--655},
title = {{K-means on commodity GPUs with CUDA}},
volume = {3},
year = {2009}
}
@article{Fred2009,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt. 3 Validation of Custering Solutions.pdf:pdf},
number = {April},
pages = {1--15},
title = {{Tutorial Pt. 3: Validation of Custering Solutions}},
year = {2009}
}
@article{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, ran- domized seeding technique, we obtain an algorithm that is $\Theta$(log k)-competitive with the optimal clustering. Prelim- inary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, D. and Arthur, D. and Vassilvitskii, S. and Vassilvitskii, S.},
doi = {10.1145/1283383.1283494},
file = {:home/chiroptera/Dropbox/mendeley/Arthur et al. - 2007 - k-means The advantages of careful seeding.pdf:pdf},
isbn = {978-0-898716-24-5},
journal = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
pages = {1027--1035},
title = {{k-means++: The advantages of careful seeding}},
url = {http://portal.acm.org/citation.cfm?id=1283494},
volume = {8},
year = {2007}
}
@article{Strehl2000,
author = {Strehl, Alexander and Ghosh, Joydeep},
doi = {10.1162/153244303321897735},
file = {:home/chiroptera/Dropbox/mendeley/Strehl, Ghosh - 2000 - Cluster Ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions.pdf:pdf},
issn = {0003-6951},
journal = {CrossRef Listing of Deleted DOIs},
keywords = {cluster analysis,clustering,consensus functions,ensemble,knowledge reuse,multi-learner systems,mutual information,partitioning,unsupervised learning},
pages = {583--617},
title = {{Cluster Ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions}},
url = {http://dl.acm.org/citation.cfm?id=944919.944935},
volume = {1},
year = {2000}
}
@article{Malakar2013,
author = {Malakar, Ranajoy and Vydyanathan, Naga},
doi = {10.1109/ParCompTech.2013.6621392},
file = {:home/chiroptera/Dropbox/mendeley/Malakar, Vydyanathan - 2013 - A CUDA-enabled hadoop cluster for fast distributed image processing.pdf:pdf},
isbn = {9781479915910},
journal = {2013 National Conference on Parallel Computing Technologies, PARCOMPTECH 2013},
keywords = {CUDA,GPGPU,Hadoop,Map-reduce},
title = {{A CUDA-enabled hadoop cluster for fast distributed image processing}},
year = {2013}
}
@article{Bennett,
author = {Bennett, Charles},
file = {:home/chiroptera/Dropbox/mendeley/Bennett - Unknown - Is quantum search practical.pdf:pdf},
pages = {22--30},
title = {{Is quantum search practical?}}
}
@article{ArulShalom2011,
abstract = {We explore the capabilities of today's high-end Graphics processing units (GPU) on desktops to efficiently perform hierarchical agglomerative clustering (HAC) through partitioning of data. Traditional HAC has high time and memory complexities leading to low clustering efficiencies. We reduce time and memory bottlenecks of the traditional HAC algorithm by exploring the performance capabilities of the GPU, significantly accelerating the computations without compromising the accuracy of clusters. We implement the traditional HAC and the Partially Overlapping Partitioning (PoP) on GPU using Compute Unified Device Architecture (CUDA) and compare the computational performance with CPU using micro array data. The result shows that the PoP HAC and traditional HAC are up to 442 times and 66 times faster on the GPU respectively than the time taken by CPU. The PoP-enabled HAC on GPU requires only a fraction of the memory required by traditional HAC both on the CPU and GPU.},
author = {{Arul Shalom}, S. a. and Dash, Manoranjan},
doi = {10.1109/PDCAT.2011.38},
file = {:home/chiroptera/Dropbox/mendeley/Arul Shalom, Dash - 2011 - Efficient hierarchical agglomerative clustering algorithms on GPU using data partitioning.pdf:pdf},
isbn = {9780769545646},
journal = {Parallel and Distributed Computing, Applications and Technologies, PDCAT Proceedings},
keywords = {Computational speed-ups,Efficient partitioning,GPGPU,GPU clustering,GPU computing,GPU for acceleration,Hierarchical agglomerative clustering},
pages = {134--139},
title = {{Efficient hierarchical agglomerative clustering algorithms on GPU using data partitioning}},
year = {2011}
}
@article{Casper2013,
abstract = {The ability to cluster data accurately is essential to applications such as image segmentation. Therefore, techniques that enhance accuracy are of keen interest. One such technique involves applying a quantum mechanical system model, such as that of the quantum bit, to generate probabilistic numerical output to be used as variable input for clustering algorithms. This work demonstrates that applying a quantum bit model to data clustering algorithms can increase clustering accuracy, as a result of simulating superposition as well as possessing both guaranteed and controllable convergence properties. For accuracy assessment purposes, four quantum-modeled clustering algorithms for multi-band image segmentation are explored and evaluated. The clustering algorithms of choice consist of quantum variants of K-Means, Fuzzy C-Means, New Weighted Fuzzy C- Means, and the Artificial Bee Colony. Data sets of interest include multi-band imagery, which subsequent to classification are analyzed and assessed for accuracy. Results demonstrate that these algorithms exhibit improved accuracy, when compared to classical counterparts. Moreover, solutions are enhanced via introduction of the quantum state machine, which provides random initial centroid and variable input values to the various clustering algorithms, and quantum operators, which bring about convergence and maximize local search space exploration. Typically, the algorithms have shown to produce better solutions. Keywords:},
author = {Casper, Ellis and Hung, Chih-cheng},
doi = {10.4156/pica.vol2.issue1.1},
file = {:home/chiroptera/Dropbox/mendeley/Casper, Hung - 2013 - Quantum Modeled Clustering Algorithms for Image Segmentation 1.pdf:pdf},
keywords = {artificial bee colony,clustering,clustering algorithms,fuzzy c-means,image segmentation,k-means,quantum computing,quantum mechanics,qubit,weighted fuzzy c-means},
mendeley-tags = {artificial bee colony,clustering,fuzzy c-means,image segmentation,k-means,quantum computing,quantum mechanics,qubit,weighted fuzzy c-means},
number = {March},
pages = {1--21},
title = {{Quantum Modeled Clustering Algorithms for Image Segmentation 1}},
volume = {2},
year = {2013}
}
@article{Jarvis1973,
abstract = {A nonparametric clustering technique incorporating the concept of similarity based on the sharing of near neighbors is pre- sented. In addition to being an essentially paraliel approach, the com- putational elegance of the method is such that the scheme is applicable to a wide class of practical problems involving large sample size and high dimensionality. No attempt is made to show how a priori problem knowledge can be introduced into the procedure.},
author = {Jarvis, R a and Patrick, Edward a},
doi = {10.1109/T-C.1973.223640},
file = {:home/chiroptera/Dropbox/mendeley/Jarvis, Patrick - 1973 - Clustering Using a Similarity Measure Based on Shared Near Neighbors.pdf:pdf},
isbn = {00189340 (ISSN)},
issn = {0018-9340},
journal = {Ieee Transactions on Computers},
keywords = {Clustering,nonparametric,pattern recognition,shared near neighbors,similarity measure.},
number = {11},
pages = {1025--1034},
title = {{Clustering Using a Similarity Measure Based on Shared Near Neighbors}},
volume = {C-22},
year = {1973}
}
@article{Zhang2010,
author = {Zhang, Yongpeng and Mueller, Frank and Cui, Xiaohui and Potok, Thomas and Ridge, Oak and Sciences, Computational and Division, Engineering and Ridge, Oak},
file = {:home/chiroptera/Dropbox/mendeley/Zhang et al. - Unknown - Large-Scale Multi-Dimensional Document Clustering on GPU Clusters.pdf:pdf},
isbn = {9781424464432},
title = {{Large-Scale Multi-Dimensional Document Clustering on GPU Clusters}}
}
@article{Horn2003,
abstract = {MOTIVATION: This paper introduces the application of a novel clustering method to microarray expression data. Its first stage involves compression of dimensions that can be achieved by applying SVD to the gene-sample matrix in microarray problems. Thus the data (samples or genes) can be represented by vectors in a truncated space of low dimensionality, 4 and 5 in the examples studied here. We find it preferable to project all vectors onto the unit sphere before applying a clustering algorithm. The clustering algorithm used here is the quantum clustering method that has one free scale parameter. Although the method is not hierarchical, it can be modified to allow hierarchy in terms of this scale parameter. RESULTS: We apply our method to three data sets. The results are very promising. On cancer cell data we obtain a dendrogram that reflects correct groupings of cells. In an AML/ALL data set we obtain very good clustering of samples into four classes of the data. Finally, in clustering of genes in yeast cell cycle data we obtain four groups in a problem that is estimated to contain five families. AVAILABILITY: Software is available as Matlab programs at http://neuron.tau.ac.il/\~{}horn/QC.htm.},
author = {Horn, David and Axel, Inon},
doi = {10.1093/bioinformatics/btg053},
file = {:home/chiroptera/Dropbox/mendeley/Horn, Axel - 2003 - Novel clustering algorithm for microarray expression data in a truncated SVD space.ps:ps;:home/chiroptera/Dropbox/mendeley/Horn, Axel - 2003 - Novel clustering algorithm for microarray expression data in a truncated SVD space.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
pages = {1110--1115},
pmid = {12801871},
title = {{Novel clustering algorithm for microarray expression data in a truncated SVD space}},
volume = {19},
year = {2003}
}
@article{Calders,
author = {Calders, Toon},
file = {:home/chiroptera/Dropbox/mendeley/Calders - Unknown - Data Mining Clustering What is Cluster Analysis.pdf:pdf},
title = {{Data Mining Clustering What is Cluster Analysis ?}}
}
@article{Su2012,
abstract = {GPU (Graphics Processing Unit) has a great impact on computing field. To enhance the performance of computing systems, researchers and developers use the parallel computing architecture of GPU. On the other hand, to reduce the development time of new products, two programming models are included in GPU, which are OpenCL (Open Computing Language) and CUDA (Compute Unified Device Architecture). The benefit of involving the two programming models in GPU is that researchers and developers don't have to understand OpenGL, DirectX or other program design, but can use GPU through simple programming language. OpenCL is an open standard API, which has the advantage of cross-platform. CUDA is a parallel computer architecture developed by NVIDIA, which includes Runtime API and Driver API. Compared with OpenCL, CUDA is with better performance. In this paper, we used plenty of similar kernels to compare the computing performance of C, OpenCL and CUDA, the two kinds of API's on NVIDIA Quadro 4000 GPU. The experimental result showed that, the executive time of CUDA Driver API was 94.9\%\~{}99.0\% faster than that of C, while and the executive time of CUDA Driver API was 3.8\%\~{}5.4\% faster than that of OpenCL. Accordingly, the cross-platform characteristic of OpenCL did not affect the performance of GPU.},
annote = {Comparison between OpenCL and CUDA.},
author = {Su, Ching Lung and Chen, Po Yu and Lan, Chun Chieh and Huang, Long Sheng and Wu, Kuo Hsuan},
doi = {10.1109/APCCAS.2012.6419068},
file = {:home/chiroptera/Dropbox/mendeley/Su et al. - 2012 - Overview and comparison of OpenCL and CUDA technology for GPGPU.pdf:pdf},
isbn = {9781457717291},
journal = {IEEE Asia-Pacific Conference on Circuits and Systems, Proceedings, APCCAS},
keywords = {cuda,opencl},
mendeley-tags = {cuda,opencl},
pages = {448--451},
title = {{Overview and comparison of OpenCL and CUDA technology for GPGPU}},
year = {2012}
}
@article{Stuart2011,
abstract = {We present GPMR, our stand-alone MapReduce library that leverages the power of GPU clusters for large-scale computing. To better utilize the GPU, we modify MapReduce by combining large amounts of map and reduce items into chunks and using partial reductions and accumulation. We use persistent map and reduce tasks and stress aspects of GPMR with a set of standard MapReduce benchmarks. We run these benchmarks on a GPU cluster and achieve desirable speedup and efficiency for all benchmarks. We compare our implementation to the current-best GPU-MapReduce library (runs only on a solo GPU) and a highly-optimized multi-core MapReduce to show the power of GPMR. We demonstrate how typical MapReduce tasks are easily modified to fit into GPMR and leverage a GPU cluster. We highlight how total and relative amounts of communication affect GPMR. We conclude with an exposition on the types of MapReduce tasks well-suited to GPMR, and why some tasks need more modifications than others to work well with GPMR.},
author = {Stuart, Jeff a. and Owens, John D.},
doi = {10.1109/IPDPS.2011.102},
file = {:home/chiroptera/Dropbox/mendeley/Stuart, Owens - 2011 - Multi-GPU MapReduce on GPU clusters.pdf:pdf},
isbn = {9780769543857},
issn = {1530-2075},
journal = {Proceedings - 25th IEEE International Parallel and Distributed Processing Symposium, IPDPS 2011},
keywords = {GPU,MapReduce,cluster},
mendeley-tags = {GPU,MapReduce,cluster},
pages = {1068--1079},
title = {{Multi-GPU MapReduce on GPU clusters}},
year = {2011}
}
@article{Fred2009c,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt.1 Basic Concepts of data clustering.pdf:pdf},
number = {April},
pages = {1--17},
title = {{Tutorial Pt.1: Basic Concepts of data clustering}},
year = {2009}
}
@article{Hung2013,
author = {Hung, Chih-cheng and Casper, Ellis and Kuo, Bor-chen and Liu, Wenping and Yu, Xiaoyi and Jung, Edward and Yang, Ming},
file = {:home/chiroptera/Dropbox/mendeley/Hung et al. - 2013 - A QUANTUM-MODELED FUZZY C-MEANS CLUSTERING ALGORITHM FOR REMOTELY SENSED MULTI-BAND IMAGE SEGMENTATION.pdf:pdf},
isbn = {9781479911141},
pages = {2501--2504},
title = {{A QUANTUM-MODELED FUZZY C-MEANS CLUSTERING ALGORITHM FOR REMOTELY SENSED MULTI-BAND IMAGE SEGMENTATION}},
year = {2013}
}
@article{Meila2003,
abstract = {This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information (VI), measures the amount of information lost and gained in changing from clustering \$\{\backslash cal C\}\$ to clustering \$\{\backslash cal C\}'\$ . The criterion makes no assumptions about how the clusterings were generated and applies to both soft and hard clusterings. The basic properties of VI are presented and discussed from the point of view of comparing clusterings. In particular, the VI is positive, symmetric and obeys the triangle inequality. Thus, surprisingly enough, it is a true metric on the space of clusterings. Keywords: Clustering; Comparing partitions; Measures of agreement; Information theory; Mutual information},
author = {Meila, Marina},
doi = {10.1007/978-3-540-45167-9\_14},
file = {:home/chiroptera/Dropbox/mendeley/Meila - 2003 - Comparing clusterings by the variation of information.pdf:pdf},
isbn = {978-3-540-40720-1, 978-3-540-45167-9},
issn = {03029743},
journal = {Learning theory and Kernel machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003: proceedings},
keywords = {clustering,comparing partitions,information theory,measures of agreement,mutual information},
pages = {173},
title = {{Comparing clusterings by the variation of information}},
url = {http://books.google.com/books?hl=en\&amp;lr=\&amp;id=hk1dqsM0XF4C\&amp;oi=fnd\&amp;pg=PA173\&amp;dq=Comparing+Clusterings+by+the+Variation+of+Information\&amp;ots=7rcmrLpFV1\&amp;sig=P-AXGQnlenPfAlSb3fdhphYv6dI},
year = {2003}
}
@article{Nvidia2009,
abstract = {The lethal outcome of high-dose pulmonary virus infection is thought to reflect high-level, sustained virus replication and associated lung inflammation prior to development of an adaptive immune response. Herein, we demonstrate that the outcome of lethal/sublethal influenza infection instead correlates with the initial virus replication tempo. Furthermore, the magnitude of early lung antiviral CD8+ T cell responses varies inversely with inoculum dose and is controlled by lymph-node-resident dendritic cells (LNDC) through IL-12p40-regulated FasL-dependent T cell apoptosis. These results suggest that the inoculum dose and replication rate of a pathogen entering the respiratory tract may regulate the strength of the adaptive immune response, and the subsequent outcome of infection and that LNDC may serve as regulators (gatekeepers) in the development of CD8+ T cell responses.},
author = {Nvidia, Whitepaper and Generation, Next and Compute, Cuda},
doi = {10.1016/j.immuni.2005.11.006},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia, Generation, Compute - 2009 - Whitepaper NVIDIA’s Next Generation CUDA Compute Architecture.pdf:pdf},
issn = {10747613},
journal = {ReVision},
number = {6},
pages = {1--22},
pmid = {16356862},
title = {{Whitepaper NVIDIA’s Next Generation CUDA Compute Architecture}},
url = {http://www.nvidia.com/content/PDF/fermi\_white\_papers/NVIDIA\_Fermi\_Compute\_Architecture\_Whitepaper.pdf},
volume = {23},
year = {2009}
}
@article{Winterstein1997,
author = {Winterstein, Felix and Bayliss, Samuel and Constantinides, Geoge a.},
file = {:home/chiroptera/Dropbox/mendeley/Winterstein, Bayliss, Constantinides - 1997 - FPGA-Based K-Means Clustering Using Tree-Based Data Structures.pdf:pdf},
pages = {1450--1455},
title = {{FPGA-Based K-Means Clustering Using Tree-Based Data Structures}},
volume = {101},
year = {1997}
}
@article{Shalom2009,
abstract = {We explore the use of todaypsilas high-end graphics processing units on desktops to perform hierarchical agglomerative clustering with the compute unified device architecture - CUDA of NVIDIA. Although the advancement in graphics cards has made the gaming industry to flourish,there is a lot more to be gained the field of scientific computing, high performance computing and their applications. Previous works have illustrated considerable speed gains on computing pair wise Euclidean distances between vectors, which is the fundamental operation in hierarchical clustering. We have used CUDA to implement the complete hierarchical agglomerative clustering algorithm and show almost double the speed gain using much cheaper desk top graphics card. In this paper we briefly explain the highly parallel and internally distributed programming structure of CUDA. We explore CUDA capabilities and propose methods to efficiently handle data within the graphics hardware for data intense, data independent, iterative or repetitive general purpose algorithms such as the hierarchical clustering. We achieved results with speed gains of about 30 to 65 times over the CPU implementation using micro array gene expressions.},
author = {Shalom, S. a Arul and Dash, Manoranjan and Tue, Minh and Wilson, Nithin},
doi = {10.1109/ICSPS.2009.167},
file = {:home/chiroptera/Dropbox/mendeley/Shalom et al. - 2009 - Hierarchical agglomerative clustering using graphics processor with compute unified device architecture.pdf:pdf},
isbn = {9780769536545},
journal = {2009 International Conference on Signal Processing Systems, ICSPS 2009},
keywords = {Acceleration of computations,CUDA hierarchical clustering,GPGPU,High performance computing,Parallel computing},
pages = {556--561},
title = {{Hierarchical agglomerative clustering using graphics processor with compute unified device architecture}},
year = {2009}
}
@article{Zhai2014,
abstract = {Big data processing is receiving significant amount of interest as an important technology to reveal the information behind the data, such as trends, characteristics, etc. MapReduce is one of the most popular distributed parallel data processing framework. However, some high-end applications, especially some scientific analyses have both data-intensive and computation intensive features. Therefore, we have designed and implemented a high performance big data process framework called Lit, which leverages the power of Hadoop and GPUs. In this paper, we presented the basic design and architecture of Lit. More importantly, we spent a lot of effort on optimizing the communications between CPU and GPU. Lit integrated GPU with Hadoop to improve the computational power of each node in the cluster. To simplify the parallel programming, Lit provided an annotation based approach to automatically generate CUDA codes from Hadoop codes. Lit hid the complexity of programming on CPU/GPU cluster by providing extended compiler and optimizer. To utilize the simplified programming, scalability and fault tolerance benefits of Hadoop and combine them with the high performance computation power of GPU, Lit extended the Hadoop by applying a GPUClassloader to detect the GPU, generate and compile CUDA codes, and invoke the shared library. For all CPU-GPU co-processing systems, the communication with the GPU is the well-known performance bottleneck. We introduced data flow optimization approach to reduce unnecessary memory copies. Our experimental results show that Lit can achieve an average speedup of 1x to 3x on three typical applications over Hadoop, and the data flow optimization approach for the Lit can achieve about 16\% performance gain. © 2013 IEEE.},
annote = {MapReduce for using GPU in clusters. Has review of other GPU MapReduce implementations.},
author = {Zhai, Yanlong and Guo, Ying and Chen, Qiurui and Yang, Kai and Mbarushimana, Emmanuel},
doi = {10.1109/HPCC.and.EUC.2013.147},
file = {:home/chiroptera/Dropbox/mendeley/Zhai et al. - 2014 - Design and optimization of a big data computing framework based on CPUGPU cluster.pdf:pdf},
isbn = {9780769550886},
journal = {Proceedings - 2013 IEEE International Conference on High Performance Computing and Communications, HPCC 2013 and 2013 IEEE International Conference on Embedded and Ubiquitous Computing, EUC 2013},
keywords = {GPU,cluster,mapreduce},
mendeley-tags = {GPU,cluster,mapreduce},
pages = {1039--1046},
title = {{Design and optimization of a big data computing framework based on CPU/GPU cluster}},
year = {2014}
}
@article{Weinstein2009b,
author = {Weinstein, Marvin and Horn, David},
doi = {10.1103/PhysRevE.80.066117},
file = {:home/chiroptera/Dropbox/mendeley//Weinstein, Horn - 2009 - Dynamic quantum clustering A method for visual exploration of structures in data.pdf:pdf},
issn = {1539-3755},
journal = {Physical Review E},
keywords = {clustering,quantum},
mendeley-tags = {clustering,quantum},
month = dec,
number = {6},
pages = {066117},
title = {{Dynamic quantum clustering: A method for visual exploration of structures in data}},
url = {http://link.aps.org/doi/10.1103/PhysRevE.80.066117},
volume = {80},
year = {2009}
}
@article{Jamsek2009a,
abstract = {The complexity of modern microprocessor design involving billions of transistors at increasingly denser scales creates many challenges particularly in the area of design reliability and predictable yields. Researchers at IBM's Austin Research Lab have increasingly depended on software based simulation of various aspects of the design and manufacturing process to help address these challenges. The computational complexity and sheer scale of these simulations have lead to the exploration of the application of high-performance hybrid computing clusters to accelerate the design process. Currently, the hybrid clusters in use are composed primarily of commodity workstations and servers incorporating commodity NVIDIA-based GPU graphics cards and TESLA GPU computational accelerators. We have also been experimenting with blade clusters composed of both general purpose servers and PowerXcell accelerators leveraging the computational throughput of the Cell processor. In this paper we will detail our experiences with accelerating our workloads on these hybrid cluster platforms. We will discuss our initial approach of combining hybrid runtimes such as CUDA with MPI to address cluster computation. We will also describe a custom cluster hybrid infrastructure we are developing to deal with some of the perceived shortcomings of MPI and other traditional cluster tools when dealing with hybrid computing environments.},
author = {Jamsek, Damir and {Van Hensbergen}, Eric},
doi = {10.1109/CLUSTR.2009.5289126},
file = {:home/chiroptera/Dropbox/mendeley/Jamsek, Van Hensbergen - 2009 - Experiences with hybrid clusters.pdf:pdf},
isbn = {9781424450121},
issn = {15525244},
journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
keywords = {cluster,gpu,mpi},
mendeley-tags = {cluster,gpu,mpi},
pages = {1--4},
title = {{Experiences with hybrid clusters}},
year = {2009}
}
@article{Klipfel1943,
author = {Klipfel, Joel},
file = {:home/chiroptera/Dropbox/mendeley/Klipfel - 1943 - A brief introduction to hilbert space and quantum logic.pdf:pdf},
pages = {1--31},
title = {{A brief introduction to hilbert space and quantum logic}},
year = {1943}
}
@book{Lanzagorta2008,
author = {Lanzagorta, Marco and Uhlmann, Jeffrey},
booktitle = {Synthesis Lectures on Quantum Computing},
doi = {10.2200/S00159ED1V01Y200810QMC002},
file = {:home/chiroptera/Dropbox/mendeley/Lanzagorta, Uhlmann - 2008 - Quantum Computer Science.pdf:pdf},
isbn = {9780511813870},
issn = {1945-9726},
pages = {1--124},
title = {{Quantum Computer Science}},
volume = {1},
year = {2008}
}
@article{Chang2009,
abstract = {Graphics processing units (GPUs) are powerful computational devices tailored towards the needs of the 3-D gaming industry for high-performance, real-time graphics engines. Nvidia Corporation released a new generation of GPUs designed for general-purpose computing in 2006, and it released a GPU programming language called CUDA in 2007. The DNA microarray technology is a high throughput tool for assaying mRNA abundance in cell samples. In data analysis, scientists often apply hierarchical clustering of the genes, where a fundamental operation is to calculate all pairwise distances. If there are n genes, it takes O(n\^{}2) time. In this work, GPUs and the CUDA language are used to calculate pairwise distances. For Manhattan distance, GPU/CUDA achieves a 40 to 90 times speed-up compared to the central processing unit implementation; for Pearson correlation coefficient, the speed-up is 28 to 38 times.},
author = {Chang, Dar J. and Desoky, Ahmed H. and Ouyang, Ming and Rouchka, Eric C.},
doi = {10.1109/SNPD.2009.34},
file = {:home/chiroptera/Dropbox/mendeley/Chang et al. - 2009 - Compute pairwise Manhattan distance and Pearson correlation coefficient of data points with GPU.pdf:pdf},
isbn = {9780769536422},
issn = {0769536425},
journal = {10th ACIS Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2009, In conjunction with IWEA 2009 and WEACR 2009},
keywords = {Hierarchical clustering,Parallel and distributed computation,Similarity and dissimilarity metrics},
pages = {501--506},
title = {{Compute pairwise Manhattan distance and Pearson correlation coefficient of data points with GPU}},
year = {2009}
}
@article{Dianxun2006,
abstract = {A novel generalized quantum particle model (GQPM) is presented for data self-organizing clustering. Using GQPM we transform the data clustering into a stochastic process of equivalence classes of particles under the quantum entanglement relation. The GQPM approach has much faster clustering speed and higher clustering quality than the nonquantum particle model GPM and GCA we proposed before. GQPM is also characterized by the self-organizing clustering and has advantages in terms of the insensitivity to noise, the quality robustness to clustered data, the learning ability, the suitability for high-dimensional multi-shape large-scale data sets. The simulations and comparisons have shown the effectiveness and good performance of the proposed GQPM approach to data clustering},
author = {Dianxun, Shuai and Zhang, Ping and Huang, Liangjun},
doi = {10.1109/ISIE.2006.296087},
file = {:home/chiroptera/Dropbox/mendeley/Dianxun, Zhang, Huang - 2006 - Self-organizing data clustering A novel quantum particle approach.pdf:pdf},
isbn = {1424404975},
journal = {IEEE International Symposium on Industrial Electronics},
number = {2},
pages = {2960--2965},
title = {{Self-organizing data clustering: A novel quantum particle approach}},
volume = {4},
year = {2006}
}
@article{Wu2009a,
abstract = {In this paper, we report our research on using GPUs to accelerate clustering of very large data sets, which are common in today's real world applications. While many published works have shown that GPUs can be used to accelerate various general purpose applications with respectable performance gains, few attempts have been made to tackle very large problems. Our goal here is to investigate if GPUs can be useful accelerators even with very large data sets that cannot fit into GPU’s onboard memory. Using a popular clustering algorithm, K-Means, as an example, our results have been very positive. On a data set with a billion data points, our GPU-accelerated implementation achieved an order of magnitude performance gain over a highly optimized CPU-only version running on 8 cores, and more than two orders of magnitude gain over a popular benchmark, MineBench, running on a single core.},
author = {Wu, Ren and Zhang, Bin and Hsu, Meichun},
doi = {10.1145/1531666.1531668},
file = {:home/chiroptera/Dropbox/mendeley/Wu, Zhang, Hsu - 2009 - Clustering billions of data points using GPUs.pdf:pdf},
isbn = {9781605585574},
journal = {Proceedings of the combined workshops on UnConventional high performance computing workshop plus memory access workshop},
keywords = {accelerator,clustering,data parallelism,data-mining,gpgpu,gpu,graphics processor,many-core,multi-core,parallel algorithm},
mendeley-tags = {clustering,gpu},
pages = {1--5},
title = {{Clustering billions of data points using GPUs}},
url = {http://portal.acm.org/citation.cfm?id=1531666.1531668},
year = {2009}
}
@article{Brassard,
author = {Brassard, Gilles and Centre-ville, Succursale},
file = {:home/chiroptera/Dropbox/mendeley/Brassard, Centre-ville - Unknown - Quantum Clustering Algorithms(2).pdf:pdf;:home/chiroptera/Dropbox/mendeley/Brassard, Centre-ville - Unknown - Quantum Clustering Algorithms.pdf:pdf},
title = {{Quantum Clustering Algorithms}}
}
@article{Varshavsky2005,
author = {Varshavsky, Roy and Linial, Michal and Horn, David},
doi = {10.1007/11576259\_18},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky, Linial, Horn - 2005 - COMPACT A Comparative Package for Clustering Assessment.pdf:pdf},
isbn = {3540297707},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {159--167},
title = {{COMPACT: A Comparative Package for Clustering Assessment}},
volume = {3759 LNCS},
year = {2005}
}
@article{Fang2011,
abstract = {This paper presents a comprehensive performance comparison between CUDA and OpenCL. We have selected 16 benchmarks ranging from synthetic applications to real-world ones. We make an extensive analysis of the performance gaps taking into account programming models, ptimization strategies, architectural details, and underlying compilers. Our results show that, for most applications, CUDA performs at most 30$\backslash$\&\#x025; better than OpenCL. We also show that this difference is due to unfair comparisons: in fact, OpenCL can achieve similar performance to CUDA under a fair comparison. Therefore, we define a fair comparison of the two types of applications, providing guidelines for more potential analyses. We also investigate OpenCL's portability by running the benchmarks on other prevailing platforms with minor modifications. Overall, we conclude that OpenCL's portability does not fundamentally affect its performance, and OpenCL can be a good alternative to CUDA.},
archivePrefix = {arXiv},
arxivId = {1005.2581},
author = {Fang, Jianbin and Varbanescu, Ana Lucia and Sips, Henk},
doi = {10.1109/ICPP.2011.45},
eprint = {1005.2581},
file = {:home/chiroptera/Dropbox/mendeley/Fang, Varbanescu, Sips - 2011 - A comprehensive performance comparison of CUDA and OpenCL.pdf:pdf},
isbn = {9780769545103},
issn = {01903918},
journal = {Proceedings of the International Conference on Parallel Processing},
keywords = {CUDA,OpenCL,Performance Comparison},
pages = {216--225},
title = {{A comprehensive performance comparison of CUDA and OpenCL}},
year = {2011}
}
@article{Wittek2013a,
abstract = {Clustering methods in machine learning may benefit from borrowing metaphors from physics. Dynamic quantum clustering associates a Gaussian wave packet with the multidimensional data points and regards them as eigenfunctions of the Schr??dinger equation. The clustering structure emerges by letting the system evolve and the visual nature of the algorithm has been shown to be useful in a range of applications. Furthermore, the method only uses matrix operations, which readily lend themselves to parallelization. In this paper, we develop an implementation on graphics hardware and investigate how this approach can accelerate the computations. We achieve a speedup of up to two magnitudes over a multicore CPU implementation, which proves that quantum-like methods and acceleration by graphics processing units have a great relevance to machine learning. ?? 2012 Elsevier Inc.},
author = {Wittek, Peter},
doi = {10.1016/j.jcp.2012.08.048},
file = {:home/chiroptera/Dropbox/mendeley/Wittek - 2013 - High-performance dynamic quantum clustering on graphics processors.pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
keywords = {Clustering,GPU computing,Quantum-like learning,Time-dependent schr??dinger equation},
month = jan,
number = {1},
pages = {262--271},
title = {{High-performance dynamic quantum clustering on graphics processors}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0021999112005165},
volume = {233},
year = {2013}
}
@article{Shuai2006a,
abstract = {This paper presents a new generalized quantum particle model for data self-organizing clustering. The stochas- tic motion and collision of quantum particles give rise to a stochastic process of quantum entanglement of particles. The stationary probability distribution over the configuration space of entangled particles results in the optimally clustering solution of the given data set. The quantum particle model has advantages in terms of the insensitivity to noise, the quality robustness to clustered data, the learning ability, and the suitability for high-dimensional multi-shape large-scale data sets. In comparison with the classical version of particle model and the cellular automata, the quantum particle mode has much faster speed and higher quality for clustering. The simulation and comparison show the effectiveness and good performance of the proposed quantum particle approach to data clustering.},
author = {Shuai, Dianxun and Zhang, Bin and Dong, Yumin},
file = {:home/chiroptera/Dropbox/mendeley/Shuai, Zhang, Dong - 2006 - Quantum Particles Model for Data Clustering in Enterprise Computing.pdf:pdf},
isbn = {1424401003},
number = {2},
pages = {4602--4607},
title = {{Quantum Particles Model for Data Clustering in Enterprise Computing}},
year = {2006}
}
@article{Wang2002,
abstract = {Clustering is the process of grouping a set of objects into classes of similar objects. Although definitions of similarity vary from one clustering model to another, in most of these models the concept of similarity is based on distances, e.g., Euclidean distance or cosine distance. In other words, similar objects are required to have close values on at least a set of dimensions. In this paper, we explore a more general type of similarity. Under the pCluster model we proposed, two objects are similar if they exhibit a coherent pattern on a subset of dimensions. For instance, in DNA microarray analysis, the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very much alike. Discovery of such clusters of genes is essential in revealing significant connections in gene regulatory networks. E-commerce applications, such as collaborative filtering, can also benefit from the new model, which captures not only the closeness of values of certain leading indicators but also the closeness of (purchasing, browsing, etc.) patterns exhibited by the customers. Our paper introduces an effective algorithm to detect such clusters, and we perform tests on several real and synthetic data sets to show its effectiveness.},
author = {Wang, H and Wang, H and Wang, W and Wang, W and Yang, H and Yang, H and Yu, P S and Yu, P S},
doi = {10.1145/564691.564737},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2002 - Clustering by pattern similarity in large data sets.pdf:pdf},
isbn = {1581134975},
issn = {07308078},
journal = {2002 ACM SIGMOD international conference on Management of Data},
pages = {394},
title = {{Clustering by pattern similarity in large data sets}},
url = {http://portal.acm.org/citation.cfm?doid=564691.564737},
volume = {2},
year = {2002}
}
@article{Aidos2012,
author = {Aidos, Helena and Fred, Ana},
doi = {10.1016/j.patcog.2011.12.009},
file = {:home/chiroptera/Dropbox/mendeley/Aidos, Fred - 2012 - Statistical modeling of dissimilarity increments for d-dimensional data Application in partitional clustering.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Dissimilarity increments,Gaussian mixture decomposition,Likelihood-ratio test,Minimum description length,Partitional clustering,dissimilarity increments,likelihood-ratio test,partitional clustering},
number = {9},
pages = {3061--3071},
publisher = {Elsevier},
title = {{Statistical modeling of dissimilarity increments for d-dimensional data Application in partitional clustering}},
url = {http://dx.doi.org/10.1016/j.patcog.2011.12.009},
volume = {45},
year = {2012}
}
@article{Kim2011,
abstract = {Hierarchical clustering is an important and powerful but computationally extensive operation. Its complexity motivates the exploration of highly parallel approaches such as Adaptive Resonance Theory (ART). Although ART has been implemented on GPU processors, this paper presents the first hierarchical ART GPU implementation we are aware of. Each ART layer is distributed in the GPU's multiprocessors and is trained simultaneously. The experimental results show that for deep trees, the GPU's performance advantage is significant.},
author = {Kim, Sejun and Wunsch, Donald C.},
doi = {10.1109/IJCNN.2011.6033584},
file = {:home/chiroptera/Dropbox/mendeley/Kim, Wunsch - 2011 - A GPU based parallel Hierarchical Fuzzy ART clustering.pdf:pdf},
isbn = {9781457710865},
issn = {2161-4393},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {GPU,clustering},
mendeley-tags = {GPU,clustering},
number = {5},
pages = {2778--2782},
title = {{A GPU based parallel Hierarchical Fuzzy ART clustering}},
year = {2011}
}
@article{Chen2012,
abstract = {Accelerators and heterogeneous architectures in general, andGPUs in particular, have recently emerged asmajor players in high perfor- mance computing. For many classes of applications, MapReduce has emerged as the framework for easing parallel programming and improving programmer productivity. There have already been sev- eral efforts on implementingMapReduce on GPUs. In this paper, we propose a new implementation of MapReduce for GPUs, which is very effective in utilizing shared memory, a small programmable cache on modern GPUs. The main idea is to use a reduction-based method to execute aMapReduce applica- tion. The reduction-based method allows us to carry out reductions in shared memory. To support a general and efficient implemen- tation, we support the following features: a memory hierarchy for maintaining the reduction object, a multi-group scheme in shared memory to trade-off space requirements and locking overheads, a general and efficient data structure for the reduction object, and an efficient swapping mechanism. We have evaluated our framework with seven commonly used MapReduce applications and compared it with the sequential im- plementations, MapCG, a recent MapReduce implementation on GPUs, and Ji et al.’s work, a recent MapReduce implementation that utilizes shared memory in a different way. The main observa- tions from our experimental results are as follows. For four of the seven applications that can be considered as reduction-intensive ap- plications, our framework has a speedup of between 5 and 200 over MapCG (for large datasets). Similarly, we achieved a speedup of between 2 and 60 over Ji et al.’s work.},
author = {Chen, Linchuan and Agrawal, Gagan},
doi = {10.1145/2287076.2287109},
file = {:home/chiroptera/Dropbox/mendeley/Chen, Agrawal - 2012 - Optimizing MapReduce for GPUs with Effective Shared Memory Usage.pdf:pdf},
isbn = {9781450308052},
journal = {Proceedings of the 21st international symposium on High-Performance Parallel and Distributed Computing (HPDC'12)},
keywords = {MapReduce,gpu,mapreduce,shared memory},
mendeley-tags = {MapReduce,gpu},
pages = {199--210},
title = {{Optimizing MapReduce for GPUs with Effective Shared Memory Usage}},
url = {http://dl.acm.org/citation.cfm?doid=2287076.2287109$\backslash$nhttp://dl.acm.org/citation.cfm?id=2287109},
year = {2012}
}
@article{Woolley,
author = {Woolley, Cliff},
file = {:home/chiroptera/Dropbox/mendeley/Woolley - Unknown - CUDA Overview GPGPU Revolutionizes Computing.pdf:pdf},
title = {{CUDA Overview GPGPU Revolutionizes Computing}}
}
@article{Varshavsky2007,
author = {Varshavsky, Roy and Horn, David and Linial, Michal},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky, Horn, Linial - 2007 - Clustering Algorithms Optimizer A Framework for Large Datasets.pdf:pdf},
isbn = {3540720308},
issn = {03029743},
pages = {85--96},
title = {{Clustering Algorithms Optimizer : A Framework for Large Datasets}},
year = {2007}
}
@article{Cui2011,
abstract = {Analyzing and clustering large scale data set is a complex problem. One explored method of solving this problem borrows from nature, imitating the flocking behavior of birds. One limitation of this method of data clustering is its complexity \$O(n\^{}2)\$. As the number of data and feature dimensions grows, it becomes increasingly difficult to generate results in a reasonable amount of time. In the last few years, the graphics processing unit (GPU) has received attention for its ability to solve highly-parallel and semi-parallel problems much faster than the traditional sequential processor. In this chapter, we have conducted research to exploit this architecture and apply its strengths to the flocking based data clustering problem. Using the CUDA platform from NVIDIA, we developed a Multiple Species Data Flocking implementation to be run on the NVIDIA GPU. Performance gains ranged from \$30\$ to \$60\$ times improvement of the GPU over the CPU implementation.},
author = {Cui, Xiaohui and Charles, Jesse St. and Potok, Thomas E.},
doi = {10.1109/CyberC.2011.44},
file = {:home/chiroptera/Dropbox/mendeley/Cui, Charles, Potok - 2011 - The GPU Enhanced Parallel Computing for Large Scale Data Clustering.pdf:pdf},
isbn = {978-0-7695-4557-8},
journal = {2011 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery},
keywords = {GPU,clustering,flocking,large scale},
pages = {220--225},
title = {{The GPU Enhanced Parallel Computing for Large Scale Data Clustering}},
year = {2011}
}
@article{Neelima2010,
abstract = {With the growth of Graphics Processor (GPU) programmability and processing power, graphics hardware has become a compelling platform for computationally demanding tasks in a wide variety of application domains. This state of art paper gives the technical motivations that underlie GPU computing and describe the hardware and software developments that have led to the recent interest in this field.},
author = {Neelima, B. and Raghavendra, Prakash S.},
doi = {10.1109/ICIINFS.2010.5578685},
file = {:home/chiroptera/Dropbox/mendeley/Neelima, Raghavendra - 2010 - Recent trends in software and hardware for GPGPU computing A comprehensive survey.pdf:pdf},
isbn = {9781424466535},
journal = {2010 5th International Conference on Industrial and Information Systems, ICIIS 2010},
keywords = {GPU computing,Graphics Processor (GPU)},
pages = {319--324},
title = {{Recent trends in software and hardware for GPGPU computing: A comprehensive survey}},
year = {2010}
}
@article{NVIDIACorporation2006,
author = {{NVIDIA Corporation}},
file = {:home/chiroptera/Dropbox/mendeley/NVIDIA Corporation - 2006 - CUDA Programming Model Overview.pdf:pdf},
title = {{CUDA Programming Model Overview}},
year = {2006}
}
@article{Fred2009a,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt.2 Clustering Algorithms.pdf:pdf},
number = {April},
pages = {1--42},
title = {{Tutorial Pt.2: Clustering Algorithms}},
year = {2009}
}
@article{Wiebe2014,
abstract = {We present several quantum algorithms for performing nearest-neighbor learning. At the core of our algorithms are fast and coherent quantum methods for computing distance metrics such as the inner product and Euclidean distance. We prove upper bounds on the number of queries to the input data required to compute these metrics. In the worst case, our quantum algorithms lead to polynomial reductions in query complexity relative to the corresponding classical algorithm. In certain cases, we show exponential or even super-exponential reductions over the classical analog. We study the performance of our quantum nearest-neighbor algorithms on several real-world binary classification tasks and find that the classification accuracy is competitive with classical methods.},
archivePrefix = {arXiv},
arxivId = {1401.2142},
author = {Wiebe, Nathan and Kapoor, Ashish and Svore, Krysta},
eprint = {1401.2142},
file = {:home/chiroptera/Dropbox/mendeley/Wiebe, Kapoor, Svore - 2014 - Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning.pdf:pdf},
pages = {31},
title = {{Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning}},
url = {http://arxiv.org/abs/1401.2142},
year = {2014}
}
@article{Weinstein2009,
abstract = {A given set of data points in some feature space may be associated with a Schr\"{o}dinger equation whose potential is determined by the data. This is known to lead to good clustering solutions. Here we extend this approach into a full-fledged dynamical scheme using a time-dependent Schr\"{o}dinger equation. Moreover, we approximate this Hamiltonian formalism by a truncated calculation within a set of Gaussian wave functions (coherent states) centered around the original points. This allows for analytic evaluation of the time evolution of all such states opening up the possibility of exploration of relationships among data points through observation of varying dynamical distances among points and convergence of points into clusters. This formalism may be further supplemented by preprocessing such as dimensional reduction through singular-value decomposition or feature filtering.},
annote = {From Duplicate 2 (Dynamic quantum clustering: a method for visual exploration of structures in data - Weinstein, Marvin; Horn, David)},
archivePrefix = {arXiv},
arxivId = {arXiv:0908.2644v1},
author = {Weinstein, Marvin and Horn, David},
doi = {10.1103/PhysRevE.80.066117},
eprint = {arXiv:0908.2644v1},
file = {:home/chiroptera/Dropbox/mendeley//Weinstein, Horn - 2009 - Dynamic quantum clustering A method for visual exploration of structures in data.pdf:pdf},
issn = {1539-3755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
keywords = {clustering,quantum},
mendeley-tags = {clustering,quantum},
month = dec,
number = {6},
pages = {1--15},
title = {{Dynamic quantum clustering: a method for visual exploration of structures in data}},
url = {http://link.aps.org/doi/10.1103/PhysRevE.80.066117},
volume = {80},
year = {2009}
}
@article{Asanovic2006,
abstract = {The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation. A multidisciplinary group of Berkeley researchers met nearly two years to discuss this change. Our view is that this evolutionary approach to parallel hardware and software may work from 2 or 8 processor systems, but is likely to face diminishing returns as 16 and 32 processor systems are realized, just as returns fell with greater instruction-level parallelism. We believe that much can be learned by examining the success of parallelism at the extremes of the computing spectrum, namely embedded computing and high performance computing. This led us to frame the parallel landscape with seven questions, and to recommend the following: The overarching goal should be to make it easy to write programs that execute efficiently on highly parallel computing systems The target should be 1000s of cores per chip, as these chips are built from processing elements that are the most efficient in MIPS (Million Instructions per Second) per watt, MIPS per area of silicon, and MIPS per development dollar. Instead of traditional benchmarks, use 13 "Dwarfs" to design and evaluate parallel programming models and architectures. (A dwarf is an algorithmic method that captures a pattern of computation and communication.) "Autotuners" should play a larger role than conventional compilers in translating parallel programs. To maximize programmer productivity, future programming models must be more human-centric than the conventional focus on hardware or applications. To be successful, programming models should be independent of the number of processors. To maximize application efficiency, programming models should support a wide range of data types and successful models of parallelism: task-level parallelism, word-level parallelism, and bit-level parallelism. Architects should not include features that significantly affect performance or energy if programmers cannot accurately measure their impact via performance counters and energy counters. Traditional operating systems will be deconstructed and operating system functionality will be orchestrated using libraries and virtual machines. To explore the design space rapidly, use system emulators based on Field Programmable Gate Arrays (FPGAs) that are highly scalable and low cost. Since real world applications are naturally parallel and hardware is naturally parallel, what we need is a programming model, system software, and a supporting architecture that are naturally parallel. Researchers have the rare opportunity to re-invent these cornerstones of computing, provided they simplify the efficient programming of highly parallel systems.},
author = {Asanovic, Krste and Catanzaro, Bryan Christopher and Patterson, David a and Yelick, Katherine a},
doi = {10.1145/1562764.1562783},
file = {:home/chiroptera/Dropbox/mendeley/Asanovic et al. - 2006 - The Landscape of Parallel Computing Research A View from Berkeley.pdf:pdf},
isbn = {UCB/EECS-2006-183},
issn = {00010782},
journal = {EECS Department University of California Berkeley Tech Rep UCBEECS2006183},
pages = {19},
pmid = {8429457},
title = {{The Landscape of Parallel Computing Research : A View from Berkeley}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.8705\&amp;rep=rep1\&amp;type=pdf},
volume = {18},
year = {2006}
}
@article{Mechanics,
author = {Mechanics, Quantum},
file = {:home/chiroptera/Dropbox/mendeley/Mechanics - Unknown - Strange Bedfellows.pdf:pdf},
title = {{Strange Bedfellows:}}
}
@book{Moler2008,
abstract = {This chapter is about eigenvalues and singular values of matrices. Computational algorithms and sensitivity to perturbations are both discussed.},
author = {Moler, Cleve},
booktitle = {Numerical Computing with MATLAB, Revised Reprint},
doi = {10.1016/0377-0427(90)90025-U},
file = {:home/chiroptera/Dropbox/mendeley/Moler - 2008 - Eigenvalues and Singular Values.pdf:pdf},
isbn = {978-0-898716-60-3},
issn = {1550-2376},
pages = {39},
pmid = {21230651},
title = {{Eigenvalues and Singular Values}},
url = {http://www.mathworks.nl/moler/chapters.html},
year = {2008}
}
@article{Liu2010,
abstract = {By reviewing the original INIQGA algorithm, an improved algorithm (IINIQGA) is put forward by revising the lookup table. In addition, By introducing the variable angle-distance rotation method into the update Q(t) procedure, a novel quantum-inspired evolutionary algorithm, QEA-VAR, was proposed. Compared with previous algorithms, our update Q(t) procedure is more simple and feasible. Finally, the corresponding experiments on the 0-1 knapsack problem were carried out, and the results show that our improvement is efficient, and comparing with IINIQGA, QEA, and CGA, QEA-VAR has a faster convergence and better profits than other algorithms.},
author = {Liu, Wenjie and Chen, Hanwu and Yan, Qiaoqiao and Liu, Zhihao and Xu, Juan and Zheng, Yu},
doi = {10.1109/CEC.2010.5586281},
file = {:home/chiroptera/Dropbox/mendeley/Liu et al. - 2010 - A novel quantum-inspired evolutionary algorithm based on variable angle-distance rotation.pdf:pdf},
isbn = {9781424469109},
journal = {2010 IEEE World Congress on Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation, CEC 2010},
keywords = {0/1 knapsack problem,quantum-inspired evolutionary algorithm,qubit,variable angle-distance,variable angle-distance rotation},
mendeley-tags = {qubit,variable angle-distance},
title = {{A novel quantum-inspired evolutionary algorithm based on variable angle-distance rotation}},
year = {2010}
}
@article{Evans,
author = {Evans, Michael R},
file = {:home/chiroptera/Dropbox/mendeley/Evans - Unknown - Spatial Big Data Case Studies on Volume , Velocity , and Variety What is Spatial Big Data.pdf:pdf},
pages = {1--16},
title = {{Spatial Big Data : Case Studies on Volume , Velocity , and Variety What is Spatial Big Data ?}}
}
@article{Xiao2008,
abstract = {The conventional k-means clustering algorithm must know the number of clusters in advance and the clustering result is sensitive to the selection of the initial cluster centroids. The sensitivity may make the algorithm converge to the local optima. This paper proposes an improved k-means clustering algorithm based on quantum-inspired genetic algorithm (KMQGA). In KMQGA, Q-bit based representation is employed for exploration and exploitation in discrete 0-1 hyperspace by using rotation operation of quantum gate as well as three genetic algorithm operations (selection, crossover and mutation) of Q-bit. Without knowing the exact number of clusters beforehand, the KMQGA can get the optimal number of clusters as well as providing the optimal cluster centroids after several iterations of the four operations (selection, crossover, mutation, and rotation). The simulated datasets and the real datasets are used to validate KMQGA and to compare KMQGA with an improved k-means clustering algorithm based on the famous variable string length genetic algorithm (KMVGA) respectively. The experimental results show that KMQGA is promising and the effectiveness and the search quality of KMQGA is better than those of KMVGA.},
author = {Xiao, Jing Xiao Jing and Yan, YuPing Yan YuPing and Lin, Ying Lin Ying and Yuan, Ling Yuan Ling and Zhang, Jun Zhang Jun},
journal = {2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)},
title = {{A Quantum-inspired Genetic Algorithm for data clustering}},
year = {2008}
}
@article{Mechanicsa,
author = {Mechanics, Quantum},
file = {:home/chiroptera/Dropbox/mendeley/Mechanics - Unknown - Strange Bedfellows.pdf:pdf},
title = {{Strange Bedfellows:}}
}
@article{Rosenbaum2011,
abstract = {We consider a generalization of the standard oracle model in which the oracle acts on the target with a permutation which is selected according to internal random coins. We show new exponential quantum speedups which may be obtained over classical algorithms in this oracle model. Even stronger, we describe several problems which are impossible to solve classically but can be solved by a quantum algorithm using a single query; we show that such infinity-vs-one separations between classical and quantum query complexities can be constructed from any separation between classical and quantum query complexities (in the unbounded-error regime).   We also give conditions to determine when oracle problems---either in the standard model, or in any of the generalizations we consider---cannot be solved with success probability better than random guessing would achieve. In the oracle model with internal randomness where the goal is to gain any nonzero advantage over guessing, we prove (roughly speaking) that (k) quantum queries are equivalent in power to (2k) classical queries, thus extending results of Meyer and Pommersheim, and Montanaro, Nishimura and Raymond.},
archivePrefix = {arXiv},
arxivId = {1111.1462v1},
author = {Rosenbaum, David and Harrow, Aram W.},
eprint = {1111.1462v1},
file = {:home/chiroptera/Dropbox/mendeley/Rosenbaum, Harrow - 2011 - Uselessness for an Oracle Model with Internal Randomness.pdf:pdf},
issn = {15337146},
journal = {arXiv:1111.1462},
pages = {1--23},
title = {{Uselessness for an Oracle Model with Internal Randomness}},
year = {2011}
}
@article{Owens2008,
abstract = {The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in general-purpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.},
annote = {Excellent survey (albeit old) of GPU, GPU architecture, GPU programming flow, GPGPU, examples and trends.},
author = {Owens, Jd and Houston, M},
doi = {10.1109/JPROC.2008.917757},
file = {:home/chiroptera/Dropbox/mendeley/Owens, Houston - 2008 - GPU computing.pdf:pdf},
isbn = {0769527000},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {gpgpu,gpu,survey,topnotch},
mendeley-tags = {gpu,survey,gpgpu,topnotch},
number = {5},
pages = {879 -- 899},
pmid = {21776805},
title = {{GPU computing}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4490127},
volume = {96},
year = {2008}
}
@article{Grossman2013,
abstract = {As the scale of high performance computing systems grows, three main challenges arise: the programmability, reliability, and energy efficiency of those systems. Accomplishing all three without sacrificing performance requires a rethinking of legacy distributed programming models and homogeneous clusters. In this work, we integrate Hadoop MapReduce with OpenCL to enable the use of heterogeneous processors in a distributed system. We do this by exploiting the implicit data- parallelism of mappers and reducers in a MapReduce system. Combining Hadoop and OpenCL provides 1) an easy-to-learn and flexible application programming interface in a high level and popular programming language, 2) the reliability guarantees and distributed filesystem of Hadoop, and 3) the low power consumption and performance acceleration of heterogeneous processors. This paper presents HadoopCL: an extension to Hadoop which supports execution of user-written Java kernels on heterogeneous devices, optimizes communication through asynchronous transfers and dedicated I/O threads, automatically generates OpenCL kernels from Java bytecode using the open source tool APARAPI, and achieves nearly 3x overall speedup and better than 55x speedup of the computational sections for example MapReduce applications, relative to Hadoop.},
author = {Grossman, Max and Breternitz, Mauricio and Sarkar, Vivek},
doi = {10.1109/IPDPSW.2013.246},
file = {:home/chiroptera/Dropbox/mendeley/Grossman, Breternitz, Sarkar - 2013 - HadoopCL MapReduce on distributed heterogeneous platforms through seamless integration of hadoop a.pdf:pdf},
isbn = {978-0-7695-4979-8},
journal = {Proceedings - IEEE 27th International Parallel and Distributed Processing Symposium Workshops and PhD Forum, IPDPSW 2013},
keywords = {GPGPU,Hadoop,OpenCL,heterogeneous,multicore},
pages = {1918--1927},
title = {{HadoopCL: MapReduce on distributed heterogeneous platforms through seamless integration of hadoop and OpenCL}},
year = {2013}
}
@article{Stovall2014,
author = {Stovall, Thomas and Kockara, Sinan and Avci, Recep},
doi = {10.1109/TPDS.2014.2374607},
file = {:home/chiroptera/Dropbox/mendeley/Stovall, Kockara, Avci - 2014 - GPUSCAN GPU-based Parallel Structural Clustering Algorithm for Networks.pdf:pdf},
issn = {1045-9219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {clustering,gpu},
mendeley-tags = {clustering,gpu},
number = {c},
pages = {1--1},
title = {{GPUSCAN: GPU-based Parallel Structural Clustering Algorithm for Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6967853},
volume = {9219},
year = {2014}
}
@article{Lee2012,
abstract = {A prominent parallel data processing tool MapReduce is gaining significant momentum from both industry and academia as the volume of data to analyze grows rapidly. While MapReduce is used in many areas where massive data analysis is required, there are still debates on its performance, efficiency per node, and simple abstraction. This survey intends to assist the database and open source communities in understanding various technical aspects of the MapReduce framework. In this survey, we characterize the MapReduce framework and discuss its inherent pros and cons. We then introduce its optimization strategies reported in the recent literature. We also discuss the open issues and challenges raised on parallel data analysis with MapReduce.},
author = {Lee, Kyong-Ha and Lee, Yoon-Joon and Choi, Hyunsik and Chung, Yon Dohn and Moon, Bongki},
doi = {10.1145/2094114.2094118},
file = {:home/chiroptera/Dropbox/mendeley/Lee et al. - 2012 - Parallel data processing with MapReduce.pdf:pdf},
isbn = {0163-5808},
issn = {01635808},
journal = {ACM SIGMOD Record},
keywords = {MapReduce,clusters,distributed computing,hadoop,ing,mapreduce,parallel data process-},
mendeley-tags = {MapReduce},
number = {4},
pages = {11},
title = {{Parallel data processing with MapReduce}},
volume = {40},
year = {2012}
}
@article{Raymond2013,
author = {Raymond, The and Sackler, Beverly},
file = {:home/chiroptera/Dropbox/mendeley/Raymond, Sackler - 2013 - Quantum Clustering of Large Data Sets.pdf:pdf},
title = {{Quantum Clustering of Large Data Sets}},
year = {2013}
}
@article{Manju2014,
abstract = {This paper makes an exhaustive survey of various applications of Quantum inspired computational intelligence (QCI) techniques proposed till date. Definition, categorization and motivation for QCI techniques are stated clearly. Major Drawbacks and challenges are discussed. The significance of this work is that it presents an overview on applications of QCI in solving various problems in engineering, which will be very much useful for researchers on Quantum computing in exploring this upcoming and young discipline.[PUBLICATION ABSTRACT]},
author = {Manju, a. and Nigam, M. J.},
doi = {10.1007/s10462-012-9330-6},
file = {:home/chiroptera/Dropbox/mendeley/Manju, Nigam - 2014 - Applications of quantum inspired computational intelligence A survey.pdf:pdf},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Computational intelligence,Quantum computing,Quantum mechanics},
pages = {79--156},
title = {{Applications of quantum inspired computational intelligence: A survey}},
volume = {42},
year = {2014}
}
@article{Guo2013,
author = {Guo, Yiru and Liu, Weiguo and Voss, Gerrit and Mueller-Wittig, Wolfgang},
doi = {10.1109/HPCC.and.EUC.2013.88},
file = {:home/chiroptera/Dropbox/mendeley/Guo et al. - 2013 - GCMR A GPU Cluster-Based MapReduce Framework for Large-Scale Data Processing.pdf:pdf},
isbn = {978-0-7695-5088-6},
journal = {2013 IEEE 10th International Conference on High Performance Computing and Communications \& 2013 IEEE International Conference on Embedded and Ubiquitous Computing},
keywords = {-mapreduce,1,15,3,8,9,MapReduce,and bioinformatics,big data,cluster,cuda,database operations,due to the continuing,gpu,gpu cluster,image processing,mpi,rapid growth of data,size in},
mendeley-tags = {MapReduce,big data,cluster,gpu},
pages = {580--586},
title = {{GCMR: A GPU Cluster-Based MapReduce Framework for Large-Scale Data Processing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6831970},
year = {2013}
}
@article{Han2000,
abstract = {This paper proposes a novel evolutionary computing method called a
genetic quantum algorithm (GQA). GQA is based on the concept and
principles of quantum computing such as qubits and superposition of
states. Instead of binary, numeric, or symbolic representation, by
adopting qubit chromosome as a representation GQA can represent a linear
superposition of solutions due to its probabilistic representation. As
genetic operators, quantum gates are employed for the search of the best
solution. Rapid convergence and good global search capability
characterize the performance of GQA. The effectiveness and the
applicability of GQA are demonstrated by experimental results on the
knapsack problem, which is a well-known combinatorial optimization
problem. The results show that GQA is superior to other genetic
algorithms using penalty functions, repair methods and decoders},
author = {Han, Kuk-Hyun Han Kuk-Hyun and Kim, Jong-Hwan Kim Jong-Hwan},
doi = {10.1109/CEC.2000.870809},
file = {:home/chiroptera/Dropbox/mendeley/Han, Kim - 2000 - Genetic quantum algorithm and its application to combinatorial optimization problem.pdf:pdf},
isbn = {0-7803-6375-2},
journal = {Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)},
title = {{Genetic quantum algorithm and its application to combinatorial
optimization problem}},
volume = {2},
year = {2000}
}
@article{Shuai2006,
abstract = {This paper presents a generalized quantum particle model to greatly quicken and improve data clustering. The proposed model uses the random dynamics and quantum entanglement of quantum particles on a particle array. In comparison with classical nonquantum methods, the quantum particle model not only clusters much faster, but also has better clustering quality for multi-shape multi-distribution high-dimensional large-scale data sets with noise. The simulations and comparisons show the effectiveness of the quantum particle model},
author = {Shuai, Dianxun and Lu, Cunpai and Zhang, Bin},
doi = {10.1109/COMPSAC.2006.131},
file = {:home/chiroptera/Dropbox/mendeley/Shuai, Lu, Zhang - 2006 - Entanglement partitioning of quantum particles for data clustering.pdf:pdf},
isbn = {0769526551},
issn = {07303157},
journal = {Proceedings - International Computer Software and Applications Conference},
number = {2},
pages = {285--290},
title = {{Entanglement partitioning of quantum particles for data clustering}},
volume = {2},
year = {2006}
}
@article{Rajaraman2011a,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
author = {Rajaraman, Anand and Ullman, Jeffrey D},
doi = {10.1017/CBO9781139058452},
file = {:home/chiroptera/Dropbox/mendeley/Rajaraman, Ullman - 2011 - Mining of Massive Datasets(2).pdf:pdf},
isbn = {9781139058452},
issn = {01420615},
journal = {Lecture Notes for Stanford CS345A Web Mining},
pages = {328},
title = {{Mining of Massive Datasets}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139058452},
volume = {67},
year = {2011}
}
@article{Weinstein2013,
abstract = {How does one search for a needle in a multi-dimensional haystack without knowing what a needle is and without knowing if there is one in the haystack? This kind of problem requires a paradigm shift - away from hypothesis driven searches of the data - towards a methodology that lets the data speak for itself. Dynamic Quantum Clustering (DQC) is such a methodology. DQC is a powerful visual method that works with big, high-dimensional data. It exploits variations of the density of the data (in feature space) and unearths subsets of the data that exhibit correlations among all the measured variables. The outcome of a DQC analysis is a movie that shows how and why sets of data-points are eventually classified as members of simple clusters or as members of - what we call - extended structures. This allows DQC to be successfully used in a non-conventional exploratory mode where one searches data for unexpected information without the need to model the data. We show how this works for big, complex, real-world datasets that come from five distinct fields: i.e., x-ray nano-chemistry, condensed matter, biology, seismology and finance. These studies show how DQC excels at uncovering unexpected, small - but meaningful - subsets of the data that contain important information. We also establish an important new result: namely, that big, complex datasets often contain interesting structures that will be missed by many conventional clustering techniques. Experience shows that these structures appear frequently enough that it is crucial to know they can exist, and that when they do, they encode important hidden information. In short, we not only demonstrate that DQC can be flexibly applied to datasets that present significantly different challenges, we also show how a simple analysis can be used to look for the needle in the haystack, determine what it is, and find what this means.},
archivePrefix = {arXiv},
arxivId = {1310.2700},
author = {Weinstein, M and Meirer, F and Hume, A},
eprint = {1310.2700},
file = {:home/chiroptera/Dropbox/mendeley/Weinstein, Meirer, Hume - 2013 - Analyzing Big Data with Dynamic Quantum Clustering.pdf:pdf},
journal = {arXiv preprint arXiv: \ldots},
pages = {1--37},
title = {{Analyzing Big Data with Dynamic Quantum Clustering}},
url = {http://arxiv.org/abs/1310.2700 http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:No+Title\#0},
year = {2013}
}
@book{Wittek,
author = {Wittek, Peter},
file = {:home/chiroptera/Dropbox/mendeley/Wittek - Unknown - Quantum Machine Learning What Quantum Computing Means to Data Mining.pdf:pdf},
title = {{Quantum Machine Learning: What Quantum Computing Means to Data Mining}}
}
@article{Solan2005,
abstract = {We address the problem, fundamental to linguistics, bioinformatics, and certain other disciplines, of using corpora of raw symbolic sequential data to infer underlying rules that govern their production. Given a corpus of strings (such as text, transcribed speech, chromosome or protein sequence data, sheet music, etc.), our unsupervised algorithm recursively distills from it hierarchically structured patterns. The adios (automatic distillation of structure) algorithm relies on a statistical method for pattern extraction and on structured generalization, two processes that have been implicated in language acquisition. It has been evaluated on artificial context-free grammars with thousands of rules, on natural languages as diverse as English and Chinese, and on protein data correlating sequence with function. This unsupervised algorithm is capable of learning complex syntax, generating grammatical novel sentences, and proving useful in other fields that call for structure discovery from raw data, such as bioinformatics.},
author = {Solan, Zach and Horn, David and Ruppin, Eytan and Edelman, Shimon},
doi = {10.1073/pnas.0409746102},
file = {:home/chiroptera/Dropbox/mendeley/Solan et al. - 2005 - Unsupervised learning of natural languages.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {33},
pages = {11629--11634},
pmid = {16087885},
title = {{Unsupervised learning of natural languages.}},
volume = {102},
year = {2005}
}
@article{Li2007,
author = {Li, Zhi-hua and Wang, Shi-tong and Wuxi, Jiangsu},
file = {:home/chiroptera/Dropbox/mendeley/Li, Wang, Wuxi - 2007 - Quantum TheoryThe Unified Framework for FCM and QC Algorithm.pdf:pdf},
isbn = {1424410665},
keywords = {2-,algorithm,function,implemented by quantum clustering,interpretation,qc,quantum clustering,quantum potential,quantum theory,wave},
pages = {2--4},
title = {{Quantum Theory:The Unified Framework for FCM and QC Algorithm}},
year = {2007}
}
@article{Horn2001,
abstract = {We discuss novel clustering methods that are based on mapping data points to a Hilbert space by means of a Gaussian kernel. The first method, support vector clustering (SVC), searches for the smallest sphere enclosing data images in Hilbert space. The second, quantum clustering (QC), searches for the minima of a potential function defined in such a Hilbert space. In SVC, the minimal sphere, when mapped back to data space, separates into several components, each enclosing a separate cluster of points. A soft margin constant helps in coping with outliers and overlapping clusters. In QC, minima of the potential define cluster centers, and equipotential surfaces are used to construct the clusters. In both methods, the width of the Gaussian kernel controls the scale at which the data are probed for cluster formations. We demonstrate the performance of the algorithms on several data sets.},
author = {Horn, David},
doi = {10.1016/S0378-4371(01)00442-3},
file = {:home/chiroptera/Dropbox/mendeley/Horn - 2001 - Clustering via Hilbert space.pdf:pdf},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
keywords = {Clustering,Hilbert space,Kernel methods,Scale-space clustering,Schr\"{o}dinger equation,Support vector clustering},
month = dec,
number = {1-4},
pages = {70--79},
title = {{Clustering via Hilbert space}},
url = {http://www.sciencedirect.com/science/article/pii/S0378437101004423},
volume = {302},
year = {2001}
}
@article{Owens2006,
author = {Owens, John D and Luebke, David and Govindraju, Naga and Harris, Mark and Kruger, Jens and Lefohn, Aaron E and Purcell, Timothy J},
file = {:home/chiroptera/Dropbox/mendeley/Owens et al. - 2006 - A Survey of General Purpose Computation on Graphics Hardware.pdf:pdf},
journal = {Computer Graphics Forum},
number = {August},
pages = {21--51},
title = {{A Survey of General Purpose Computation on Graphics Hardware}},
url = {http://www.cs.virginia.edu/papers/ASurveyofGeneralPurposeComputationonGraphicsHardware.pdf},
year = {2006}
}
@article{Lopes2011,
abstract = {Graphics Processing Units (GPUs) placed at our disposal an unprecedented computational-power, largely surpassing the performance of cutting-edge CPUs (Central Processing Units). The high-parallelism inherent to the GPU makes this device especially well-suited to address Machine Learning (ML) problems with prohibitively computational intensive tasks. Nevertheless, few ML algorithms have been implemented on the GPU and most are not openly shared, posing difficulties for researchers and engineers aiming to develop GPU-based systems. To mitigate this problem, we propose the creation of an open source GPU Machine Learning Library (GPUMLib) that aims to provide the building blocks for the development of efficient GPU ML software. Experimental results on benchmark datasets show that the algorithms already implemented yield significant time savings over the CPU counterparts.},
author = {Lopes, Noel and Ribeiro, Bernardete},
file = {:home/chiroptera/Dropbox/mendeley/Lopes, Ribeiro - 2011 - GPUMLib An efficient open-source GPU machine learning library.pdf:pdf},
journal = {\ldots Journal of Computer Information Systems and \ldots},
keywords = {gpu,gpu computing,machine learning,machine learning algorithms},
mendeley-tags = {gpu,machine learning},
pages = {355--362},
title = {{GPUMLib: An efficient open-source GPU machine learning library}},
url = {http://www.ece.neu.edu/groups/nucar/NUCARTALKS/GPUMLib.pdf},
volume = {3},
year = {2011}
}
@misc{Guerra2009,
abstract = {The integration of usable and flexible analysis support in modelling environments is a key success factor in Model-Driven Development. In this paradigm, models are the core asset from which code is automatically generated, and thus ensuring model correctness is a fundamental quality control activity. For this purpose, a common approach is to transform the system models into formal semantic domains for verification. However, if the analysis results are not shown in a proper way to the end-user (e.g. in terms of the original language) they may become useless. In this paper we present a novel DSVL called BaVeL that facilitates the flexible annotation of verification results obtained in semantic domains to different formats, including the context of the original language. BaVeL is used in combination with a consistency framework, providing support for all steps in a verification process: acquisition of additional input data, transformation of the system models into semantic domains, verification, and flexible annotation of analysis results. The approach has been validated analytically by the cognitive dimensions framework, and empirically by its implementation and application to several DSVLs. Here we present a case study of a notation in the area of Digital Libraries, where the analysis is performed by transformations into Petri nets and a process algebra. © 2008 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Guerra, Esther and de Lara, Juan and Malizia, Alessio and D\'{\i}az, Paloma},
booktitle = {Information and Software Technology},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {0402594v3},
file = {:home/chiroptera/Dropbox/mendeley/Guerra et al. - 2009 - Supporting user-oriented analysis for multi-view domain-specific visual languages.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
keywords = {Back-annotation,Consistency,Domain-specific visual languages,Formal methods,Model transformation,Modelling environments},
number = {4},
pages = {769--784},
primaryClass = {arXiv:cond-mat},
title = {{Supporting user-oriented analysis for multi-view domain-specific visual languages}},
volume = {51},
year = {2009}
}
@article{Wang2007,
abstract = {A new hybrid fuzzy clustering algorithm that incorporates the fuzzy c-means (FCM) into the quantum-behaved particle swarm optimization (QPSO) algorithm is proposed in this paper (QPSO+FCM). The QPSO has less parameters and higher convergent capability of the global optimizing than particle swarm optimization algorithm (PSO). So the iteration algorithm is replaced by the QPSO based on the gradient descent of FCM, which makes the algorithm have a strong global searching capacity and avoids the local minimum problems of FCM and in a large degree avoids depending on the initialization values. This paper also investigates the ability of FCM algorithm, PSO+FCM algorithm and GA+FCM algorithm with Iris testing data and Wine testing data. The simulation result proves that compared with other algorithms, the new algorithm not only has the favorable convergence but also has been obviously improved the clustering effect.},
author = {Wang, Hao Wang Hao and Yang, Shiqin Yang Shiqin and Xu, Wenbo Xu Wenbo and Sun, Jun Sun Jun},
doi = {10.1109/FSKD.2007.507},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2007 - Scalability of Hybrid Fuzzy C-Means Algorithm Based on Quantum-Behaved PSO.pdf:pdf},
isbn = {978-0-7695-2874-8},
journal = {Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)},
number = {Fskd},
title = {{Scalability of Hybrid Fuzzy C-Means Algorithm Based on Quantum-Behaved PSO}},
volume = {2},
year = {2007}
}
@article{Zhang2008,
abstract = {The principle advantage and shortcoming of quantum clustering algorithm (QC) is analyzed. Based on its shortcomings, an improved algorithm - exponent distance-based quantum clustering algorithm (EQDC) is produced. It improved the iterative procedure of QC algorithm and used exponent distance formula to measure the distance between data points and the cluster centers. Experimental results demonstrate that the cluster accuracy of EDQC outperforms that of QC, and the exponent distance formula used in the clustering process performs better than the Euclidean distance in data preprocessing. What's more, the IRIS dataset can come to a satisfied result without preprocessing.},
author = {Zhang, Yao and Wang, Peng and Chen, Gao Yun and Chen, Dong Dong and Ding, Rui and Zhang, Yan},
doi = {10.1109/KAMW.2008.4810518},
file = {:home/chiroptera/Dropbox/mendeley/Zhang et al. - 2008 - Quantum clustering algorithm based on exponent measuring distance.pdf:pdf},
isbn = {9781424435296},
journal = {2008 IEEE International Symposium on Knowledge Acquisition and Modeling Workshop Proceedings, KAM 2008},
keywords = {Clustering accuracy,Data preprocessing,Exponent distance-based quantum clustering algorit,Measuring formula,Quantum clustering algorithm,Quantum potential},
number = {1},
pages = {436--439},
title = {{Quantum clustering algorithm based on exponent measuring distance}},
year = {2008}
}
@article{Blekas2007a,
abstract = {Given a data set, a dynamical procedure is applied to the data points in order to shrink and separate, possibly overlapping clusters. Namely, Newton's equations of motion are employed to concentrate the data points around their cluster centers, using an attractive potential, constructed specially for this purpose. During this process, important information is gathered concerning the spread of each cluster. In succession this information is used to create an objective function that maps each cluster to a local maximum. Global optimization is then used to retrieve the positions of the maxima that correspond to the locations of the cluster centers. Further refinement is achieved by applying the EM-algorithm to a Gaussian mixture model whose construction and initialization is based on the acquired information. To assess the effectiveness of our method, we have conducted experiments on a plethora of benchmark data sets. In addition we have compared its performance against four clustering techniques that are well established in the literature. [All rights reserved Elsevier]},
author = {Blekas, K. and Lagaris, I.E.},
doi = {10.1016/j.patcog.2006.07.012},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Lagaris - 2007 - Newtonian clustering An approach based on molecular dynamics and global optimization(2).pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {clustering,global optimization,molecular dynamics,order statistics},
pages = {1734--1744},
title = {{Newtonian clustering: An approach based on molecular dynamics and global optimization}},
volume = {40},
year = {2007}
}
@article{Li2008,
abstract = {The enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum random walk (QRW) with the problem of data clustering, and develop two clustering algorithms based on the one dimensional QRW. Then, the probability distributions on the positions induced by QRW in these algorithms are investigated, which also indicates the possibility of obtaining better results. Consequently, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms are of fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.},
archivePrefix = {arXiv},
arxivId = {0812.1357},
author = {Li, Qiang and He, Yan and Jiang, Jing-ping},
eprint = {0812.1357},
file = {:home/chiroptera/Dropbox/mendeley/Li, He, Jiang - 2008 - A Novel Clustering Algorithm Based on Quantum Random Walk.pdf:pdf},
keywords = {data clustering,quantum compu-,quantum game,tation,unsupervised learning},
pages = {14},
title = {{A Novel Clustering Algorithm Based on Quantum Random Walk}},
url = {http://arxiv.org/abs/0812.1357},
year = {2008}
}
@article{Kumar2004,
author = {Kumar, Nimit and Behera, Laxmidhar},
doi = {10.1023/B:NEPL.0000039429.89321.07},
issn = {1370-4621},
journal = {Neural Processing Letters},
keywords = {clustering,quantum},
mendeley-tags = {clustering,quantum},
month = aug,
number = {1},
pages = {11--22},
title = {{Visual–Motor Coordination Using a Quantum Clustering Based Neural Control Scheme}},
url = {http://link.springer.com/10.1023/B:NEPL.0000039429.89321.07},
volume = {20},
year = {2004}
}
@article{Zentall2014,
abstract = {Learning by rats was facilitated when response-relevant cues were provided by other rats; learning increased as a fzlnction of number of cues provided. These results suggest that rats can learn by imitation. Learning by rats that observed conspeclifics not esnitting response-relevant cues was retarded compared to learning by rats that did not observe conspecifics. This indicates that a conspecific's presence can also inhibit learning, a result consistent with socia facilitation theory.},
author = {Zentall, Thomas R. and Levine, John M.},
file = {:home/chiroptera/Dropbox/mendeley/Zentall, Levine - 2014 - American Association for the Advancement of Science.pdf:pdf},
journal = {Science},
number = {5278},
pages = {1220--1221},
title = {{American Association for the Advancement of Science}},
volume = {178},
year = {2014}
}
@article{El-sherbiny,
abstract = {Quantum computing proved good results and performance when applied to solving optimization problems. This paper proposes a quantum crossover-based quantum genetic algorithm (QXQGA) for solving non-linear programming. Due to the significant role of mutation function on the QXQGA's quality, a number of quantum crossover and quantum mutation operators are presented for improving the capabilities of searching, overcoming premature convergence, and keeping diversity of population. For calibrating the QXQGA, the quantum crossover and mutation operators are evaluated using relative percentage deviation for selecting the best combination. In addition, a set of non-linear problems is used as benchmark functions to illustrate the effectiveness of optimizing the complexities with different dimensions, and the performance of the proposed QXQGA algorithm is compared with the quantum inspired evolutionary algorithm to demonstrate its superiority.},
author = {El-sherbiny, Mahmoud M},
file = {:home/chiroptera/Dropbox/mendeley/El-sherbiny - Unknown - Quantum Crossover Based Quantum Genetic Algorithm for Solving Non-linear Programming Quantum Rotational gate.pdf:pdf},
keywords = {-component,quantum},
title = {{Quantum Crossover Based Quantum Genetic Algorithm for Solving Non-linear Programming Quantum Rotational gate}}
}
@misc{Weinstein,
author = {Weinstein, Marvin},
title = {{DQClib: A Maple package implementing Dynamic Quantum Clustering (DQC)}},
url = {http://www.slac.stanford.edu/~niv/index\_files/DQCOverview1.html}
}
@article{Zou2014a,
abstract = {Bioinformatics is challenged by the fact that traditional analysis tools have difficulty in processing large-scale data from high-throughput sequencing. The open source Apache Hadoop project, which adopts the MapReduce framework and a distributed file system, has recently given bioinformatics researchers an opportunity to achieve scalable, efficient and reliable computing performance on Linux clusters and on cloud computing services. In this article, we present MapReduce frame-based applications that can be employed in the next-generation sequencing and other biological domains. In addition, we discuss the challenges faced by this field as well as the future works on parallel computing in bioinformatics.},
author = {Zou, Quan and Li, Xu Bin and Jiang, Wen Rui and Lin, Zi Yu and Li, Gui Lin and Chen, Ke},
doi = {10.1093/bib/bbs088},
file = {:home/chiroptera/Dropbox/mendeley//Zou et al. - 2014 - Survey of MapReduce frame operation in bioinformatics.pdf:pdf},
isbn = {1477-4054 (Electronic)$\backslash$n1467-5463 (Linking)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics,Hadoop,MapReduce,bioinformatics},
mendeley-tags = {MapReduce,bioinformatics},
number = {4},
pages = {637--647},
pmid = {23396756},
title = {{Survey of MapReduce frame operation in bioinformatics}},
volume = {15},
year = {2014}
}
@article{Topchy,
author = {Topchy, Alexander and Minaei-bidgoli, Behrouz and Jain, Anil K and Punch, William F and Lansing, E},
file = {:home/chiroptera/Dropbox/mendeley/Topchy et al. - Unknown - Adaptive Clustering Ensembles.pdf:pdf},
number = {i},
title = {{Adaptive Clustering Ensembles}}
}
@article{Wittek2013,
abstract = {Clustering methods in machine learning may benefit from borrowing metaphors from physics. Dynamic quantum clustering associates a Gaussian wave packet with the multidimensional data points and regards them as eigenfunctions of the Schr??dinger equation. The clustering structure emerges by letting the system evolve and the visual nature of the algorithm has been shown to be useful in a range of applications. Furthermore, the method only uses matrix operations, which readily lend themselves to parallelization. In this paper, we develop an implementation on graphics hardware and investigate how this approach can accelerate the computations. We achieve a speedup of up to two magnitudes over a multicore CPU implementation, which proves that quantum-like methods and acceleration by graphics processing units have a great relevance to machine learning. ?? 2012 Elsevier Inc.},
author = {Wittek, Peter},
doi = {10.1016/j.jcp.2012.08.048},
file = {:home/chiroptera/Dropbox/mendeley/Wittek - 2013 - High-performance dynamic quantum clustering on graphics processors(2).pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
keywords = {Clustering,GPU computing,Quantum-like learning,Time-dependent schr??dinger equation},
pages = {262--271},
publisher = {Elsevier Inc.},
title = {{High-performance dynamic quantum clustering on graphics processors}},
url = {http://dx.doi.org/10.1016/j.jcp.2012.08.048},
volume = {233},
year = {2013}
}
@article{Demar2012,
abstract = {Exascale science translates to big data. In the case of the Large Hadron Collider (LHC), the data is not only immense, it is also globally distributed. Fermilab is host to the LHC Compact Muon Solenoid (CMS) experiment's US Tier-1 Center, the largest of the LHC Tier-1s. The Laboratory must deal with both scaling and wide-area distribution challenges in processing its CMS data. Fortunately, evolving technologies in the form of 100Gigabit ethernet, multi-core architectures, and GPU processing provide tools to help meet these challenges. Current Fermilab R\&amp;D efforts in these areas include optimization of network I/O handling in multi-core systems, modification of middleware to improve application performance in 100GE network environments, and network path reconfiguration and analysis for effective use of high bandwidth networks. This poster will describe the ongoing network-related R\&amp;D activities at Fermilab as a mosaic of efforts that combine to facilitate big data processing and movement. © 2012 IEEE.},
author = {Demar, Phillip J. and Dykstra, David and Garzoglio, Gabriele and Mhashilkar, Parag and Rajendran, Anupam and Wu, Wenji},
doi = {10.1109/SC.Companion.2012.215},
file = {:home/chiroptera/Dropbox/mendeley/Demar et al. - 2012 - Big data networking at fermilab.pdf:pdf},
isbn = {9780769549569},
journal = {Proceedings - 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, SCC 2012},
keywords = {100GE network,GPU processing,Large Hadron Collider experiments,big data,big data processing,cluster,gpu,multi-core systems,scaling,wide-area distribution},
mendeley-tags = {big data,cluster,gpu},
number = {1007115},
pages = {1400},
title = {{Big data networking at fermilab}},
year = {2012}
}
@article{Topics2010,
author = {Topics, Advanced and Topic, Machine Learning and Scribe, Dimensionality Reduction and Lecturer, Matt Faulkner and Date, Andreas Krause},
file = {:home/chiroptera/Dropbox/mendeley/Topics et al. - 2010 - Dimensionality reduction.pdf:pdf},
pages = {1--6},
title = {{Dimensionality reduction}},
year = {2010}
}
@article{Han2011,
abstract = {Graphics Processing Units (GPUs) have become a competitive accelerator for applications outside the graphics domain, mainly driven by the improvements inGPUprogrammability. Although the Compute Unified Device Architecture (CUDA) is a simple C-like interface for programming NVIDIA GPUs, porting applications to CUDA remains a challenge to average programmers. In particular, CUDA places on the programmer the burden of packaging GPU code in separate functions, of explicitly managing data transfer between the host and GPU memories, and of manually optimizing the utilization of the GPU memory. Practical experience shows that the programmer needs to make significant code changes, often tedious and error-prone, before getting an optimized program. We have designed hiCUDA, a high-level directive-based language for CUDA programming. It allows programmers to perform these tedious tasks in a simpler manner and directly to the sequential code, thus speeding up the porting process. In this paper, we describe the hiCUDA directives as well as the design and implementation of a prototype compiler that translates a hiCUDA program to a CUDA program. Our compiler is able to support real-world applications that span multiple procedures and use dynamically allocated arrays. Experiments using nine CUDA benchmarks show that the simplicity hiCUDA provides comes at no expense to performance.},
author = {Han, Tianyi David and Abdelrahman, Tarek S.},
doi = {10.1109/TPDS.2010.62},
file = {:home/chiroptera/Dropbox/mendeley/Han, Abdelrahman - 2011 - HiCUDA High-level GPGPU programming.pdf:pdf},
isbn = {1045-9219 VO - 22},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {CUDA,GPGPU,data-parallel programming,directive-based language,source-to-source compiler},
number = {1},
pages = {78--90},
title = {{HiCUDA: High-level GPGPU programming}},
volume = {23},
year = {2011}
}
@article{gao20xx,
author = {Gao, Zhanchun and Li, Enxing and Jiang, Yanjun},
file = {:home/chiroptera/Dropbox/mendeley/Gao, Li, Jiang - Unknown - A gpu-based harmony k-means algorithm for document clustering.pdf:pdf},
keywords = {cluster number is large,document clustering,faster,gpu,harmony search,implementation is 20 times,k-means,parallel computing,results show that cuda,than cpu implement when},
pages = {2--5},
title = {{A gpu-based harmony k-means algorithm for document clustering}}
}
@article{Chen2014a,
author = {Chen, Shangyi and Li, Wei and Li, Min and Zhang, Xiaofei and Min, Yue},
doi = {10.1109/CCBD.2014.25},
file = {:home/chiroptera/Dropbox/mendeley/Chen et al. - 2014 - Latest Progress and Infrastructure Innovations of Big Data Technology.pdf:pdf},
isbn = {978-1-4799-6621-9},
journal = {2014 International Conference on Cloud Computing and Big Data},
keywords = {big data,incremental,mapreduce},
mendeley-tags = {big data},
pages = {8--15},
title = {{Latest Progress and Infrastructure Innovations of Big Data Technology}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7062865},
year = {2014}
}
@article{Zou2014,
abstract = {Bioinformatics is challenged by the fact that traditional analysis tools have difficulty in processing large-scale data from high-throughput sequencing. The open source Apache Hadoop project, which adopts the MapReduce framework and a distributed file system, has recently given bioinformatics researchers an opportunity to achieve scalable, efficient and reliable computing performance on Linux clusters and on cloud computing services. In this article, we present MapReduce frame-based applications that can be employed in the next-generation sequencing and other biological domains. In addition, we discuss the challenges faced by this field as well as the future works on parallel computing in bioinformatics.},
author = {Zou, Quan and Li, Xu Bin and Jiang, Wen Rui and Lin, Zi Yu and Li, Gui Lin and Chen, Ke},
doi = {10.1093/bib/bbs088},
file = {:home/chiroptera/Dropbox/mendeley//Zou et al. - 2014 - Survey of MapReduce frame operation in bioinformatics.pdf:pdf},
isbn = {1477-4054 (Electronic)$\backslash$n1467-5463 (Linking)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics,Hadoop,MapReduce,bioinformatics},
mendeley-tags = {MapReduce,bioinformatics},
number = {4},
pages = {637--647},
pmid = {23396756},
title = {{Survey of MapReduce frame operation in bioinformatics}},
volume = {15},
year = {2014}
}
@article{Kindratenko2009a,
abstract = {Large-scale GPU clusters are gaining popularity in the scientific computing community. However, their deployment and production use are associated with a number of new challenges. In this paper, we present our efforts to address some of the challenges with building and running GPU clusters in HPC environments. We touch upon such issues as balanced cluster architecture, resource sharing in a cluster environment, programming models, and applications for GPU clusters.},
author = {Kindratenko, Volodymyr V. and Enos, Jeremy J. and Shi, Guochun and Showerman, Michael T. and Arnold, Galen W. and Stone, John E. and Phillips, James C. and Hwu, Wen Mei},
doi = {10.1109/CLUSTR.2009.5289128},
file = {:home/chiroptera/Dropbox/mendeley/Kindratenko et al. - 2009 - GPU clusters for high-performance computing.pdf:pdf},
isbn = {9781424450121},
issn = {15525244},
journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
title = {{GPU clusters for high-performance computing}},
year = {2009}
}
@article{Karantasis2010,
abstract = {Many-core graphics processors are playing today an important role in the advancements of modern highly concurrent processors. Their ability to accelerate computation is being explored under several scientific fields. In the current paper we present the acceleration of a widely used data clustering algorithm, K-means, in the context of high performance GPU clusters. As opposed to most related implementation efforts that use MPI to port their target applications on a GPU cluster, our implementation follows the Software Distributed Shared Memory (SDSM) paradigm in order to distribute information and computation across the accelerator cluster. In order to investigate the efficiency of a programming model that offers shared memory abstraction on GPU clusters we present two implementations, one that is based on a SDSM implementation of OpenMP and another that utilizes the Pleiad cluster middleware on top of the Java platform. The first results show that such an implementation is feasible in order to accelerate a broad category of large scale, data intensive applications, among which K-means is a characteristic case.},
author = {Karantasis, Konstantinos I. and Polychronopoulos, Eleftherios D. and Dimitrakopoulos, George N.},
doi = {10.1109/CLUSTERWKSP.2010.5613079},
file = {:home/chiroptera/Dropbox/mendeley/Karantasis, Polychronopoulos, Dimitrakopoulos - 2010 - Accelerating data clustering on GPU-based clusters under shared memory abstractio.pdf:pdf},
isbn = {9781424483969},
journal = {2010 IEEE International Conference on Cluster Computing Workshops and Posters, Cluster Workshops 2010},
keywords = {GPU clusters, SDSM, Pleiad, CUDA, K-means},
title = {{Accelerating data clustering on GPU-based clusters under shared memory abstraction}},
year = {2010}
}
@article{Wu2011,
abstract = {The \$k\$-means algorithm is widely used for unsupervised clustering. This paper describes an efficient CUDA-based \$k\$-means algorithm. Different from existing GPU-based k-means algorithms, our algorithm achieves better efficiency by utilizing the triangle inequality. Our algorithm explores the trade-off between load balance and memory access coalescing through data layout management. Because the effectiveness of the triangle inequity depends on the input data, we further propose a hybrid algorithm that adaptively determines whether to apply the triangle inequality. The efficiency of our algorithm is validated through extensive experiments, which demonstrate improved performance over existing CPU-based and CUDA-based k-means algorithms, in terms of both speed and scalability.},
author = {Wu, Jiadong and Hong, Bo},
doi = {10.1109/IPDPS.2011.331},
file = {:home/chiroptera/Dropbox/mendeley/Wu, Hong - 2011 - An efficient k-means algorithm on CUDA.pdf:pdf},
isbn = {9780769543857},
issn = {1530-2075},
journal = {IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum},
keywords = {CUDA,GPU,K-means},
pages = {1740--1749},
title = {{An efficient k-means algorithm on CUDA}},
year = {2011}
}
@article{Karimi2010,
abstract = {CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is an open standard that can be used to program CPUs, GPUs, and other devices from different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL promises a portable language for GPU programming, its generality may entail a performance penalty. In this paper, we use complex, near-identical kernels from a Quantum Monte Carlo application to compare the performance of CUDA and OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel to an OpenCL kernel involves minimal modifications. Making such a kernel compile with ATI's build tools involves more modifications. Our performance tests measure and compare data transfer times to and from the GPU, kernel execution times, and end-to-end application execution times for both CUDA and OpenCL.},
archivePrefix = {arXiv},
arxivId = {1005.2581},
author = {Karimi, Kamran and Dickson, Neil G. and Hamze, Firas},
doi = {10.1109/ICPP.2011.45},
eprint = {1005.2581},
file = {:home/chiroptera/Dropbox/mendeley/Karimi, Dickson, Hamze - 2010 - A Performance Comparison of CUDA and OpenCL.pdf:pdf},
isbn = {978-1-4577-1336-1},
number = {1},
pages = {12},
title = {{A Performance Comparison of CUDA and OpenCL}},
url = {http://arxiv.org/abs/1005.2581},
year = {2010}
}
@article{Blekas2007,
abstract = {Given a data set, a dynamical procedure is applied to the data points in order to shrink and separate, possibly overlapping clusters. Namely, Newton's equations of motion are employed to concentrate the data points around their cluster centers, using an attractive potential, constructed specially for this purpose. During this process, important information is gathered concerning the spread of each cluster. In succession this information is used to create an objective function that maps each cluster to a local maximum. Global optimization is then used to retrieve the positions of the maxima that correspond to the locations of the cluster centers. Further refinement is achieved by applying the EM-algorithm to a Gaussian mixture model whose construction and initialization is based on the acquired information. To assess the effectiveness of our method, we have conducted experiments on a plethora of benchmark data sets. In addition we have compared its performance against four clustering techniques that are well established in the literature. [All rights reserved Elsevier]},
author = {Blekas, K. and Lagaris, I.E.},
doi = {10.1016/j.patcog.2006.07.012},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Lagaris - 2007 - Newtonian clustering An approach based on molecular dynamics and global optimization.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {clustering,global optimization,molecular dynamics,order statistics},
pages = {1734--1744},
title = {{Newtonian clustering: An approach based on molecular dynamics and global optimization}},
volume = {40},
year = {2007}
}
@article{Fred2005,
author = {Fred, Ana N L and Jain, Anil K},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2005 - Combining multiple clusterings using evidence accumulation.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {K-means,cluster fusion,cluster validity,combining clustering partitions,evidence accumulation,mutual information,robust clustering,single-link},
mendeley-tags = {K-means,cluster fusion,cluster validity,combining clustering partitions,evidence accumulation,mutual information,robust clustering,single-link},
number = {6},
pages = {835--850},
title = {{Combining multiple clusterings using evidence accumulation}},
volume = {27},
year = {2005}
}
@article{DiMarco2013,
author = {DiMarco, Jeffrey and Taufer, Michela},
doi = {10.1117/12.2018069},
file = {:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms(3).pdf:pdf},
isbn = {9780819495433},
issn = {0277786X},
journal = {Spie},
keywords = {0,cuda,cuda 5,divisive hierarchical clustering,k-means},
mendeley-tags = {cuda,k-means},
pages = {87520E},
title = {{Performance impact of dynamic parallelism on different clustering algorithms}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2018069},
year = {2013}
}
@article{Wang2014,
author = {Wang, Wei},
doi = {10.1109/ICSC.2014.65},
file = {:home/chiroptera/Dropbox/mendeley/Wang - 2014 - Big Data, Big Challenges.pdf:pdf},
isbn = {978-1-4799-4003-5},
journal = {2014 IEEE International Conference on Semantic Computing},
pages = {6--6},
title = {{Big Data, Big Challenges}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6881994},
year = {2014}
}
@article{Rajaraman2011,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
author = {Rajaraman, Anand and Ullman, Jeffrey D},
doi = {10.1017/CBO9781139058452},
file = {:home/chiroptera/Dropbox/mendeley/Rajaraman, Ullman - 2011 - Mining of Massive Datasets.pdf:pdf},
isbn = {9781139058452},
issn = {01420615},
journal = {Lecture Notes for Stanford CS345A Web Mining},
pages = {328},
title = {{Mining of Massive Datasets}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139058452},
volume = {67},
year = {2011}
}
@article{Chen2014,
abstract = {In this paper, we review the background and state-of-the-art of big data. We first introduce the general background of big data and review related technologies, such as could computing, Internet of Things, data centers, and Hadoop.We then focus on the four phases of the value chain of big data, i.e., data generation, data acquisition, data storage, and data analysis. For each phase, we introduce the general background, discuss the technical challenges, and review the latest advances. We finally examine the several representative applications of big data, including enterprise management, Internet of Things, online social networks, medial applications, collective intelligence, and smart grid. These discussions aimto provide a comprehensive overview and big-picture to readers of this exciting area. This survey is concluded with a discussion of open problems and future directions.},
author = {Chen, Min and Mao, Shiwen and Liu, Yunhao},
doi = {10.1007/s11036-013-0489-0},
file = {:home/chiroptera/Dropbox/mendeley/Chen, Mao, Liu - 2014 - Big data A survey.pdf:pdf},
isbn = {1383-469X},
issn = {1383469X},
journal = {Mobile Networks and Applications},
keywords = {Big data,Big data analysis,Cloud computing,Data center,Hadoop,Internet of things,Smart grid},
number = {2},
pages = {171--209},
title = {{Big data: A survey}},
volume = {19},
year = {2014}
}
@article{Shell2008,
abstract = {Despite growing evidence that increased brain-derived neurotrophic factor (BDNF) and hippocampal adult neurogenesis are necessary for the behavioral actions of antidepressants in rodents, the cellular mechanisms involved in these effects are still unknown. Li et al. in this issue of Neuron demonstrate that the presence of TrkB, the high-affinity receptor for BDNF, in hippocampal neural progenitor cells is required for the neurogenic and behavioral actions of antidepressant treatments.},
author = {Shell, M.},
doi = {10.1016/j.neuron.2008.07.028},
issn = {1097-4199},
journal = {Neuron},
keywords = {Adult Stem Cells,Adult Stem Cells: drug effects,Animals,Antidepressive Agents,Antidepressive Agents: pharmacology,Cell Proliferation,Cell Proliferation: drug effects,Mice,Neurons,Neurons: drug effects,Receptor,trkB,trkB: metabolism},
number = {3},
pages = {349--51},
pmid = {18701059},
title = {{How to Use the IEEEtran L TEX Class}},
url = {http://www.cs.northwestern.edu/~sle841/papers/Arch\_TVCG/misc/tex files/original\_tvcg\_files/IEEEtran\_HOWTO.pdf$\backslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/18701059},
volume = {59},
year = {2008}
}
@article{Faro2012,
author = {Faro, Alberto and Giordano, Daniela and Palazzo, Simone},
file = {:home/chiroptera/Dropbox/mendeley/Faro, Giordano, Palazzo - 2012 - Integrating Unsupervised and Supervised Clustering Methods on a GPU platform for Fast Image Segmentatio.pdf:pdf},
isbn = {9781467325844},
keywords = {gpu,image segmentation,parallel clustering},
title = {{Integrating Unsupervised and Supervised Clustering Methods on a GPU platform for Fast Image Segmentation}},
year = {2012}
}
@article{Aimeur2013,
abstract = {We show how the quantum paradigm can be used to speed up unsupervised learning algorithms. More precisely, we explain how it is possible to accelerate learning algorithms by quantizing some of their subroutines. Quantization refers to the process that partially or totally converts a classical algorithm to its quantum counterpart in order to improve performance. In particular, we give quantized versions of clustering via minimum spanning tree, divisive clustering and k-medians that are faster than their classical analogues. We also describe a distributed version of k-medians that allows the participants to save on the global communication cost of the protocol compared to the classical version. Finally, we design quantum algorithms for the construction of a neighbourhood graph, outlier detection as well as smart initialization of the cluster centres.},
author = {A\"{\i}meur, Esma and Brassard, Gilles and Gambs, S\'{e}bastien},
doi = {10.1007/s10994-012-5316-5},
file = {:home/chiroptera/Dropbox/mendeley/A\"{\i}meur, Brassard, Gambs - 2013 - Quantum speed-up for unsupervised learning.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Clustering,Grover's algorithm,Quantum information processing,Quantum learning,Unsupervised learning},
number = {February 2012},
pages = {261--287},
title = {{Quantum speed-up for unsupervised learning}},
volume = {90},
year = {2013}
}
@article{Ji2011,
abstract = {Modern General Purpose Graphics Processing Units (GPGPUs) provide high degrees of parallelism in computation and memory access, making them suitable for data parallel applications such as those using the elastic MapReduce model. Yet designing a MapReduce framework for GPUs faces significant challenges brought by their multi-level memory hierarchy. Due to the absence of atomic operations in the earlier generations of GPUs, existing GPU MapReduce frameworks have problems in handling input/output data with varied or unpredictable sizes. Also, existing frameworks utilize mostly a single level of memory, $\backslash$emph\{i.e.\}, the relatively spacious yet slow global memory. In this work, we attempt to explore the potential benefit of enabling a GPU MapReduce framework to use multiple levels of the GPU memory hierarchy. We propose a novel GPU data staging scheme for MapReduce workloads, tailored toward the GPU memory hierarchy. Centering around the efficient utilization of the fast but very small shared memory, we designed and implemented a GPU MapReduce framework, whose key techniques include (1) shared memory staging area management, (2) thread-role partitioning, and (3) intra-block thread synchronization. We carried out evaluation with five popular MapReduce workloads and studied their performance under different GPU memory usage choices. Our results reveal that exploiting GPU shared memory is highly promising for the Map phase (with an average 2.85x speedup over using global memory only), while in the Reduce phase the benefit of using shared memory is much less pronounced, due to the high input-to-output ratio. In addition, when compared to Mars, an existing GPU MapReduce framework, our system is shown to bring a significant speedup in Map/Reduce phases.},
author = {Ji, Feng and Ma, Xiaosong},
doi = {10.1109/IPDPS.2011.80},
file = {:home/chiroptera/Dropbox/mendeley/Ji, Ma - 2011 - Using Shared Memory to Accelerate MapReduce on Graphics Processing Units.pdf:pdf},
isbn = {978-0-7695-4385-7},
issn = {1530-2075},
journal = {2011 IEEE International Parallel \& Distributed Processing Symposium},
keywords = {MapReduce,gpu},
mendeley-tags = {MapReduce,gpu},
pages = {805--816},
title = {{Using Shared Memory to Accelerate MapReduce on Graphics Processing Units}},
year = {2011}
}
@article{Hou2013,
author = {Hou, Rui and Jiang, Tao and Zhang, Liuhang and Qi, Pengfei and Dong, Jianbo and Wang, Haibin and Gu, Xiongli and Zhang, Shujie},
doi = {10.1109/HPCA.2013.6522317},
file = {:home/chiroptera/Dropbox/mendeley/Hou et al. - 2013 - Cost effective data center servers.pdf:pdf},
isbn = {978-1-4673-5587-2},
journal = {2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)},
pages = {179--187},
title = {{Cost effective data center servers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6522317},
year = {2013}
}
@article{Ghorpade2012,
abstract = {The future of computation is the Graphical Processing Unit, i.e. the GPU. The promise that the graphics cards have shown in the field of image processing and accelerated rendering of 3D scenes, and the computational capability that these GPUs possess, they are developing into great parallel computing units. It is quite simple to program a graphics processor to perform general parallel tasks. But after understanding the various architectural aspects of the graphics processor, it can be used to perform other taxing tasks as well. In this paper, we will show how CUDA can fully utilize the tremendous power of these GPUs. CUDA is NVIDIA’s parallel computing architecture. It enables dramatic increases in computing performance, by harnessing the power of the GPU. This paper talks about CUDA and its architecture. It takes us through a comparison of CUDA C/C++ with other parallel programming languages like OpenCL and DirectCompute. The paper also lists out the common myths about CUDA and how the future seems to be promising for CUDA.},
archivePrefix = {arXiv},
arxivId = {1202.4347},
author = {Ghorpade, Jayshree},
doi = {10.5121/acij.2012.3109},
eprint = {1202.4347},
file = {:home/chiroptera/Dropbox/mendeley/Ghorpade - 2012 - GPGPU Processing in CUDA Architecture.pdf:pdf},
issn = {2229726X},
journal = {Advanced Computing: An International Journal},
keywords = {gpgpu,gpu},
mendeley-tags = {gpu,gpgpu},
number = {1},
pages = {105--120},
title = {{GPGPU Processing in CUDA Architecture}},
volume = {3},
year = {2012}
}
@article{Horn2001b,
author = {Horn, David and Gottlieb, Assaf},
doi = {10.1103/PhysRevLett.88.018702},
file = {:home/chiroptera/Dropbox/mendeley//Horn, Gottlieb - 2001 - Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics.ps:ps;:home/chiroptera/Dropbox/mendeley/Horn, Gottlieb - 2001 - Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics.pdf:pdf},
issn = {0031-9007},
journal = {Physical Review Letters},
number = {1},
pages = {1--4},
title = {{Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics}},
url = {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.88.018702},
volume = {88},
year = {2001}
}
@misc{Casper,
abstract = {A Quantum-Modeled K-Means clustering algorithm for multi- band image segmentation is explored and evaluated. Data sets of interest include multi-band RGB imagery, which subsequent to classification is analyzed and assessed for accuracy. Results demonstrate that under specific conditions the algorithm exhibits improved accuracy, when compared to its classical counterpart. Categories},
author = {Casper, Ellis and Hung, Chih-Cheng and Jung, Edward and Yang, Ming},
file = {:home/chiroptera/Dropbox/mendeley/Casper et al. - Unknown - A Quantum-Modeled K-Means Clustering Algorithm for Multi-band Image Segmentation.pdf:pdf},
keywords = {K-Means,QK-Means,Quantum computing,image,image segmentation,k-means,qk-means,quantum computing,qubit},
mendeley-tags = {image segmentation,k-means,qk-means,qubit},
number = {3},
pages = {158--163},
title = {{A Quantum-Modeled K-Means Clustering Algorithm for Multi-band Image Segmentation}},
url = {http://delivery.acm.org/10.1145/2410000/2401639/p158-casper.pdf?ip=193.136.132.10\&id=2401639\&acc=ACTIVE SERVICE\&key=2E5699D25B4FE09E.F7A57B2C5B227641.4D4702B0C3E38B35.4D4702B0C3E38B35\&CFID=476955365\&CFTOKEN=55494231\&\_\_acm\_\_=1423057410\_0d77d9b5028cb3},
urldate = {2015-02-04},
volume = {1}
}
@article{Horn2004,
abstract = {We describe results of a novel algorithm for grammar induction from a large corpus. The ADIOS (Automatic DIstillation of Structure) algorithm searches for significant patterns, chosen according to context dependent statistical criteria, and builds a hierarchy of such patterns according to a set of rules leading to structured generalization. The corpus is thus generalized into a context free grammar (CFG), composed of patterns, equivalence classes and words of the initial lexicon. We have evaluated our method both on corpora generated by CFG and on natural language ones. The performance of ADIOS is judged by searching for both good recall (acceptance of correct novel sentences) and good precision (production of correct novel sentences). The results are very encouraging.},
author = {Horn, David and Solan, Zach and Ruppin, Eytan and Edelman, Shimon},
file = {:home/chiroptera/Dropbox/mendeley/Horn et al. - 2004 - Unsupervised language acquisition syntax from plain corpus.pdf:pdf},
journal = {\ldots on Human Language},
title = {{Unsupervised language acquisition: syntax from plain corpus}},
url = {http://horn.tau.ac.il/~horn/publications/newcastle.pdf},
year = {2004}
}
@book{Pedersen2008,
abstract = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
author = {Pedersen, Michael Syskind and Baxter, Bill and Templeton, Brian and Rish\o j, Christian and Theobald, Douglas L and Hoegh-rasmussen, Esben and Casteel, Glynne and Gao, Jun Bin and Dedecius, Kamil and Strim, Korbinian and Christiansen, Lars and Hansen, Lars Kai and Wilkinson, Leland and He, Liguo and Bar, Miguel and Winther, Ole and Sakov, Pavel and Hattinger, Stephan and Petersen, Kaare Breandt and Rish\o j, Christian},
booktitle = {Matrix},
doi = {10.1111/j.1365-294X.2006.03161.x},
editor = {Bloom, B R},
institution = {Technical University of Denmark},
issn = {09621083},
keywords = {acknowledgements,bill baxter,brian templeton,christian,christian rish\o j,contributions,derivative,derivative inverse matrix,determinant,differentiate a matrix,matrix algebra,matrix identities,matrix relations,suggestions,thank following,we would like},
number = {1},
pages = {1--71},
pmid = {17284204},
publisher = {Citeseer},
title = {{The Matrix Cookbook}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.3165\&amp;rep=rep1\&amp;type=pdf},
volume = {M},
year = {2008}
}
@article{Fred2001,
abstract = {Given an arbitrary data set, to which no particular paramet- rical, statistical or geometrical structure can be assumed, different clus- tering algorithms will in general produce different data partitions. In fact, several partitions can also be obtained by using a single clustering algo- rithm due to dependencies on initialization or the selection of the value of some design parameter. This paper addresses the problem of finding consistent clusters in data partitions, proposing the analysis of the most common associations performed in a majority voting scheme. Combina- tion of clustering results are performed by transforming data partitions into a co-association sample matrix, which maps coherent associations. This matrix is then used to extract the underlying consistent clusters. The proposed methodology is evaluated in the context of k-means clus- tering, a new clustering algorithm - voting-k-means, being presented. Examples, using both simulated and real data, show how this major- ity voting combination scheme simultaneously handles the problems of selecting the number of clusters, and dependency on initialization. Fur- thermore, resulting clusters are not constrained to be hyper-spherically shaped.},
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2001 - Finding consistent clusters in data partitions.pdf:pdf},
isbn = {3540422846},
journal = {Multiple classifier systems},
pages = {309--318},
title = {{Finding consistent clusters in data partitions}},
url = {http://link.springer.com/chapter/10.1007/3-540-48219-9\_31},
year = {2001}
}
@article{Harvard2005,
author = {Harvard},
file = {:home/chiroptera/Dropbox/mendeley/Harvard - 2005 - The Assignment Problem and the Hungarian Method.pdf:pdf},
journal = {Introduction to Linear Algebra and Multivariable Calculus},
title = {{The Assignment Problem and the Hungarian Method}},
year = {2005}
}
@article{Pennycook2013,
abstract = {This paper reports on the development of an MPI/OpenCL implementation of LU, an application-level benchmark from the NAS Parallel Benchmark Suite. An account of the design decisions addressed during the development of this code is presented, demonstrating the importance of memory arrangement and work-item/work-group distribution strategies when applications are deployed on different device types. The resulting platform-agnostic, single source application is benchmarked on a number of different architectures, and is shown to be 1.3-1.5× slower than native FORTRAN 77 or CUDA implementations on a single node and 1.3-3.1× slower on multiple nodes. We also explore the potential performance gains of OpenCL's device fissioning capability, demonstrating up to a 3× speed-up over our original OpenCL implementation. © 2013 Elsevier Ltd. All rights reserved.},
author = {Pennycook, S. J. and Hammond, S. D. and Wright, S. a. and Herdman, J. a. and Miller, I. and Jarvis, S. a.},
doi = {10.1016/j.jpdc.2012.07.005},
file = {:home/chiroptera/Dropbox/mendeley/Pennycook et al. - 2013 - An investigation of the performance portability of OpenCL.pdf:pdf},
isbn = {Pennycook, S.J., Hammond, S.D., Wright, S.A., Herdman, J.A., Miller, I. and Jarvis, S.A. (2012) An Investigation of the Performance Portability of OpenCL. Journal of Parallel and Distributed Computing. ISSN 0743-7315 (In Press)},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {GPU computing,High performance computing,Many-core computing,OpenCL,Optimisation},
number = {11},
pages = {1439--1450},
publisher = {Elsevier Inc.},
title = {{An investigation of the performance portability of OpenCL}},
url = {http://dx.doi.org/10.1016/j.jpdc.2012.07.005},
volume = {73},
year = {2013}
}
@article{Varshavsky2007a,
abstract = {Motivation: Feature selection methods aim to reduce the complexity of data and to uncover the most relevant biological variables. In reality, information in biological datasets is often incomplete as a result of untrustworthy samples and missing values. The reliability of selection methods may therefore be questioned.  Method: Information loss is incorporated into a perturbation scheme, testing which features are stable under it. This method is applied to data analysis by unsupervised feature filtering (UFF). The latter has been shown to be a very successful method in analysis of gene-expression data.  Results: We find that the UFF quality degrades smoothly with information loss. It remains successful even under substantial damage. Our method allows for selection of a best imputation method on a dataset treated by UFF. More importantly, scoring features according to their stability under information loss is shown to be correlated with biological importance in cancer studies. This scoring may lead to novel biological insights.  Contact: royke@cs.huji.ac.il  Supplementary information and code availability: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btm528},
author = {Varshavsky, Roy and Gottlieb, Assaf and Horn, David and Linial, Michal},
doi = {10.1093/bioinformatics/btm528},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky et al. - 2007 - Unsupervised feature selection under perturbations Meeting the challenges of biological data.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {24},
pages = {3343--3349},
title = {{Unsupervised feature selection under perturbations: Meeting the challenges of biological data}},
volume = {23},
year = {2007}
}
@article{Mokhtari2014,
author = {Mokhtari, Reza and Stumm, Michael},
doi = {10.1109/IPDPS.2014.89},
file = {:home/chiroptera/Dropbox/mendeley/Mokhtari, Stumm - 2014 - BigKernel -- High Performance CPU-GPU Communication Pipelining for Big Data-Style Applications.pdf:pdf},
isbn = {978-1-4799-3800-1},
issn = {23321237},
journal = {Proceedings of the 2014 IEEE 28th International Parallel and Distributed Processing Symposium},
keywords = {CPU,GPU,communication,cpu-gpu,gpu,memory management,optimization,programming model,stream processing},
mendeley-tags = {cpu-gpu,gpu,memory management,programming model},
pages = {819--828},
title = {{BigKernel -- High Performance CPU-GPU Communication Pipelining for Big Data-Style Applications}},
url = {http://dx.doi.org/10.1109/IPDPS.2014.89},
year = {2014}
}
@article{Wu2009,
abstract = {In this paper, we report our research on using GPUs as accelerators for Business Intelligence(BI) analytics. We are particularly interested in analytics on very large data sets, which are common in today's real world BI applications. While many published works have shown that GPUs can be used to accelerate various general purpose applications with respectable performance gains, few attempts have been made to tackle very large problems. Our goal here is to investigate if the GPUs can be useful accelerators for BI analytics with very large data sets that cannot fit into GPUs onboard memory. Using a popular clustering algorithm, K-Means, as an example, our results have been very positive. For data sets smaller than GPU's onboard memory, the GPU-accelerated version is 6-12x faster than our highly optimized CPU-only version running on an 8-core workstation, or 200-400x faster than the popular benchmark program, MineBench, running on a single core. This is also 2-4x faster than the best reported work. For large data sets which cannot fit in GPU's memory, we further show that with a design which allows the computation on both CPU and GPU, as well as data transfers between them, to proceed in parallel, the GPU-accelerated version can still offer a dramatic performance boost. For example, for a data set with 100 million 2-d data points and 2,000 clusters, the GPU-accelerated version took about 6 minutes, while the CPU-only version running on an 8-core workstation took about 58 minutes. Compared to other approaches, GPU-accelerated implementations of analytics potentially provide better raw performance, better cost-performance ratios, and better energy performance ratios.},
author = {Wu, Ren and Zhang, Bin and Hsu, Meichun},
file = {:home/chiroptera/Dropbox/mendeley/Wu, Zhang, Hsu - 2009 - GPU-Accelerated Large Scale Analytics.pdf:pdf},
isbn = {HPL-2009-38},
journal = {Development},
keywords = {algorithm,big data,clustering,data mining,data mining clustering parallel algorithm gpu gpgp,gpgpu,gpu,k means,many core,multi core,parallel,s},
mendeley-tags = {big data,gpu},
number = {HPL-2009-38},
pages = {10},
title = {{GPU-Accelerated Large Scale Analytics}},
url = {http://www.hpl.hp.com/techreports/2009/HPL-2009-38.pdf},
year = {2009}
}
@article{Fred2009b,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt. 4 - Clustering Ensemble Methods.pdf:pdf},
journal = {Methods},
number = {April},
pages = {1--27},
title = {{Tutorial Pt. 4 - Clustering Ensemble Methods}},
year = {2009}
}
@article{Fred2002,
abstract = {We explore the idea of evidence accumulation for combining the results of multiple clusterings. Initially, n d-dimensional data is decomposed into a large number of compact clusters; the K-means algorithm performs this decomposition, with several clusterings obtained by N random initializations of the K-means. Taking the co-occurrences of pairs of patterns in the same cluster as votes for their association, the data partitions are mapped into a co-association matrix of patterns. This n\&amp;times;n matrix represents a new similarity measure between patterns. The final clusters are obtained by applying a MST-based clustering algorithm on this matrix. Results on both synthetic and real data show the ability of the method to identify arbitrary shaped clusters in multidimensional data.},
author = {a.L.N. Fred and a.K. Jain},
doi = {10.1109/ICPR.2002.1047450},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2002 - Data clustering using evidence accumulation.pdf:pdf},
isbn = {0-7695-1695-X},
issn = {1051-4651},
journal = {Object recognition supported by user interaction for service robots},
title = {{Data clustering using evidence accumulation}},
volume = {4},
year = {2002}
}
@article{Zechner2009a,
abstract = {In this paper an optimized k-means implementation on the graphics processing unit (GPU) is presented. NVIDIApsilas compute unified device architecture (CUDA), available from the G80 GPU family onwards, is used as the programming environment. Emphasis is placed on optimizations directly targeted at this architecture to best exploit the computational capabilities available. Additionally drawbacks and limitations of previous related work, e.g. maximum instance, dimension and centroid count are addressed. The algorithm is realized in a hybrid manner, parallelizing distance calculations on the GPU while sequentially updating cluster centroids on the CPU based on the results from the GPU calculations. An empirical performance study on synthetic data is given, demonstrating a maximum 14times speed increase to a fully SIMD optimized CPU implementation.},
author = {Zechner, Mario and Granitzer, Michael},
doi = {10.1109/INTENSIVE.2009.19},
file = {:home/chiroptera/Dropbox/mendeley/Zechner, Granitzer - 2009 - Accelerating k-means on the graphics processor via CUDA(2).pdf:pdf},
isbn = {9780769535852},
journal = {Proceedings of the 1st International Conference on Intensive Applications and Services, INTENSIVE 2009},
pages = {7--15},
title = {{Accelerating k-means on the graphics processor via CUDA}},
year = {2009}
}
@article{Joshi2003,
abstract = {Clustering large data sets can be time consuming and processor intensive. This project is an implementation of the parallel version of a popular clustering algorithm, the k-means algorithm, to provide faster clustering solutions. This algorithm was tested such that 3,4,5,7 clusters were created on a cluster of Sun workstations. Optimal levels of speedup were not achieved; but the benefits of parallelization were observed. This methodology exploits the inherent data- parallelism in the k-means algorithm and makes use of the message-passing model.},
annote = {Results only on very small datasets, only accuracy results and no timings. Speedup purely theoretical.},
author = {Joshi, Manasi N},
file = {:home/chiroptera/Dropbox/mendeley/Joshi - 2003 - Parallel K - Means Algorithm on Distributed Memory Multiprocessors.pdf:pdf},
journal = {Cities},
pages = {12},
title = {{Parallel K - Means Algorithm on Distributed Memory Multiprocessors}},
year = {2003}
}
@article{Fred2003,
author = {Fred, Ana L N},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2003 - A New Cluster Isolation Criterion Based on Dissimilarity Increments ´.pdf:pdf},
number = {8},
pages = {1--15},
title = {{A New Cluster Isolation Criterion Based on Dissimilarity Increments ´}},
volume = {25},
year = {2003}
}
@article{Lloyd2013,
abstract = {Machine-learning tasks frequently involve problems of manipulating and classi- fying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number ofvectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervisedandunsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1307.0411v2},
author = {Lloyd, Seth and Mohseni, Masoud and Rebentrost, Patrick},
eprint = {arXiv:1307.0411v2},
file = {:home/chiroptera/Dropbox/mendeley/Lloyd, Mohseni, Rebentrost - 2013 - Quantum algorithms for supervised and unsupervised machine learning.pdf:pdf},
keywords = {Machine Learning,quantum computers},
mendeley-tags = {quantum computers},
pages = {1--11},
title = {{Quantum algorithms for supervised and unsupervised machine learning}},
url = {http://arxiv.org/pdf/1307.0411.pdf$\backslash$npapers2://publication/uuid/2BDCBB85-812C-46E4-B506-73B9A41EBBC1},
year = {2013}
}
@article{Chiosa2011,
author = {Chiosa, Iurie and Kolb, Andreas},
file = {:home/chiroptera/Dropbox/mendeley/Chiosa, Kolb - 2011 - GPU-Based Multilevel Clustering.pdf:pdf},
number = {2},
pages = {132--145},
title = {{GPU-Based Multilevel Clustering}},
volume = {17},
year = {2011}
}
@article{Xiao2010,
abstract = {The number of clusters has to be known in advance for the conventional k-means clustering algorithm and moreover the clustering result is sensitive to the selection of the initial cluster centroids. This sensitivity may make the algorithm converge to the local optima. This paper proposes a quantum-inspired genetic algorithm for k-means clustering (KMQGA). In KMQGA, a Q-bit based representation is employed for exploration and exploitation in discrete 0-1 hyperspace using rotation operation of quantum gate as well as the typical genetic algorithm operations (selection, crossover and mutation) of Q-bits. Different from the typical quantum-inspired genetic algorithms (QGA), the length of a Q-bit in KMQGA is variable during evolution. Without knowing the exact number of clusters beforehand, KMQGA can obtain the optimal number of clusters as well as providing the optimal cluster centroids. Both the simulated datasets and the real datasets are used to validate KMQGA, respectively. The experimental results show that KMQGA is promising and effective. ?? 2009 Elsevier Ltd. All rights reserved.},
annote = {From Duplicate 2 (A quantum-inspired genetic algorithm for k-means clustering - Xiao, Jing; Yan, YuPing; Zhang, Jun; Tang, Yong)},
author = {Xiao, Jing and Yan, YuPing and Zhang, Jun and Tang, Yong},
doi = {10.1016/j.eswa.2009.12.017},
file = {:home/chiroptera/Dropbox/mendeley/Xiao et al. - 2010 - A quantum-inspired genetic algorithm for k-means clustering.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Genetic algorithms,Quantum-inspired genetic algorithms,k-means,k-means clustering,qubit},
mendeley-tags = {k-means,qubit},
pages = {4966--4973},
title = {{A quantum-inspired genetic algorithm for k-means clustering}},
url = {http://ac.els-cdn.com/S095741740901063X/1-s2.0-S095741740901063X-main.pdf?\_tid=f303a76c-ac71-11e4-be73-00000aacb35e\&acdnat=1423056793\_66291f279193fa69b86c93aecea405b0},
volume = {37},
year = {2010}
}
