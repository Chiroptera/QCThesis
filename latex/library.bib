Automatically generated by Mendeley Desktop 1.14
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Joshi2003,
abstract = {Clustering large data sets can be time consuming and processor intensive. This project is an implementation of the parallel version of a popular clustering algorithm, the k-means algorithm, to provide faster clustering solutions. This algorithm was tested such that 3,4,5,7 clusters were created on a cluster of Sun workstations. Optimal levels of speedup were not achieved; but the benefits of parallelization were observed. This methodology exploits the inherent data- parallelism in the k-means algorithm and makes use of the message-passing model.},
annote = {Results only on very small datasets, only accuracy results and no timings. Speedup purely theoretical.},
author = {Joshi, Manasi N},
file = {:home/chiroptera/Dropbox/mendeley/Joshi - 2003 - Parallel K - Means Algorithm on Distributed Memory Multiprocessors.pdf:pdf},
journal = {Cities},
pages = {12},
title = {{Parallel K - Means Algorithm on Distributed Memory Multiprocessors}},
year = {2003}
}
@article{Lloyd2013,
abstract = {Machine-learning tasks frequently involve problems of manipulating and classi- fying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number ofvectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervisedandunsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1307.0411v2},
author = {Lloyd, Seth and Mohseni, Masoud and Rebentrost, Patrick},
eprint = {arXiv:1307.0411v2},
file = {:home/chiroptera/Dropbox/mendeley/Lloyd, Mohseni, Rebentrost - 2013 - Quantum algorithms for supervised and unsupervised machine learning.pdf:pdf},
keywords = {Machine Learning,quantum computers},
mendeley-tags = {quantum computers},
pages = {1--11},
title = {{Quantum algorithms for supervised and unsupervised machine learning}},
url = {http://arxiv.org/pdf/1307.0411.pdf$\backslash$npapers2://publication/uuid/2BDCBB85-812C-46E4-B506-73B9A41EBBC1},
year = {2013}
}
@article{Arefin2011,
author = {Arefin, Ahmed Shamsul and Inostroza-Ponta, Mario and Mathieson, Luke and Berretta, Regina and Moscato, Pablo},
doi = {10.1007/978-3-642-24669-2\_36},
file = {:home/chiroptera/Dropbox/mendeley/Arefin et al. - 2011 - Clustering nodes in large-scale biological networks using external memory algorithms.pdf:pdf},
isbn = {9783642246685},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data clustering,external memory algorithms,gene expression data analysis,graph algorithms},
number = {PART 2},
pages = {375--386},
title = {{Clustering nodes in large-scale biological networks using external memory algorithms}},
volume = {7017 LNCS},
year = {2011}
}
@article{Barnat2011,
abstract = {The problem of decomposing a directed graph into its strongly connected components is a fundamental graph problem inherently present in many scientific and commercial applications. In this paper we show how some of the existing parallel algorithms can be reformulated in order to be accelerated by NVIDIA CUDA technology. In particular, we design a new CUDA-aware procedure for pivot selection and we adapt selected parallel algorithms for CUDA accelerated computation. We also experimentally demonstrate that with a single GTX 480 GPU card we can easily outperform the optimal serial CPU implementation by an order of magnitude in most cases, 40 times on some sufficiently big instances. This is an interesting result as unlike the serial CPU case, the asymptotic complexity of the parallel algorithms is not optimal.},
author = {Barnat, Jiř\'{\i} and Bauch, Petr and Brim, Lubo\v{s} and \v{C}e\v{s}ka, Milan},
doi = {10.1109/IPDPS.2011.59},
file = {:home/chiroptera/Dropbox/mendeley/Barnat et al. - 2011 - Computing strongly connected components in parallel on CUDA.pdf:pdf},
isbn = {9780769543857},
issn = {1530-2075},
journal = {Proceedings - 25th IEEE International Parallel and Distributed Processing Symposium, IPDPS 2011},
pages = {544--555},
title = {{Computing strongly connected components in parallel on CUDA}},
year = {2011}
}
@article{Calders,
author = {Calders, Toon},
file = {:home/chiroptera/Dropbox/mendeley/Calders - Unknown - Data Mining Clustering What is Cluster Analysis.pdf:pdf},
title = {{Data Mining Clustering What is Cluster Analysis ?}}
}
@article{Xiao2010,
abstract = {The number of clusters has to be known in advance for the conventional k-means clustering algorithm and moreover the clustering result is sensitive to the selection of the initial cluster centroids. This sensitivity may make the algorithm converge to the local optima. This paper proposes a quantum-inspired genetic algorithm for k-means clustering (KMQGA). In KMQGA, a Q-bit based representation is employed for exploration and exploitation in discrete 0-1 hyperspace using rotation operation of quantum gate as well as the typical genetic algorithm operations (selection, crossover and mutation) of Q-bits. Different from the typical quantum-inspired genetic algorithms (QGA), the length of a Q-bit in KMQGA is variable during evolution. Without knowing the exact number of clusters beforehand, KMQGA can obtain the optimal number of clusters as well as providing the optimal cluster centroids. Both the simulated datasets and the real datasets are used to validate KMQGA, respectively. The experimental results show that KMQGA is promising and effective. ?? 2009 Elsevier Ltd. All rights reserved.},
annote = {From Duplicate 2 (A quantum-inspired genetic algorithm for k-means clustering - Xiao, Jing; Yan, YuPing; Zhang, Jun; Tang, Yong)},
author = {Xiao, Jing and Yan, YuPing and Zhang, Jun and Tang, Yong},
doi = {10.1016/j.eswa.2009.12.017},
file = {:home/chiroptera/Dropbox/mendeley/Xiao et al. - 2010 - A quantum-inspired genetic algorithm for k-means clustering.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Genetic algorithms,Quantum-inspired genetic algorithms,k-means,k-means clustering,qubit},
mendeley-tags = {k-means,qubit},
pages = {4966--4973},
title = {{A quantum-inspired genetic algorithm for k-means clustering}},
url = {http://ac.els-cdn.com/S095741740901063X/1-s2.0-S095741740901063X-main.pdf?\_tid=f303a76c-ac71-11e4-be73-00000aacb35e\&acdnat=1423056793\_66291f279193fa69b86c93aecea405b0},
volume = {37},
year = {2010}
}
@article{Stuart2011,
abstract = {We present GPMR, our stand-alone MapReduce library that leverages the power of GPU clusters for large-scale computing. To better utilize the GPU, we modify MapReduce by combining large amounts of map and reduce items into chunks and using partial reductions and accumulation. We use persistent map and reduce tasks and stress aspects of GPMR with a set of standard MapReduce benchmarks. We run these benchmarks on a GPU cluster and achieve desirable speedup and efficiency for all benchmarks. We compare our implementation to the current-best GPU-MapReduce library (runs only on a solo GPU) and a highly-optimized multi-core MapReduce to show the power of GPMR. We demonstrate how typical MapReduce tasks are easily modified to fit into GPMR and leverage a GPU cluster. We highlight how total and relative amounts of communication affect GPMR. We conclude with an exposition on the types of MapReduce tasks well-suited to GPMR, and why some tasks need more modifications than others to work well with GPMR.},
author = {Stuart, Jeff a. and Owens, John D.},
doi = {10.1109/IPDPS.2011.102},
file = {:home/chiroptera/Dropbox/mendeley/Stuart, Owens - 2011 - Multi-GPU MapReduce on GPU clusters.pdf:pdf},
isbn = {9780769543857},
issn = {1530-2075},
journal = {Proceedings - 25th IEEE International Parallel and Distributed Processing Symposium, IPDPS 2011},
keywords = {GPU,MapReduce,cluster},
mendeley-tags = {GPU,MapReduce,cluster},
pages = {1068--1079},
title = {{Multi-GPU MapReduce on GPU clusters}},
year = {2011}
}
@inproceedings{DiBuccio2011,
abstract = {Dynamic Quantum Clustering is a recent clustering technique which makes use of Parzen window estimator to construct a potential function whose minima are related to the clusters to be found. The dynamic of the system is computed by means of the Schr\"{o}dinger differential equation. In this paper, we apply this technique in the context of Information Retrieval to explore its performance in terms of the quality of clusters and the efficiency of the computation. In particular, we want to analyze the clusters produced by using datasets of relevant and non-relevant documents given a topic. © 2011 Springer-Verlag.},
author = {{Di Buccio}, Emanuele and {Di Nunzio}, Giorgio Maria},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {360--363},
title = {{Distilling relevant documents by means of dynamic quantum clustering}},
volume = {6931 LNCS},
year = {2011}
}
@article{Zhai2014,
abstract = {Big data processing is receiving significant amount of interest as an important technology to reveal the information behind the data, such as trends, characteristics, etc. MapReduce is one of the most popular distributed parallel data processing framework. However, some high-end applications, especially some scientific analyses have both data-intensive and computation intensive features. Therefore, we have designed and implemented a high performance big data process framework called Lit, which leverages the power of Hadoop and GPUs. In this paper, we presented the basic design and architecture of Lit. More importantly, we spent a lot of effort on optimizing the communications between CPU and GPU. Lit integrated GPU with Hadoop to improve the computational power of each node in the cluster. To simplify the parallel programming, Lit provided an annotation based approach to automatically generate CUDA codes from Hadoop codes. Lit hid the complexity of programming on CPU/GPU cluster by providing extended compiler and optimizer. To utilize the simplified programming, scalability and fault tolerance benefits of Hadoop and combine them with the high performance computation power of GPU, Lit extended the Hadoop by applying a GPUClassloader to detect the GPU, generate and compile CUDA codes, and invoke the shared library. For all CPU-GPU co-processing systems, the communication with the GPU is the well-known performance bottleneck. We introduced data flow optimization approach to reduce unnecessary memory copies. Our experimental results show that Lit can achieve an average speedup of 1x to 3x on three typical applications over Hadoop, and the data flow optimization approach for the Lit can achieve about 16\% performance gain. © 2013 IEEE.},
annote = {MapReduce for using GPU in clusters. Has review of other GPU MapReduce implementations.},
author = {Zhai, Yanlong and Guo, Ying and Chen, Qiurui and Yang, Kai and Mbarushimana, Emmanuel},
doi = {10.1109/HPCC.and.EUC.2013.147},
file = {:home/chiroptera/Dropbox/mendeley/Zhai et al. - 2014 - Design and optimization of a big data computing framework based on CPUGPU cluster.pdf:pdf},
isbn = {9780769550886},
journal = {Proceedings - 2013 IEEE International Conference on High Performance Computing and Communications, HPCC 2013 and 2013 IEEE International Conference on Embedded and Ubiquitous Computing, EUC 2013},
keywords = {GPU,cluster,mapreduce},
mendeley-tags = {GPU,cluster,mapreduce},
pages = {1039--1046},
title = {{Design and optimization of a big data computing framework based on CPU/GPU cluster}},
year = {2014}
}
@article{Nvidia2010,
author = {Nvidia},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia - 2010 - Introduction to CUDA C.pdf:pdf},
journal = {Siggraph Asia 2010},
title = {{Introduction to CUDA C}},
year = {2010}
}
@article{Neelima2010,
abstract = {With the growth of Graphics Processor (GPU) programmability and processing power, graphics hardware has become a compelling platform for computationally demanding tasks in a wide variety of application domains. This state of art paper gives the technical motivations that underlie GPU computing and describe the hardware and software developments that have led to the recent interest in this field.},
author = {Neelima, B. and Raghavendra, Prakash S.},
doi = {10.1109/ICIINFS.2010.5578685},
file = {:home/chiroptera/Dropbox/mendeley/Neelima, Raghavendra - 2010 - Recent trends in software and hardware for GPGPU computing A comprehensive survey.pdf:pdf},
isbn = {9781424466535},
journal = {2010 5th International Conference on Industrial and Information Systems, ICIIS 2010},
keywords = {GPU computing,Graphics Processor (GPU)},
pages = {319--324},
title = {{Recent trends in software and hardware for GPGPU computing: A comprehensive survey}},
year = {2010}
}
@article{Weinstein2009a,
abstract = {Last year, in 2008, I gave a talk titled \{$\backslash$it Quantum Calisthenics\}. This year I am going to tell you about how the work I described then has spun off into a most unlikely direction. What I am going to talk about is how one maps the problem of finding clusters in a given data set into a problem in quantum mechanics. I will then use the tricks I described to let quantum evolution lets the clusters come together on their own.},
archivePrefix = {arXiv},
arxivId = {0911.0462},
author = {Weinstein, Marvin},
eprint = {0911.0462},
file = {:home/chiroptera/Dropbox/mendeley/Weinstein - 2009 - Strange Bedfellows Quantum Mechanics and Data Mining.pdf:pdf},
pages = {11},
title = {{Strange Bedfellows: Quantum Mechanics and Data Mining}},
url = {http://arxiv.org/abs/0911.0462},
year = {2009}
}
@article{Duarte2005,
abstract = {We explore the idea of evidence accumulation (EAC) for combining the results of multiple clusterings. The EAC paradigm combines the information existent in n partitions into a co-association matrix (similarity matrix) based on pairwise associations, where each partition has an identical weight in the combination process. The final data partition is obtained by applying a clustering algorithm over this co-association matrix. In this paper we propose the idea of weighting differently the partitions (WEAC). Each partition contributes differently in a weighted co-association matrix depending on the quality of the partitions, as measured by internal and relative validity indices. Based on experimental results in synthetic and real data sets, the weighting of the partitions (WEAC), generally leads to a better performance than EAC. The evaluation of results is based on a consistency index between the combined partition and the "ideal" data partition taken as ground truth},
author = {Duarte, F.J. and a.L.N. Fred and Lourenco, a. and Rodrigues, M.F.},
doi = {10.1109/EPIA.2005.341287},
file = {:home/chiroptera/Dropbox/mendeley/Duarte et al. - 2005 - Weighting Cluster Ensembles in Evidence Accumulation Clustering.pdf:pdf},
isbn = {0-7803-9366-X},
journal = {2005 Portuguese Conference on Artificial Intelligence},
keywords = {Clustering,Combining Multiple Partitions,Validity Indices,Weighting Cluster Ensembles},
pages = {159--167},
title = {{Weighting Cluster Ensembles in Evidence Accumulation Clustering}},
volume = {00},
year = {2005}
}
@article{Blekas2009,
author = {Blekas, Konstantinos and Christodoulidou, K and Lagaris, I E},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Christodoulidou, Lagaris - 2009 - LNCS 5769 - Newtonian Spectral Clustering.pdf:pdf},
pages = {145--154},
title = {{LNCS 5769 - Newtonian Spectral Clustering}},
year = {2009}
}
@article{Kumar2004,
author = {Kumar, Nimit and Behera, Laxmidhar},
doi = {10.1023/B:NEPL.0000039429.89321.07},
issn = {1370-4621},
journal = {Neural Processing Letters},
keywords = {clustering,quantum},
mendeley-tags = {clustering,quantum},
month = aug,
number = {1},
pages = {11--22},
title = {{Visual–Motor Coordination Using a Quantum Clustering Based Neural Control Scheme}},
url = {http://link.springer.com/10.1023/B:NEPL.0000039429.89321.07},
volume = {20},
year = {2004}
}
@article{Wu2009a,
abstract = {In this paper, we report our research on using GPUs to accelerate clustering of very large data sets, which are common in today's real world applications. While many published works have shown that GPUs can be used to accelerate various general purpose applications with respectable performance gains, few attempts have been made to tackle very large problems. Our goal here is to investigate if GPUs can be useful accelerators even with very large data sets that cannot fit into GPU’s onboard memory. Using a popular clustering algorithm, K-Means, as an example, our results have been very positive. On a data set with a billion data points, our GPU-accelerated implementation achieved an order of magnitude performance gain over a highly optimized CPU-only version running on 8 cores, and more than two orders of magnitude gain over a popular benchmark, MineBench, running on a single core.},
author = {Wu, Ren and Zhang, Bin and Hsu, Meichun},
doi = {10.1145/1531666.1531668},
file = {:home/chiroptera/Dropbox/mendeley/Wu, Zhang, Hsu - 2009 - Clustering billions of data points using GPUs.pdf:pdf},
isbn = {9781605585574},
journal = {Proceedings of the combined workshops on UnConventional high performance computing workshop plus memory access workshop},
keywords = {accelerator,clustering,data parallelism,data-mining,gpgpu,gpu,graphics processor,many-core,multi-core,parallel algorithm},
mendeley-tags = {clustering,gpu},
pages = {1--5},
title = {{Clustering billions of data points using GPUs}},
url = {http://portal.acm.org/citation.cfm?id=1531666.1531668},
year = {2009}
}
@article{Harvard2005,
author = {Harvard},
file = {:home/chiroptera/Dropbox/mendeley/Harvard - 2005 - The Assignment Problem and the Hungarian Method.pdf:pdf},
journal = {Introduction to Linear Algebra and Multivariable Calculus},
title = {{The Assignment Problem and the Hungarian Method}},
year = {2005}
}
@article{Chiosa2011,
author = {Chiosa, Iurie and Kolb, Andreas},
file = {:home/chiroptera/Dropbox/mendeley/Chiosa, Kolb - 2011 - GPU-Based Multilevel Clustering.pdf:pdf},
number = {2},
pages = {132--145},
title = {{GPU-Based Multilevel Clustering}},
volume = {17},
year = {2011}
}
@article{Mechanics,
author = {Mechanics, Quantum},
file = {:home/chiroptera/Dropbox/mendeley/Mechanics - Unknown - Strange Bedfellows.pdf:pdf},
title = {{Strange Bedfellows:}}
}
@article{Duarte2005a,
abstract = {We explore the idea of evidence accumulation (EAC) for combining the results of multiple clusterings. The EAC paradigm combines the information existent in n partitions into a co-association matrix (similarity matrix) based on pairwise associations, where each partition has an identical weight in the combination process. The final data partition is obtained by applying a clustering algorithm over this co-association matrix. In this paper we propose the idea of weighting differently the partitions (WEAC). Each partition contributes differently in a weighted co-association matrix depending on the quality of the partitions, as measured by internal and relative validity indices. Based on experimental results in synthetic and real data sets, the weighting of the partitions (WEAC), generally leads to a better performance than EAC. The evaluation of results is based on a consistency index between the combined partition and the "ideal" data partition taken as ground truth},
author = {Duarte, F.J. and a.L.N. Fred and Lourenco, a. and Rodrigues, M.F.},
doi = {10.1109/EPIA.2005.341287},
file = {:home/chiroptera/Dropbox/mendeley/Duarte et al. - 2005 - Weighting Cluster Ensembles in Evidence Accumulation Clustering.pdf:pdf},
isbn = {0-7803-9366-X},
journal = {2005 Portuguese Conference on Artificial Intelligence},
keywords = {Clustering,Combining Multiple Partitions,Validity Indices,Weighting Cluster Ensembles},
pages = {159--167},
title = {{Weighting Cluster Ensembles in Evidence Accumulation Clustering}},
volume = {00},
year = {2005}
}
@article{Harris2007,
abstract = {Parallel prefix sum, also known as parallel Scan, is a useful building block for many parallel algorithms including sorting and building data structures. In this document we introduce Scan and describe step-by-step how it can be implemented efficiently in NVIDIA CUDA. We start with a basic na\"{\i}ve algorithm and proceed through more advanced techniques to obtain best performance. We then explain how to scan arrays of arbitrary size that cannot be processed with a single block of threads.},
author = {Harris, Mark and Sengupta, Shubhabrata and Owens, John D.},
file = {:home/chiroptera/Dropbox/mendeley/Harris, Sengupta, Owens - 2007 - Parallel Prefix Sum (Scan) with CUDA Mark.pdf:pdf},
isbn = {9780321515261},
journal = {Gpu gems 3},
number = {April},
pages = {1--24},
title = {{Parallel Prefix Sum (Scan) with CUDA Mark}},
url = {http://dl.acm.org/citation.cfm?id=1407436},
year = {2007}
}
@article{Chen2012,
abstract = {Accelerators and heterogeneous architectures in general, andGPUs in particular, have recently emerged asmajor players in high perfor- mance computing. For many classes of applications, MapReduce has emerged as the framework for easing parallel programming and improving programmer productivity. There have already been sev- eral efforts on implementingMapReduce on GPUs. In this paper, we propose a new implementation of MapReduce for GPUs, which is very effective in utilizing shared memory, a small programmable cache on modern GPUs. The main idea is to use a reduction-based method to execute aMapReduce applica- tion. The reduction-based method allows us to carry out reductions in shared memory. To support a general and efficient implemen- tation, we support the following features: a memory hierarchy for maintaining the reduction object, a multi-group scheme in shared memory to trade-off space requirements and locking overheads, a general and efficient data structure for the reduction object, and an efficient swapping mechanism. We have evaluated our framework with seven commonly used MapReduce applications and compared it with the sequential im- plementations, MapCG, a recent MapReduce implementation on GPUs, and Ji et al.’s work, a recent MapReduce implementation that utilizes shared memory in a different way. The main observa- tions from our experimental results are as follows. For four of the seven applications that can be considered as reduction-intensive ap- plications, our framework has a speedup of between 5 and 200 over MapCG (for large datasets). Similarly, we achieved a speedup of between 2 and 60 over Ji et al.’s work.},
author = {Chen, Linchuan and Agrawal, Gagan},
doi = {10.1145/2287076.2287109},
file = {:home/chiroptera/Dropbox/mendeley/Chen, Agrawal - 2012 - Optimizing MapReduce for GPUs with Effective Shared Memory Usage.pdf:pdf},
isbn = {9781450308052},
journal = {Proceedings of the 21st international symposium on High-Performance Parallel and Distributed Computing (HPDC'12)},
keywords = {MapReduce,gpu,mapreduce,shared memory},
mendeley-tags = {MapReduce,gpu},
pages = {199--210},
title = {{Optimizing MapReduce for GPUs with Effective Shared Memory Usage}},
url = {http://dl.acm.org/citation.cfm?doid=2287076.2287109$\backslash$nhttp://dl.acm.org/citation.cfm?id=2287109},
year = {2012}
}
@article{Fred2002,
abstract = {We explore the idea of evidence accumulation for combining the results of multiple clusterings. Initially, n d-dimensional data is decomposed into a large number of compact clusters; the K-means algorithm performs this decomposition, with several clusterings obtained by N random initializations of the K-means. Taking the co-occurrences of pairs of patterns in the same cluster as votes for their association, the data partitions are mapped into a co-association matrix of patterns. This n\&amp;times;n matrix represents a new similarity measure between patterns. The final clusters are obtained by applying a MST-based clustering algorithm on this matrix. Results on both synthetic and real data show the ability of the method to identify arbitrary shaped clusters in multidimensional data.},
author = {a.L.N. Fred and a.K. Jain},
doi = {10.1109/ICPR.2002.1047450},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2002 - Data clustering using evidence accumulation.pdf:pdf},
isbn = {0-7695-1695-X},
issn = {1051-4651},
journal = {Object recognition supported by user interaction for service robots},
title = {{Data clustering using evidence accumulation}},
volume = {4},
year = {2002}
}
@inproceedings{Li2010,
abstract = {Based on the concepts and principles of quantum computing, a novel clustering algorithm, called a quantum-inspired immune clonal clustering algorithm based on watershed (QICW), is proposed to deal with the problem of image segmentation. In QICW, antibody is proliferated and divided into a set of subpopulation groups. Antibodies in a subpopulation group are represented by multi-state gene quantum bits. In the antibody's updating, the quantum mutation operator is applied to accelerate convergence. The quantum recombination realizes the information communication between the subpopulation groups so as to avoid premature convergences. In this paper, the segmentation problem is viewed as a combinatorial optimization problem, the original image is partitioned into small blocks by watershed algorithm, and the quantum-inspired immune clonal algorithm is used to search the optimal clustering centre, and make the sequence of maximum affinity function as clustering result, and finally obtain the segmentation result. Experimental results show that the proposed method is effective for texture image and SAR image segmentation, compared with the genetic clustering algorithm based on watershed (W-GAC), and the k-means algorithm based on watershed (W-KM).},
author = {Li, Yangyang and Wu, Nana and Ma, Jingjing and Jiao, Licheng},
booktitle = {IEEE Congress on Evolutionary Computation},
doi = {10.1109/CEC.2010.5586362},
file = {:home/chiroptera/Dropbox/mendeley/Li et al. - 2010 - Quantum-inspired immune clonal clustering algorithm based on watershed.pdf:pdf},
isbn = {978-1-4244-6909-3},
keywords = {Clustering algorithms,Error analysis,Feature extraction,Image segmentation,Partitioning algorithms,Radiative recombination,SAR image segmentation,Wavelet transforms,combinatorial mathematics,combinatorial optimization problem,genetic algorithms,genetic clustering algorithm,image segmentation,image texture,k-means algorithm,maximum affinity function,multistate gene quantum bits,pattern clustering,quantum computing,quantum mutation operator,quantum-inspired immune clonal clustering algorith,synthetic aperture radar,texture image,watershed algorithm},
month = jul,
pages = {1--7},
publisher = {IEEE},
shorttitle = {Evolutionary Computation (CEC), 2010 IEEE Congress},
title = {{Quantum-inspired immune clonal clustering algorithm based on watershed}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5586362},
year = {2010}
}
@article{Dong2010,
abstract = {Building the quantum clustering model by quantum characteristic. It is proved by the Simulation experiment, that It can deal with exceptional, high-dimension complicated data and large-scale data set.},
author = {Dong, Yumin and Jia, Fanghua},
doi = {10.1109/ISIP.2010.111},
file = {:home/chiroptera/Dropbox/mendeley/Dong, Jia - 2010 - A new-style generalized quantum clustering model.pdf:pdf},
isbn = {9780769542614},
journal = {Proceedings - 3rd International Symposium on Information Processing, ISIP 2010},
keywords = {Clustering arithmetic,Quantum clustering model,Quantum entropy},
pages = {519--522},
title = {{A new-style generalized quantum clustering model}},
year = {2010}
}
@article{Herrero-Lopez2011,
abstract = {The uninterrupted growth of information repositories has progressively lead data-intensive applications, such as MapReduce-based systems, to the mainstream. The MapReduce paradigm has frequently proven to be a simple yet flexible and scalable technique to distribute algorithms across thousands of nodes and petabytes of information. Under these circumstances, classic data mining algorithms have been adapted to this model, in order to run in production environments. Unfortunately, the high latency nature of this architecture has relegated the applicability of these algorithms to batch-processing scenarios. In spite of this shortcoming, the emergence of massively threaded shared-memory multiprocessors, such as Graphics Processing Units (GPU), on the commodity computing market has enabled these algorithms to be executed orders of magnitude faster, while keeping the same MapReduce based model. In this paper, we propose the integration of massively threaded shared-memory multiprocessors into MapReduce-based clusters creating a unified heterogeneous architecture that enables executing Map and Reduce operators on thousands of threads across multiple GPU devices and nodes, while maintaining the built-in reliability of the baseline system. For this purpose, we created a programming model that facilitates the collaboration of multiple CPU cores and multiple GPU devices towards the resolution of a data intensive problem. In order to prove the potential of this hybrid system, we take a popular NP-Hard supervised learning algorithm, the Support Vector Machine (SVM) and show that a 36x \&\#x2013; 192x speedup can be achieved on large datasets without changing the model or leaving the commodity hardware paradigm.},
author = {Herrero-Lopez, Sergio},
doi = {10.1109/ICSMC.2011.6083839},
file = {:home/chiroptera/Dropbox/mendeley/Herrero-Lopez - 2011 - Accelerating SVMs by integrating GPUs into MapReduce clusters.pdf:pdf},
isbn = {9781457706523},
issn = {1062922X},
journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
keywords = {Multiprocessing,Parallel Algorithms,Pattern Classification,cluster,gpu,machine learning,mapreduce,svm,topnotch},
mendeley-tags = {cluster,gpu,machine learning,mapreduce,svm,topnotch},
pages = {1298--1305},
title = {{Accelerating SVMs by integrating GPUs into MapReduce clusters}},
year = {2011}
}
@article{Dutton2010,
abstract = {Today's design of avionics is being pulled in two opposite directions - increased use of COTS and more requirements for certification. In particular, graphics processing has largely relied upon COTS devices even though the need for certification of GPUs has been amplified. This paper provides a background on graphics processing, summarizes the avionics requirements, surveys the options for graphics processing in avionics, and presents an architecture for an FPGA-based graphics processor.},
author = {Dutton, Marcus and Keezer, David},
doi = {10.1109/DASC.2010.5655325},
file = {:home/chiroptera/Dropbox/mendeley/Dutton, Keezer - 2010 - The challenges of graphics processing in the avionics industry.pdf:pdf},
isbn = {9781424466160},
issn = {2155-7195},
journal = {AIAA/IEEE Digital Avionics Systems Conference - Proceedings},
pages = {1--9},
title = {{The challenges of graphics processing in the avionics industry}},
year = {2010}
}
@article{Topchy,
author = {Topchy, Alexander and Minaei-bidgoli, Behrouz and Jain, Anil K and Punch, William F and Lansing, E},
file = {:home/chiroptera/Dropbox/mendeley/Topchy et al. - Unknown - Adaptive Clustering Ensembles.pdf:pdf},
number = {i},
title = {{Adaptive Clustering Ensembles}}
}
@article{Guha2003,
abstract = {The data stream model has recently attracted attention for its applicability to numerous types of data, including telephone records, Web documents, and clickstreams. For analysis of such data, the ability to process the data in a single pass, or a small number of passes, while using little memory, is crucial. We describe such a streaming algorithm that effectively clusters large data streams. We also provide empirical evidence of the algorithm's performance on synthetic and real data streams.},
author = {Guha, Sudipto and Meyerson, a},
doi = {10.1109/TKDE.2003.1198387},
file = {:home/chiroptera/Dropbox/mendeley/Guha, Meyerson - 2003 - Clustering data streams Theory and practice.pdf:pdf},
issn = {1041-4347},
journal = {Knowledge and Data \ldots},
pages = {1--33},
title = {{Clustering data streams: Theory and practice}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1198387},
year = {2003}
}
@article{Wang2013,
author = {Wang, Huaixiao and Liu, Jianyong and Zhi, Jun and Fu, Chengqun},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2013 - The Improvement of Quantum Genetic Algorithm and Its Application on Function Optimization.pdf:pdf},
number = {1},
title = {{The Improvement of Quantum Genetic Algorithm and Its Application on Function Optimization}},
volume = {2013},
year = {2013}
}
@article{Fred2005,
author = {Fred, Ana N L and Jain, Anil K},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2005 - Combining multiple clusterings using evidence accumulation.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {K-means,cluster fusion,cluster validity,combining clustering partitions,evidence accumulation,mutual information,robust clustering,single-link},
mendeley-tags = {K-means,cluster fusion,cluster validity,combining clustering partitions,evidence accumulation,mutual information,robust clustering,single-link},
number = {6},
pages = {835--850},
title = {{Combining multiple clusterings using evidence accumulation}},
volume = {27},
year = {2005}
}
@article{Vacaliuc2011,
abstract = {Design of data structures for high performance computing (HPC) is one of the principal challenges facing researchers looking to utilize heterogeneous computing machinery. Heterogeneous systems derive cost, power, and speed efficiency by being composed of the appropriate hardware for the task. Yet, each type of processor requires a specific organization of the application state in order to achieve peak performance. Discovering this and refactoring the code can be a challenging and time-consuming task for the researcher, as the data structures and the computational model must be co-designed. We present a methodology that uses Python as the environment for which to explore tradeoffs in both the data structure design as well as the code executing on the computation accelerator. Our method enables multi-dimensional arrays to be used effectively in any target environment. We have chosen to focus on OpenMP and CUDA environments, thus exploring the development of optimized kernels for the two most common classes of computing hardware available today: multi-core CPU and GPU. Python's large palette of file and network access routines, its associative indexing syntax and support for common HPC environments makes it relevant for diverse hardware ranging from laptops through computing clusters to the highest performance supercomputers. Our work enables researchers to accelerate the development of their codes on the computing hardware of their choice.},
author = {Vacaliuc, Bogdan and Patlolla, Dilip R. and D'Azevedo, Ed and Davidson, Greg G. and Munro, John K. and Evans, Thomas M. and Joubert, Wayne and Bell, Zane W.},
doi = {10.1109/SAAHPC.2011.26},
file = {:home/chiroptera/Dropbox/mendeley/Vacaliuc et al. - 2011 - Python for development of OpenMP and CUDA kernels for multidimensional data.pdf:pdf},
isbn = {9780769544489},
journal = {Proceedings - 2011 Symposium on Application Accelerators in High-Performance Computing, SAAHPC 2011},
keywords = {cuda,openMP,python},
mendeley-tags = {cuda,openMP,python},
pages = {159--167},
title = {{Python for development of OpenMP and CUDA kernels for multidimensional data}},
year = {2011}
}
@article{Singh1977,
annote = {Absolute trash paper. Use only to get references for applications of GPGPU.},
author = {Singh, Sarabjeet},
file = {:home/chiroptera/Dropbox/mendeley/Singh - 1977 - CUDA for GPGPU Applications – A Survey.pdf:pdf},
keywords = {GPGPU,examples},
mendeley-tags = {GPGPU,examples},
pages = {1--4},
title = {{CUDA for GPGPU Applications – A Survey}},
year = {1977}
}
@article{Rodriguez2011,
abstract = {Ultra high density oligonucleotide micro arrays allow several millions of genetic markers in a single experiment to be observed. Current bioinformatics software for gene expression quantile data normalization is unable to process such huge datasets. In parallel with this perception, the huge volume of molecular data produced by current high-throughput technologies in modern molecular biology has increased at a similar pace the challenge in our capacity to process and understand data. On the other hand, the arrival of CUDA has unveiled the extraordinary power of Graphics Processors (GPUs) to accelerate data intensive general purpose computing more and more as times goes by. This work takes these two emerging trends to benefit side by side during the development of a high performance version for a biomedical application of growing popularity: gene expression normalization. A variety of experimental issues are analyzed for this execution, including cost, performance and scalability of the graphics architecture on three different platforms. Our study reveals advantages and drawbacks of using the GPU as target hardware, providing lessons to benefit a broad set of existing genetic applications, either based on those pillars or having similarities with their procedures.},
author = {Rodriguez, Andres and Trelles, Oswaldo and Ujaldon, Manuel},
doi = {10.1109/HPCC.2011.85},
file = {:home/chiroptera/Dropbox/mendeley/Rodriguez, Trelles, Ujaldon - 2011 - Using Graphics Processors for a High Performance Normalization of Gene Expressions.pdf:pdf},
isbn = {978-0-7695-4538-7},
journal = {2011 IEEE International Conference on High Performance Computing and Communications},
keywords = {Bioinformatics,CUDA,Graphics Processing Units (GPUs),High Performance Computing,Normalization of Gene Expressions},
pages = {599--604},
title = {{Using Graphics Processors for a High Performance Normalization of Gene Expressions}},
year = {2011}
}
@misc{Horn2010,
author = {Horn, David and Aviv, Tel and Gottlieb, Assaf and HaSharon, Hod and Axel, Inon and Gan, Ramat},
file = {:home/chiroptera/Dropbox/mendeley/Horn et al. - 2010 - Method and Apparatus for Quantum Clustring.pdf:pdf},
number = {12},
title = {{Method and Apparatus for Quantum Clustring}},
volume = {2},
year = {2010}
}
@article{Nvidia2014,
author = {Nvidia},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia - 2014 - Cuda c programming guide.pdf:pdf},
journal = {Programming Guides},
number = {August},
title = {{Cuda c programming guide}},
year = {2014}
}
@article{Farivar2008,
abstract = {Abstract - Graphics Processing Units (GPU) have recently been the subject of attention in re- search as an efficient coprocessor for implementing many classes of highly parallel applications. The GPUs design is engineered for graphics applications, where many independent SIMD workloads are simultaneously dispatched to processing elements. While parallelism has been explored in the context of traditional CPU threads and SIMD processing elements, the principles involved in dividing the steps of a parallel algorithm for execution on GPU architectures remains a significant challenge. In this paper, we introduce a first step towards building an efficient GPU-based parallel implemen- tation of a commonly used clustering algorithm called K-Means on an NVIDIA G80 PCI express graphics board using the CUDA processing extensions. Clustering algorithms are important for search, data mining, spam and intrusion detection applications. Modern desktop machines commonly include desktop search software that can be greatly enhanced by these advances, while low-power ma- chines such as laptops can reduce power consump- tion by utilizing the video chip for these clustering and indexing operations. Our preliminary results show over a 13x performance improvement com- pared to a baseline 3 GHz Intel Pentium(R) based PC running the same algorithm with an average spec G80 graphics card, the NVIDIA 8600GT. The low cost of these video cards (less than \$100 market price as of 2008), and the high performance gains suggest that our approach is both practical and economical for common applications.},
author = {Farivar, Reza and Rebolledo, Daniel and Chan, Ellick},
file = {:home/chiroptera/Dropbox/mendeley/Farivar, Rebolledo, Chan - 2008 - A parallel implementation of k-means clustering on GPUs.pdf:pdf},
isbn = {1601320841},
journal = {on Parallel and},
pages = {1--6},
title = {{A parallel implementation of k-means clustering on GPUs}},
url = {http://nguyendangbinh.org/Proceedings/IPCV08/Papers/PDP3663.pdf},
year = {2008}
}
@article{Gupta2014,
author = {Gupta, Siddharth and Palsetia, Diana and Patwary, Mostofa Ali and Agrawal, Ankit and Choudhary, Alok},
doi = {10.1109/IPDPSW.2014.152},
file = {:home/chiroptera/Dropbox/mendeley/Gupta et al. - 2014 - A New Parallel Algorithm for Two-Pass Connected Component Labeling.pdf:pdf},
isbn = {9781479941162},
title = {{A New Parallel Algorithm for Two-Pass Connected Component Labeling}},
year = {2014}
}
@article{Zechner2009,
abstract = {In this paper an optimized k-means implementation on the graphics processing unit (GPU) is presented. NVIDIApsilas compute unified device architecture (CUDA), available from the G80 GPU family onwards, is used as the programming environment. Emphasis is placed on optimizations directly targeted at this architecture to best exploit the computational capabilities available. Additionally drawbacks and limitations of previous related work, e.g. maximum instance, dimension and centroid count are addressed. The algorithm is realized in a hybrid manner, parallelizing distance calculations on the GPU while sequentially updating cluster centroids on the CPU based on the results from the GPU calculations. An empirical performance study on synthetic data is given, demonstrating a maximum 14times speed increase to a fully SIMD optimized CPU implementation.},
author = {Zechner, Mario and Granitzer, Michael},
doi = {10.1109/INTENSIVE.2009.19},
file = {:home/chiroptera/Dropbox/mendeley/Zechner, Granitzer - 2009 - Accelerating k-means on the graphics processor via CUDA.pdf:pdf},
isbn = {9780769535852},
journal = {Proceedings of the 1st International Conference on Intensive Applications and Services, INTENSIVE 2009},
pages = {7--15},
title = {{Accelerating k-means on the graphics processor via CUDA}},
year = {2009}
}
@article{Cs2003,
author = {Cs, Sael Lee and Biology, Computational},
file = {:home/chiroptera/Dropbox/mendeley/Cs, Biology - 2003 - Lecture 16 pca and svd.pdf:pdf},
title = {{Lecture 16: pca and svd}},
year = {2003}
}
@article{McColl2013,
abstract = {Social networks, communication networks, busi- ness intelligence databases, and large scientific data sources now contain hundreds of millions elements with billions of relationships. The relationships in these massive datasets are changing at ever-faster rates. Through representing these datasets as dynamic and semantic graphs of vertices and edges, it is possible to characterize the structure of the relationships and to quickly respond to queries about how the elements in the set are connected. Statically computing analytics on snapshots of these dynamic graphs is frequently not fast enough to provide current and accurate information as the graph changes. This has led to the development of dynamic graph algorithms that can maintain analytic information without resorting to full static recomputation. In this work we present a novel parallel algorithm for tracking the connected components of a dynamic graph. Our approach has a low memory requirement of O(V ) and is appropriate for all graph densities. On a graph with 512 million edges, we show that our new dynamic algorithm is up to 128X faster than well-known static algorithms and that our algorithm achieves a 14X parallel speedup on a x86 64-core shared-memory system. To the best of the authors’ knowledge, this is the first parallel implementation of dynamic connected components that does not eventually require static recomputation.},
author = {McColl, Robert and Green, Oded and Bader, David a.},
doi = {10.1109/HiPC.2013.6799108},
file = {:home/chiroptera/Dropbox/mendeley/McColl, Green, Bader - 2013 - A new parallel algorithm for connected components in dynamic graphs.pdf:pdf},
isbn = {978-1-4799-0730-4},
journal = {20th Annual International Conference on High Performance Computing, HiPC 2013},
pages = {246--255},
title = {{A new parallel algorithm for connected components in dynamic graphs}},
year = {2013}
}
@article{Chang2014,
author = {Chang, Wan Yu and Chiu, Chung Cheng},
doi = {10.1109/IS3C.2014.81},
file = {:home/chiroptera/Dropbox/mendeley/Chang, Chiu - 2014 - Directional Connected Components Algorithm Based on Gradient Information.pdf:pdf},
isbn = {978-1-4799-5277-9},
journal = {2014 International Symposium on Computer, Consumer and Control},
keywords = {-directional connected components,components,connected,gradient information,overlapping components},
pages = {280--283},
title = {{Directional Connected Components Algorithm Based on Gradient Information}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6845873},
year = {2014}
}
@article{Grossman2013,
abstract = {As the scale of high performance computing systems grows, three main challenges arise: the programmability, reliability, and energy efficiency of those systems. Accomplishing all three without sacrificing performance requires a rethinking of legacy distributed programming models and homogeneous clusters. In this work, we integrate Hadoop MapReduce with OpenCL to enable the use of heterogeneous processors in a distributed system. We do this by exploiting the implicit data- parallelism of mappers and reducers in a MapReduce system. Combining Hadoop and OpenCL provides 1) an easy-to-learn and flexible application programming interface in a high level and popular programming language, 2) the reliability guarantees and distributed filesystem of Hadoop, and 3) the low power consumption and performance acceleration of heterogeneous processors. This paper presents HadoopCL: an extension to Hadoop which supports execution of user-written Java kernels on heterogeneous devices, optimizes communication through asynchronous transfers and dedicated I/O threads, automatically generates OpenCL kernels from Java bytecode using the open source tool APARAPI, and achieves nearly 3x overall speedup and better than 55x speedup of the computational sections for example MapReduce applications, relative to Hadoop.},
author = {Grossman, Max and Breternitz, Mauricio and Sarkar, Vivek},
doi = {10.1109/IPDPSW.2013.246},
file = {:home/chiroptera/Dropbox/mendeley/Grossman, Breternitz, Sarkar - 2013 - HadoopCL MapReduce on distributed heterogeneous platforms through seamless integration of hadoop a.pdf:pdf},
isbn = {978-0-7695-4979-8},
journal = {Proceedings - IEEE 27th International Parallel and Distributed Processing Symposium Workshops and PhD Forum, IPDPSW 2013},
keywords = {GPGPU,Hadoop,OpenCL,heterogeneous,multicore},
pages = {1918--1927},
title = {{HadoopCL: MapReduce on distributed heterogeneous platforms through seamless integration of hadoop and OpenCL}},
year = {2013}
}
@article{Stovall2014,
author = {Stovall, Thomas and Kockara, Sinan and Avci, Recep},
doi = {10.1109/TPDS.2014.2374607},
file = {:home/chiroptera/Dropbox/mendeley/Stovall, Kockara, Avci - 2014 - GPUSCAN GPU-based Parallel Structural Clustering Algorithm for Networks.pdf:pdf},
issn = {1045-9219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {clustering,gpu},
mendeley-tags = {clustering,gpu},
number = {c},
pages = {1--1},
title = {{GPUSCAN: GPU-based Parallel Structural Clustering Algorithm for Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6967853},
volume = {9219},
year = {2014}
}
@article{Lee2012,
abstract = {A prominent parallel data processing tool MapReduce is gaining significant momentum from both industry and academia as the volume of data to analyze grows rapidly. While MapReduce is used in many areas where massive data analysis is required, there are still debates on its performance, efficiency per node, and simple abstraction. This survey intends to assist the database and open source communities in understanding various technical aspects of the MapReduce framework. In this survey, we characterize the MapReduce framework and discuss its inherent pros and cons. We then introduce its optimization strategies reported in the recent literature. We also discuss the open issues and challenges raised on parallel data analysis with MapReduce.},
author = {Lee, Kyong-Ha and Lee, Yoon-Joon and Choi, Hyunsik and Chung, Yon Dohn and Moon, Bongki},
doi = {10.1145/2094114.2094118},
file = {:home/chiroptera/Dropbox/mendeley/Lee et al. - 2012 - Parallel data processing with MapReduce.pdf:pdf},
isbn = {0163-5808},
issn = {01635808},
journal = {ACM SIGMOD Record},
keywords = {MapReduce,clusters,distributed computing,hadoop,ing,mapreduce,parallel data process-},
mendeley-tags = {MapReduce},
number = {4},
pages = {11},
title = {{Parallel data processing with MapReduce}},
volume = {40},
year = {2012}
}
@article{Mullner2013,
abstract = {The fastcluster package is a C++ library for hierarchical, agglomerative clustering. It provides a fast implementation of the most efficient, current algorithms when the input is a dissimilarity index. Moreover, it features memory-saving routines for hierarchical clustering of vector data. It improves both asymptotic time complexity (in most cases) and practical performance (in all cases) compared to the existing implementations in standard software: several R packages, MATLAB, Mathematica, Python with SciPy.},
archivePrefix = {arXiv},
arxivId = {1109.2378},
author = {M\"{u}llner, Daniel},
eprint = {1109.2378},
file = {:home/chiroptera/Dropbox/mendeley/M\"{u}llner - 2013 - fastcluster Fast Hierarchical , Agglomerative.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {age,agglomerative,algorithm,aver-,c,centroid,clustering,complete,hierarchical,linkage,mathematica,matlab,mcquitty,median,python,scipy,single,upgma,upgmc,ward,weighted,wpgma,wpgmc},
number = {9},
pages = {1--18},
title = {{fastcluster : Fast Hierarchical , Agglomerative}},
url = {http://www.jstatsoft.org/v53/i09},
volume = {53},
year = {2013}
}
@article{Bai2009,
abstract = {K-means algorithm is one of the most famous unsupervised clustering algorithms. Many theoretical improvements for the performance of original algorithms have been put forward, while almost all of them are based on single instruction single data (SISD) architecture processors (GPUs), which partly ignored the inherent paralleled characteristic of the algorithms. In this paper, a novel single instruction multiple data (SIMD) architecture processors (GPUs) based k-means algorithm is proposed. In this algorithm, in order to accelerate compute-intensive portions of traditional k-means, both data objects assignment and k-centroids recalculation are offloaded to the GPU in parallel. We have implemented this GPU-based k-means on the newest generation GPU with compute unified device architecture(CUDA). The numerical experiments demonstrated that the speed of GPU-based k-means could reach as high as 40 times of the CPU-based k-means.},
annote = {Doesn't say which tools were used (C, Matlab, Fortran???).},
author = {Bai, Hong Tao and He, Li Li and Ouyang, Dan Tong and Li, Zhan Shan and Li, He},
doi = {10.1109/CSIE.2009.491},
file = {:home/chiroptera/Dropbox/mendeley/Bai et al. - 2009 - K-means on commodity GPUs with CUDA.pdf:pdf},
isbn = {9780769535074},
journal = {2009 WRI World Congress on Computer Science and Information Engineering, CSIE 2009},
pages = {651--655},
title = {{K-means on commodity GPUs with CUDA}},
volume = {3},
year = {2009}
}
@phdthesis{SousaThesis,
author = {Sousa, Cristiano Rafael da Silva},
file = {:home/chiroptera/Dropbox/mendeley/Sousa - 2014 - Efficient sequential and parallel versions of MST-solvers for multi-core CPU-chips and GPUs.pdf:pdf},
title = {{Efficient sequential and parallel versions of MST-solvers for multi-core CPU-chips and GPUs}},
year = {2014}
}
@article{Fred2009,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt. 3 Validation of Custering Solutions.pdf:pdf},
number = {April},
pages = {1--15},
title = {{Tutorial Pt. 3: Validation of Custering Solutions}},
year = {2009}
}
@article{Harish2007,
abstract = {Large graphs involving millions of vertices are common in many practical applications and are challenging to process. Practical-time implementations using high-end computers are reported but are accessible only to a few. Graphics Processing Units (GPUs) of today have ... $\backslash$n},
author = {Harish, Pawan and Narayanan, Pj},
doi = {10.1007/978-3-540-77220-0\_21},
file = {:home/chiroptera/Dropbox/mendeley/Harish, Narayanan - 2007 - Accelerating large graph algorithms on the GPU using CUDA.pdf:pdf},
isbn = {9783540772194},
issn = {978-3-540-77219-4},
journal = {High performance computing–HiPC 2007},
number = {Chapter 21},
pages = {197--208},
title = {{Accelerating large graph algorithms on the GPU using CUDA}},
url = {http://www.springerlink.com/index/10.1007/978-3-540-77220-0\_21$\backslash$npapers2://publication/doi/10.1007/978-3-540-77220-0\_21$\backslash$nhttp://link.springer.com/chapter/10.1007/978-3-540-77220-0\_21},
volume = {4873},
year = {2007}
}
@article{Strehl2000,
author = {Strehl, Alexander and Ghosh, Joydeep},
doi = {10.1162/153244303321897735},
file = {:home/chiroptera/Dropbox/mendeley/Strehl, Ghosh - 2000 - Cluster Ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions.pdf:pdf},
issn = {0003-6951},
journal = {CrossRef Listing of Deleted DOIs},
keywords = {cluster analysis,clustering,consensus functions,ensemble,knowledge reuse,multi-learner systems,mutual information,partitioning,unsupervised learning},
pages = {583--617},
title = {{Cluster Ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions}},
url = {http://dl.acm.org/citation.cfm?id=944919.944935},
volume = {1},
year = {2000}
}
@article{Guo2013,
author = {Guo, Yiru and Liu, Weiguo and Voss, Gerrit and Mueller-Wittig, Wolfgang},
doi = {10.1109/HPCC.and.EUC.2013.88},
file = {:home/chiroptera/Dropbox/mendeley/Guo et al. - 2013 - GCMR A GPU Cluster-Based MapReduce Framework for Large-Scale Data Processing.pdf:pdf},
isbn = {978-0-7695-5088-6},
journal = {2013 IEEE 10th International Conference on High Performance Computing and Communications \& 2013 IEEE International Conference on Embedded and Ubiquitous Computing},
keywords = {-mapreduce,1,15,3,8,9,MapReduce,and bioinformatics,big data,cluster,cuda,database operations,due to the continuing,gpu,gpu cluster,image processing,mpi,rapid growth of data,size in},
mendeley-tags = {MapReduce,big data,cluster,gpu},
pages = {580--586},
title = {{GCMR: A GPU Cluster-Based MapReduce Framework for Large-Scale Data Processing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6831970},
year = {2013}
}
@article{Bennett,
author = {Bennett, Charles},
file = {:home/chiroptera/Dropbox/mendeley/Bennett - Unknown - Is quantum search practical.pdf:pdf},
pages = {22--30},
title = {{Is quantum search practical?}}
}
@article{Pennycook2013,
abstract = {This paper reports on the development of an MPI/OpenCL implementation of LU, an application-level benchmark from the NAS Parallel Benchmark Suite. An account of the design decisions addressed during the development of this code is presented, demonstrating the importance of memory arrangement and work-item/work-group distribution strategies when applications are deployed on different device types. The resulting platform-agnostic, single source application is benchmarked on a number of different architectures, and is shown to be 1.3-1.5× slower than native FORTRAN 77 or CUDA implementations on a single node and 1.3-3.1× slower on multiple nodes. We also explore the potential performance gains of OpenCL's device fissioning capability, demonstrating up to a 3× speed-up over our original OpenCL implementation. © 2013 Elsevier Ltd. All rights reserved.},
author = {Pennycook, S. J. and Hammond, S. D. and Wright, S. a. and Herdman, J. a. and Miller, I. and Jarvis, S. a.},
doi = {10.1016/j.jpdc.2012.07.005},
file = {:home/chiroptera/Dropbox/mendeley/Pennycook et al. - 2013 - An investigation of the performance portability of OpenCL.pdf:pdf},
isbn = {Pennycook, S.J., Hammond, S.D., Wright, S.A., Herdman, J.A., Miller, I. and Jarvis, S.A. (2012) An Investigation of the Performance Portability of OpenCL. Journal of Parallel and Distributed Computing. ISSN 0743-7315 (In Press)},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {GPU computing,High performance computing,Many-core computing,OpenCL,Optimisation,opencl},
mendeley-tags = {opencl},
number = {11},
pages = {1439--1450},
publisher = {Elsevier Inc.},
title = {{An investigation of the performance portability of OpenCL}},
url = {http://dx.doi.org/10.1016/j.jpdc.2012.07.005},
volume = {73},
year = {2013}
}
@article{Jarvis1973,
abstract = {A nonparametric clustering technique incorporating the concept of similarity based on the sharing of near neighbors is pre- sented. In addition to being an essentially paraliel approach, the com- putational elegance of the method is such that the scheme is applicable to a wide class of practical problems involving large sample size and high dimensionality. No attempt is made to show how a priori problem knowledge can be introduced into the procedure.},
author = {Jarvis, R a and Patrick, Edward a},
doi = {10.1109/T-C.1973.223640},
file = {:home/chiroptera/Dropbox/mendeley/Jarvis, Patrick - 1973 - Clustering Using a Similarity Measure Based on Shared Near Neighbors.pdf:pdf},
isbn = {00189340 (ISSN)},
issn = {0018-9340},
journal = {Ieee Transactions on Computers},
keywords = {Clustering,nonparametric,pattern recognition,shared near neighbors,similarity measure.},
number = {11},
pages = {1025--1034},
title = {{Clustering Using a Similarity Measure Based on Shared Near Neighbors}},
volume = {C-22},
year = {1973}
}
@article{Soman2010,
abstract = {Graphics processing units provide a large computational power at a very low price which position them as an ubiquitous accelerator. General purpose programming on the graphics processing units (GPGPU) is best suited for regular data parallel algorithms. They are not directly amenable for algorithms which have irregular data access patterns such as list ranking, and finding the connected components of a graph, and the like. In this work, we present a GPU-optimized implementation for finding the connected components of a given graph. Our implementation tries to minimize the impact of irregularity, both at the data level and functional level. Our implementation achieves a speed up of 9 to 12 times over the best sequential CPU implementation. For instance, our implementation finds connected components of a graph of 10 million nodes and 60 million edges in about 500 milliseconds on a GPU, given a random edge list. We also draw interesting observations on why PRAM algorithms, such as the Shiloach-Vishkin algorithm may not be a good fit for the GPU and how they should be modified.},
author = {Soman, Jyothish and Kishore, Kothapalli and Narayanan, P. J.},
doi = {10.1109/IPDPSW.2010.5470817},
file = {:home/chiroptera/Dropbox/mendeley/Soman, Kishore, Narayanan - 2010 - A fast GPU algorithm for graph connectivity.pdf:pdf},
isbn = {9781424465347},
journal = {Proceedings of the 2010 IEEE International Symposium on Parallel and Distributed Processing, Workshops and Phd Forum, IPDPSW 2010},
keywords = {Connected components,GPGPU,GPU,Irregular algorithms},
title = {{A fast GPU algorithm for graph connectivity}},
year = {2010}
}
@article{Horn2003,
abstract = {MOTIVATION: This paper introduces the application of a novel clustering method to microarray expression data. Its first stage involves compression of dimensions that can be achieved by applying SVD to the gene-sample matrix in microarray problems. Thus the data (samples or genes) can be represented by vectors in a truncated space of low dimensionality, 4 and 5 in the examples studied here. We find it preferable to project all vectors onto the unit sphere before applying a clustering algorithm. The clustering algorithm used here is the quantum clustering method that has one free scale parameter. Although the method is not hierarchical, it can be modified to allow hierarchy in terms of this scale parameter. RESULTS: We apply our method to three data sets. The results are very promising. On cancer cell data we obtain a dendrogram that reflects correct groupings of cells. In an AML/ALL data set we obtain very good clustering of samples into four classes of the data. Finally, in clustering of genes in yeast cell cycle data we obtain four groups in a problem that is estimated to contain five families. AVAILABILITY: Software is available as Matlab programs at http://neuron.tau.ac.il/\~{}horn/QC.htm.},
author = {Horn, David and Axel, Inon},
doi = {10.1093/bioinformatics/btg053},
file = {:home/chiroptera/Dropbox/mendeley/Horn, Axel - 2003 - Novel clustering algorithm for microarray expression data in a truncated SVD space.ps:ps;:home/chiroptera/Dropbox/mendeley/Horn, Axel - 2003 - Novel clustering algorithm for microarray expression data in a truncated SVD space.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
pages = {1110--1115},
pmid = {12801871},
title = {{Novel clustering algorithm for microarray expression data in a truncated SVD space}},
volume = {19},
year = {2003}
}
@book{Aggarwal2014,
author = {Aggarwal, Charu C and Reddy, Chandan K},
file = {:home/chiroptera/Dropbox/mendeley/Aggarwal, Reddy - Unknown - Data clustering algorithms and applications.pdf:pdf},
isbn = {9781466558229},
title = {{Data clustering algorithms and applications}}
}
@article{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, ran- domized seeding technique, we obtain an algorithm that is $\Theta$(log k)-competitive with the optimal clustering. Prelim- inary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, D. and Arthur, D. and Vassilvitskii, S. and Vassilvitskii, S.},
doi = {10.1145/1283383.1283494},
file = {:home/chiroptera/Dropbox/mendeley/Arthur et al. - 2007 - k-means The advantages of careful seeding.pdf:pdf},
isbn = {978-0-898716-24-5},
journal = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
pages = {1027--1035},
title = {{k-means++: The advantages of careful seeding}},
url = {http://portal.acm.org/citation.cfm?id=1283494},
volume = {8},
year = {2007}
}
@article{Fred2009c,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt.1 Basic Concepts of data clustering.pdf:pdf},
number = {April},
pages = {1--17},
title = {{Tutorial Pt.1: Basic Concepts of data clustering}},
year = {2009}
}
@article{Shell2008,
abstract = {Despite growing evidence that increased brain-derived neurotrophic factor (BDNF) and hippocampal adult neurogenesis are necessary for the behavioral actions of antidepressants in rodents, the cellular mechanisms involved in these effects are still unknown. Li et al. in this issue of Neuron demonstrate that the presence of TrkB, the high-affinity receptor for BDNF, in hippocampal neural progenitor cells is required for the neurogenic and behavioral actions of antidepressant treatments.},
author = {Shell, M.},
doi = {10.1016/j.neuron.2008.07.028},
issn = {1097-4199},
journal = {Neuron},
keywords = {Adult Stem Cells,Adult Stem Cells: drug effects,Animals,Antidepressive Agents,Antidepressive Agents: pharmacology,Cell Proliferation,Cell Proliferation: drug effects,Mice,Neurons,Neurons: drug effects,Receptor,trkB,trkB: metabolism},
number = {3},
pages = {349--51},
pmid = {18701059},
title = {{How to Use the IEEEtran L TEX Class}},
url = {http://www.cs.northwestern.edu/~sle841/papers/Arch\_TVCG/misc/tex files/original\_tvcg\_files/IEEEtran\_HOWTO.pdf$\backslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/18701059},
volume = {59},
year = {2008}
}
@article{Meila2003,
abstract = {This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information (VI), measures the amount of information lost and gained in changing from clustering \$\{\backslash cal C\}\$ to clustering \$\{\backslash cal C\}'\$ . The criterion makes no assumptions about how the clusterings were generated and applies to both soft and hard clusterings. The basic properties of VI are presented and discussed from the point of view of comparing clusterings. In particular, the VI is positive, symmetric and obeys the triangle inequality. Thus, surprisingly enough, it is a true metric on the space of clusterings. Keywords: Clustering; Comparing partitions; Measures of agreement; Information theory; Mutual information},
author = {Meila, Marina},
doi = {10.1007/978-3-540-45167-9\_14},
file = {:home/chiroptera/Dropbox/mendeley/Meila - 2003 - Comparing clusterings by the variation of information.pdf:pdf},
isbn = {978-3-540-40720-1, 978-3-540-45167-9},
issn = {03029743},
journal = {Learning theory and Kernel machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003: proceedings},
keywords = {clustering,comparing partitions,information theory,measures of agreement,mutual information},
pages = {173},
title = {{Comparing clusterings by the variation of information}},
url = {http://books.google.com/books?hl=en\&amp;lr=\&amp;id=hk1dqsM0XF4C\&amp;oi=fnd\&amp;pg=PA173\&amp;dq=Comparing+Clusterings+by+the+Variation+of+Information\&amp;ots=7rcmrLpFV1\&amp;sig=P-AXGQnlenPfAlSb3fdhphYv6dI},
year = {2003}
}
@article{Hung2013,
author = {Hung, Chih-cheng and Casper, Ellis and Kuo, Bor-chen and Liu, Wenping and Yu, Xiaoyi and Jung, Edward and Yang, Ming},
file = {:home/chiroptera/Dropbox/mendeley/Hung et al. - 2013 - A QUANTUM-MODELED FUZZY C-MEANS CLUSTERING ALGORITHM FOR REMOTELY SENSED MULTI-BAND IMAGE SEGMENTATION.pdf:pdf},
isbn = {9781479911141},
pages = {2501--2504},
title = {{A QUANTUM-MODELED FUZZY C-MEANS CLUSTERING ALGORITHM FOR REMOTELY SENSED MULTI-BAND IMAGE SEGMENTATION}},
year = {2013}
}
@article{Nvidia2009,
abstract = {The lethal outcome of high-dose pulmonary virus infection is thought to reflect high-level, sustained virus replication and associated lung inflammation prior to development of an adaptive immune response. Herein, we demonstrate that the outcome of lethal/sublethal influenza infection instead correlates with the initial virus replication tempo. Furthermore, the magnitude of early lung antiviral CD8+ T cell responses varies inversely with inoculum dose and is controlled by lymph-node-resident dendritic cells (LNDC) through IL-12p40-regulated FasL-dependent T cell apoptosis. These results suggest that the inoculum dose and replication rate of a pathogen entering the respiratory tract may regulate the strength of the adaptive immune response, and the subsequent outcome of infection and that LNDC may serve as regulators (gatekeepers) in the development of CD8+ T cell responses.},
author = {Nvidia, Whitepaper and Generation, Next and Compute, Cuda},
doi = {10.1016/j.immuni.2005.11.006},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia, Generation, Compute - 2009 - Whitepaper NVIDIA’s Next Generation CUDA Compute Architecture.pdf:pdf},
issn = {10747613},
journal = {ReVision},
number = {6},
pages = {1--22},
pmid = {16356862},
title = {{Whitepaper NVIDIA’s Next Generation CUDA Compute Architecture}},
url = {http://www.nvidia.com/content/PDF/fermi\_white\_papers/NVIDIA\_Fermi\_Compute\_Architecture\_Whitepaper.pdf},
volume = {23},
year = {2009}
}
@article{Winterstein1997,
author = {Winterstein, Felix and Bayliss, Samuel and Constantinides, Geoge a.},
file = {:home/chiroptera/Dropbox/mendeley/Winterstein, Bayliss, Constantinides - 1997 - FPGA-Based K-Means Clustering Using Tree-Based Data Structures.pdf:pdf},
pages = {1450--1455},
title = {{FPGA-Based K-Means Clustering Using Tree-Based Data Structures}},
volume = {101},
year = {1997}
}
@article{Fred2003,
author = {Fred, Ana L N},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2003 - A New Cluster Isolation Criterion Based on Dissimilarity Increments ´.pdf:pdf},
number = {8},
pages = {1--15},
title = {{A New Cluster Isolation Criterion Based on Dissimilarity Increments ´}},
volume = {25},
year = {2003}
}
@article{Banerjee,
author = {Banerjee, Dip Sankar and Sharma, Shashank and Kothapalli, Kishore},
file = {:home/chiroptera/Dropbox/mendeley/Banerjee, Sharma, Kothapalli - Unknown - Work Efficient Parallel Algorithms for Large Graph Exploration.pdf:pdf},
isbn = {9781479907304},
keywords = {gpu,graph},
mendeley-tags = {gpu,graph},
pages = {433--442},
title = {{Work Efficient Parallel Algorithms for Large Graph Exploration}}
}
@article{Elteir2011,
abstract = {MapReduce is a programming model from Google that facilitates parallel processing on a cluster of thousands of commodity computers. The success of MapReduce in cluster environments has motivated several studies of implementing MapReduce on a graphics processing unit (GPU), but generally focusing on the NVIDIA GPU. Our investigation reveals that the design and mapping of the MapReduce framework needs to be revisited for AMD GPUs due to their notable architectural differences from NVIDIA GPUs. For instance, current state-of-the-art MapReduce implementations employ atomic operations to coordinate the execution of different threads. However, atomic operations can implicitly cause inefficient memory access, and in turn, severely impact performance. In this paper, we propose Streamer, an OpenCL MapReduce framework optimized for AMD GPUs. With efficient atomic-free algorithms for output handling and intermediate result shuffling, Stream MR is superior to atomic-based MapReduce designs and can outperform existing atomic-free MapReduce implementations by nearly five-fold on an AMD Radeon HD 5870.},
author = {Elteir, Marwa and Lin, Heshan and Feng, Wu Chun and Scogland, Tom},
doi = {10.1109/ICPADS.2011.131},
file = {:home/chiroptera/Dropbox/mendeley/Elteir et al. - 2011 - StreamMR An optimized MapReduce framework for AMD GPUs.pdf:pdf},
isbn = {9780769545769},
issn = {15219097},
journal = {Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS},
keywords = {AMD GPU,Atomics,GPGPU,MapCG,MapReduce,Mars,OpenCL,Parallel computing},
pages = {364--371},
title = {{StreamMR: An optimized MapReduce framework for AMD GPUs}},
year = {2011}
}
@article{Pettie2002,
author = {Pettie, Seth and Ramachandran, Vijaya},
doi = {10.1137/S0097539700371065},
file = {:home/chiroptera/Dropbox/mendeley/Pettie, Ramachandran - 2002 - A Randomized Time-Work Optimal Parallel Algorithm for Finding a Minimum Spanning Forest.pdf:pdf},
issn = {0097-5397},
journal = {SIAM Journal on Computing},
number = {6},
pages = {1879--1895},
title = {{A Randomized Time-Work Optimal Parallel Algorithm for Finding a Minimum Spanning Forest}},
volume = {31},
year = {2002}
}
@article{Republic2010,
author = {Republic, Czech and Mareboyana, Manohar},
file = {:home/chiroptera/Dropbox/mendeley/Republic, Mareboyana - 2010 - GPU Accelerated One-pass Algorithm for Computing Minimal Rectangles of Connected Components Lubom\'{\i}r Ř\'{\i}h.pdf:pdf},
isbn = {9781424494972},
journal = {Components},
pages = {479--484},
title = {{GPU Accelerated One-pass Algorithm for Computing Minimal Rectangles of Connected Components Lubom\'{\i}r Ř\'{\i}ha Manohar Mareboyana Bowie State University}},
year = {2010}
}
@article{Kim2011,
abstract = {Hierarchical clustering is an important and powerful but computationally extensive operation. Its complexity motivates the exploration of highly parallel approaches such as Adaptive Resonance Theory (ART). Although ART has been implemented on GPU processors, this paper presents the first hierarchical ART GPU implementation we are aware of. Each ART layer is distributed in the GPU's multiprocessors and is trained simultaneously. The experimental results show that for deep trees, the GPU's performance advantage is significant.},
author = {Kim, Sejun and Wunsch, Donald C.},
doi = {10.1109/IJCNN.2011.6033584},
file = {:home/chiroptera/Dropbox/mendeley/Kim, Wunsch - 2011 - A GPU based parallel Hierarchical Fuzzy ART clustering.pdf:pdf},
isbn = {9781457710865},
issn = {2161-4393},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {GPU,clustering},
mendeley-tags = {GPU,clustering},
number = {5},
pages = {2778--2782},
title = {{A GPU based parallel Hierarchical Fuzzy ART clustering}},
year = {2011}
}
@article{Halkidi2001,
abstract = {Cluster analysis aims at identifying groups of similar objects and, therefore helps to discover distribution of patterns and interesting correlations in large data sets. It has been subject of wide research since it arises in many application domains in engineering, business and social sciences. Especially, in the last years the availability of huge transactional and experimental data sets and the arising requirements for data mining created needs for clustering algorithms that scale and can be applied in diverse domains.},
author = {Halkidi, Maria and Batistakis, Yannis and Vazirgiannis, Michalis},
doi = {10.1023/A:1012801612483},
file = {:home/chiroptera/Dropbox/mendeley/Halkidi, Batistakis, Vazirgiannis - 2001 - On clustering validation techniques.pdf:pdf},
isbn = {0925-9902},
issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Cluster validity,Clustering algorithms,Unsupervised learning,Validity indices},
number = {2-3},
pages = {107--145},
title = {{On clustering validation techniques}},
volume = {17},
year = {2001}
}
@book{Lanzagorta2008,
author = {Lanzagorta, Marco and Uhlmann, Jeffrey},
booktitle = {Synthesis Lectures on Quantum Computing},
doi = {10.2200/S00159ED1V01Y200810QMC002},
file = {:home/chiroptera/Dropbox/mendeley/Lanzagorta, Uhlmann - 2008 - Quantum Computer Science.pdf:pdf},
isbn = {9780511813870},
issn = {1945-9726},
pages = {1--124},
title = {{Quantum Computer Science}},
volume = {1},
year = {2008}
}
@article{Klipfel1943,
author = {Klipfel, Joel},
file = {:home/chiroptera/Dropbox/mendeley/Klipfel - 1943 - A brief introduction to hilbert space and quantum logic.pdf:pdf},
pages = {1--31},
title = {{A brief introduction to hilbert space and quantum logic}},
year = {1943}
}
@article{Zou2014a,
abstract = {Bioinformatics is challenged by the fact that traditional analysis tools have difficulty in processing large-scale data from high-throughput sequencing. The open source Apache Hadoop project, which adopts the MapReduce framework and a distributed file system, has recently given bioinformatics researchers an opportunity to achieve scalable, efficient and reliable computing performance on Linux clusters and on cloud computing services. In this article, we present MapReduce frame-based applications that can be employed in the next-generation sequencing and other biological domains. In addition, we discuss the challenges faced by this field as well as the future works on parallel computing in bioinformatics.},
author = {Zou, Quan and Li, Xu Bin and Jiang, Wen Rui and Lin, Zi Yu and Li, Gui Lin and Chen, Ke},
doi = {10.1093/bib/bbs088},
file = {:home/chiroptera/Dropbox/mendeley//Zou et al. - 2014 - Survey of MapReduce frame operation in bioinformatics.pdf:pdf},
isbn = {1477-4054 (Electronic)$\backslash$n1467-5463 (Linking)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics,Hadoop,MapReduce,bioinformatics},
mendeley-tags = {MapReduce,bioinformatics},
number = {4},
pages = {637--647},
pmid = {23396756},
title = {{Survey of MapReduce frame operation in bioinformatics}},
volume = {15},
year = {2014}
}
@article{Nvidia,
author = {Nvidia, Whitepaper and Generation, Next and Compute, Cuda},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia, Generation, Compute - Unknown - NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf:pdf},
title = {{NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper}}
}
@article{Demar2012,
abstract = {Exascale science translates to big data. In the case of the Large Hadron Collider (LHC), the data is not only immense, it is also globally distributed. Fermilab is host to the LHC Compact Muon Solenoid (CMS) experiment's US Tier-1 Center, the largest of the LHC Tier-1s. The Laboratory must deal with both scaling and wide-area distribution challenges in processing its CMS data. Fortunately, evolving technologies in the form of 100Gigabit ethernet, multi-core architectures, and GPU processing provide tools to help meet these challenges. Current Fermilab R\&amp;D efforts in these areas include optimization of network I/O handling in multi-core systems, modification of middleware to improve application performance in 100GE network environments, and network path reconfiguration and analysis for effective use of high bandwidth networks. This poster will describe the ongoing network-related R\&amp;D activities at Fermilab as a mosaic of efforts that combine to facilitate big data processing and movement. © 2012 IEEE.},
author = {Demar, Phillip J. and Dykstra, David and Garzoglio, Gabriele and Mhashilkar, Parag and Rajendran, Anupam and Wu, Wenji},
doi = {10.1109/SC.Companion.2012.215},
file = {:home/chiroptera/Dropbox/mendeley/Demar et al. - 2012 - Big data networking at fermilab.pdf:pdf},
isbn = {9780769549569},
journal = {Proceedings - 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, SCC 2012},
keywords = {100GE network,GPU processing,Large Hadron Collider experiments,big data,big data processing,cluster,gpu,multi-core systems,scaling,wide-area distribution},
mendeley-tags = {big data,cluster,gpu},
number = {1007115},
pages = {1400},
title = {{Big data networking at fermilab}},
year = {2012}
}
@article{Dianxun2006,
abstract = {A novel generalized quantum particle model (GQPM) is presented for data self-organizing clustering. Using GQPM we transform the data clustering into a stochastic process of equivalence classes of particles under the quantum entanglement relation. The GQPM approach has much faster clustering speed and higher clustering quality than the nonquantum particle model GPM and GCA we proposed before. GQPM is also characterized by the self-organizing clustering and has advantages in terms of the insensitivity to noise, the quality robustness to clustered data, the learning ability, the suitability for high-dimensional multi-shape large-scale data sets. The simulations and comparisons have shown the effectiveness and good performance of the proposed GQPM approach to data clustering},
author = {Dianxun, Shuai and Zhang, Ping and Huang, Liangjun},
doi = {10.1109/ISIE.2006.296087},
file = {:home/chiroptera/Dropbox/mendeley/Dianxun, Zhang, Huang - 2006 - Self-organizing data clustering A novel quantum particle approach.pdf:pdf},
isbn = {1424404975},
journal = {IEEE International Symposium on Industrial Electronics},
number = {2},
pages = {2960--2965},
title = {{Self-organizing data clustering: A novel quantum particle approach}},
volume = {4},
year = {2006}
}
@article{Varshavsky2005,
author = {Varshavsky, Roy and Linial, Michal and Horn, David},
doi = {10.1007/11576259\_18},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky, Linial, Horn - 2005 - COMPACT A Comparative Package for Clustering Assessment.pdf:pdf},
isbn = {3540297707},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {159--167},
title = {{COMPACT: A Comparative Package for Clustering Assessment}},
volume = {3759 LNCS},
year = {2005}
}
@article{Fang2011,
abstract = {This paper presents a comprehensive performance comparison between CUDA and OpenCL. We have selected 16 benchmarks ranging from synthetic applications to real-world ones. We make an extensive analysis of the performance gaps taking into account programming models, ptimization strategies, architectural details, and underlying compilers. Our results show that, for most applications, CUDA performs at most 30$\backslash$\&\#x025; better than OpenCL. We also show that this difference is due to unfair comparisons: in fact, OpenCL can achieve similar performance to CUDA under a fair comparison. Therefore, we define a fair comparison of the two types of applications, providing guidelines for more potential analyses. We also investigate OpenCL's portability by running the benchmarks on other prevailing platforms with minor modifications. Overall, we conclude that OpenCL's portability does not fundamentally affect its performance, and OpenCL can be a good alternative to CUDA.},
annote = {Very good introduction.},
archivePrefix = {arXiv},
arxivId = {1005.2581},
author = {Fang, Jianbin and Varbanescu, Ana Lucia and Sips, Henk},
doi = {10.1109/ICPP.2011.45},
eprint = {1005.2581},
file = {:home/chiroptera/Dropbox/mendeley/Fang, Varbanescu, Sips - 2011 - A comprehensive performance comparison of CUDA and OpenCL.pdf:pdf},
isbn = {9780769545103},
issn = {01903918},
journal = {Proceedings of the International Conference on Parallel Processing},
keywords = {CUDA,OpenCL,Performance Comparison,comparison,cuda,opencl},
mendeley-tags = {comparison,cuda,opencl},
pages = {216--225},
title = {{A comprehensive performance comparison of CUDA and OpenCL}},
year = {2011}
}
@article{Wittek2013a,
abstract = {Clustering methods in machine learning may benefit from borrowing metaphors from physics. Dynamic quantum clustering associates a Gaussian wave packet with the multidimensional data points and regards them as eigenfunctions of the Schr??dinger equation. The clustering structure emerges by letting the system evolve and the visual nature of the algorithm has been shown to be useful in a range of applications. Furthermore, the method only uses matrix operations, which readily lend themselves to parallelization. In this paper, we develop an implementation on graphics hardware and investigate how this approach can accelerate the computations. We achieve a speedup of up to two magnitudes over a multicore CPU implementation, which proves that quantum-like methods and acceleration by graphics processing units have a great relevance to machine learning. ?? 2012 Elsevier Inc.},
author = {Wittek, Peter},
doi = {10.1016/j.jcp.2012.08.048},
file = {:home/chiroptera/Dropbox/mendeley/Wittek - 2013 - High-performance dynamic quantum clustering on graphics processors.pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
keywords = {Clustering,GPU computing,Quantum-like learning,Time-dependent schr??dinger equation},
month = jan,
number = {1},
pages = {262--271},
title = {{High-performance dynamic quantum clustering on graphics processors}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0021999112005165},
volume = {233},
year = {2013}
}
@article{Brassard,
author = {Brassard, Gilles and Centre-ville, Succursale},
file = {:home/chiroptera/Dropbox/mendeley/Brassard, Centre-ville - Unknown - Quantum Clustering Algorithms(2).pdf:pdf;:home/chiroptera/Dropbox/mendeley/Brassard, Centre-ville - Unknown - Quantum Clustering Algorithms.pdf:pdf},
title = {{Quantum Clustering Algorithms}}
}
@article{Shuai2006a,
abstract = {This paper presents a new generalized quantum particle model for data self-organizing clustering. The stochas- tic motion and collision of quantum particles give rise to a stochastic process of quantum entanglement of particles. The stationary probability distribution over the configuration space of entangled particles results in the optimally clustering solution of the given data set. The quantum particle model has advantages in terms of the insensitivity to noise, the quality robustness to clustered data, the learning ability, and the suitability for high-dimensional multi-shape large-scale data sets. In comparison with the classical version of particle model and the cellular automata, the quantum particle mode has much faster speed and higher quality for clustering. The simulation and comparison show the effectiveness and good performance of the proposed quantum particle approach to data clustering.},
author = {Shuai, Dianxun and Zhang, Bin and Dong, Yumin},
file = {:home/chiroptera/Dropbox/mendeley/Shuai, Zhang, Dong - 2006 - Quantum Particles Model for Data Clustering in Enterprise Computing.pdf:pdf},
isbn = {1424401003},
number = {2},
pages = {4602--4607},
title = {{Quantum Particles Model for Data Clustering in Enterprise Computing}},
year = {2006}
}
@article{gao20xx,
author = {Gao, Zhanchun and Li, Enxing and Jiang, Yanjun},
file = {:home/chiroptera/Dropbox/mendeley/Gao, Li, Jiang - Unknown - A gpu-based harmony k-means algorithm for document clustering.pdf:pdf},
keywords = {cluster number is large,document clustering,faster,gpu,harmony search,implementation is 20 times,k-means,parallel computing,results show that cuda,than cpu implement when},
pages = {2--5},
title = {{A gpu-based harmony k-means algorithm for document clustering}}
}
@article{Zhang2010,
author = {Zhang, Yongpeng and Mueller, Frank and Cui, Xiaohui and Potok, Thomas and Ridge, Oak and Sciences, Computational and Division, Engineering and Ridge, Oak},
file = {:home/chiroptera/Dropbox/mendeley/Zhang et al. - Unknown - Large-Scale Multi-Dimensional Document Clustering on GPU Clusters.pdf:pdf},
isbn = {9781424464432},
title = {{Large-Scale Multi-Dimensional Document Clustering on GPU Clusters}}
}
@article{Chen2014a,
author = {Chen, Shangyi and Li, Wei and Li, Min and Zhang, Xiaofei and Min, Yue},
doi = {10.1109/CCBD.2014.25},
file = {:home/chiroptera/Dropbox/mendeley/Chen et al. - 2014 - Latest Progress and Infrastructure Innovations of Big Data Technology.pdf:pdf},
isbn = {978-1-4799-6621-9},
journal = {2014 International Conference on Cloud Computing and Big Data},
keywords = {big data,incremental,mapreduce},
mendeley-tags = {big data},
pages = {8--15},
title = {{Latest Progress and Infrastructure Innovations of Big Data Technology}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7062865},
year = {2014}
}
@article{Zou2014,
abstract = {Bioinformatics is challenged by the fact that traditional analysis tools have difficulty in processing large-scale data from high-throughput sequencing. The open source Apache Hadoop project, which adopts the MapReduce framework and a distributed file system, has recently given bioinformatics researchers an opportunity to achieve scalable, efficient and reliable computing performance on Linux clusters and on cloud computing services. In this article, we present MapReduce frame-based applications that can be employed in the next-generation sequencing and other biological domains. In addition, we discuss the challenges faced by this field as well as the future works on parallel computing in bioinformatics.},
author = {Zou, Quan and Li, Xu Bin and Jiang, Wen Rui and Lin, Zi Yu and Li, Gui Lin and Chen, Ke},
doi = {10.1093/bib/bbs088},
file = {:home/chiroptera/Dropbox/mendeley//Zou et al. - 2014 - Survey of MapReduce frame operation in bioinformatics.pdf:pdf},
isbn = {1477-4054 (Electronic)$\backslash$n1467-5463 (Linking)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics,Hadoop,MapReduce,bioinformatics},
mendeley-tags = {MapReduce,bioinformatics},
number = {4},
pages = {637--647},
pmid = {23396756},
title = {{Survey of MapReduce frame operation in bioinformatics}},
volume = {15},
year = {2014}
}
@article{Woolley,
author = {Woolley, Cliff},
file = {:home/chiroptera/Dropbox/mendeley/Woolley - Unknown - CUDA Overview GPGPU Revolutionizes Computing.pdf:pdf},
title = {{CUDA Overview GPGPU Revolutionizes Computing}}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, CJC Christopher J C},
doi = {10.1023/A:1009715923555},
editor = {Fayyad, Usama},
eprint = {1111.6189v1},
institution = {Bell Laboratories, Lucent Technologies},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
publisher = {Springer},
series = {NetGames '06},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf$\backslash$nhttp://link.springer.com/article/10.1023/A:1009715923555},
volume = {2},
year = {1998}
}
@article{Varshavsky2007,
author = {Varshavsky, Roy and Horn, David and Linial, Michal},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky, Horn, Linial - 2007 - Clustering Algorithms Optimizer A Framework for Large Datasets.pdf:pdf},
isbn = {3540720308},
issn = {03029743},
pages = {85--96},
title = {{Clustering Algorithms Optimizer : A Framework for Large Datasets}},
year = {2007}
}
@article{Qian2010,
abstract = {We described a combined multiple clustering approach to automatically identify chronic lymphocytic leukemia neoplastic population by flow cytometry immunophenotyping. Flow cytometry data from various specimens were preprocessed by data cross-linking and subset selection before undergoing subspace and consensus clustering. This approach was implemented as a Server-side application, with results comparable to those performed by manual gating on commercial software.},
author = {Qian, You Wen and Cukierski, William and Osman, Mona and Goodell, Lauri},
doi = {10.1109/ICBBT.2010.5478955},
file = {:home/chiroptera/Dropbox/mendeley/Qian et al. - 2010 - Combined multiple clusterings on flow cytometry data to automatically identify chronic lymphocytic leukemia.pdf:pdf},
isbn = {9781424467761},
journal = {ICBBT 2010 - 2010 International Conference on Bioinformatics and Biomedical Technology},
keywords = {Chronic lymphocytic leukemia,Clustering,Flow cytometry},
pages = {305--309},
title = {{Combined multiple clusterings on flow cytometry data to automatically identify chronic lymphocytic leukemia}},
year = {2010}
}
@article{Bustamam2010,
abstract = {The massively parallel computing using graphical processing unit (GPU), which based on tens of thousands of parallel threats within hundreds of GPU's streaming processors, has gained broad popularity and attracted researchers in a wide range of application areas from finance, computer aided engineering, computational fluid dynamics, game physics, numerics, science, medical imaging, life science, and so on, including molecular biology and bioinformatics. Meanwhile, Markov clustering algorithm (MCL) has become one of the most effective and highly cited methods to detect and analyze the communities/clusters within an interaction network dataset on many real world problems such us social, technological, or biological networks including protein-protein interaction networks. However, as the dataset become bigger and bigger, the computation time of MCL algorithm become slower and slower. Hence, GPU computing is an interesting and challenging alternative to attempt to improve the MCL performance. In this poster paper we introduce our improvement of MCL performance based on ELLPACK-R sparse dataset format using GPU computing with the Compute Unified Device Architecture tool (CUDA) from NVIDIA (called CUDA-MCL). As the results show the significant improvement in CUDA-MCL performance and with the low-cost and widely available GPU devices in the market today, this CUDA-MCL implementation is allowing large-scale parallel computation on off-the-shelf desktop machines. Moreover the GPU computing approaches potentially may contribute to significantly change the way bioinformaticians and biologists compute and interact with their data.},
author = {Bustamam, Alhadi and Burrage, Kevin and Hamilton, Nicholas a.},
doi = {10.1109/ACT.2010.10},
file = {:home/chiroptera/Dropbox/mendeley/Bustamam, Burrage, Hamilton - 2010 - A GPU implementation of fast parallel Markov clustering in bioinformatics using ELLPACK-R sparse da.pdf:pdf},
isbn = {9780769542690},
journal = {Proceedings - 2010 2nd International Conference on Advances in Computing, Control and Telecommunication Technologies, ACT 2010},
pages = {173--175},
title = {{A GPU implementation of fast parallel Markov clustering in bioinformatics using ELLPACK-R sparse data format}},
year = {2010}
}
@article{Zechner2009b,
author = {Zechner, Mario and Granitzer, Michael},
doi = {issn: 1942-261x},
file = {:home/chiroptera/Dropbox/mendeley/Zechner, Granitzer - 2009 - K-Means on the Graphics Processor Design And Experimental Analysis.pdf:pdf},
journal = {International Journal on Advances in System and Measurements},
keywords = {-parallelization,gpgpu,k-means},
number = {2},
pages = {224--235},
title = {{K-Means on the Graphics Processor: Design And Experimental Analysis}},
url = {http://www.iariajournals.org/systems\_and\_measurements/sysmea\_v2\_n23\_2009\_paged.pdf},
volume = {2},
year = {2009}
}
@article{Shalom2009,
abstract = {We explore the use of todaypsilas high-end graphics processing units on desktops to perform hierarchical agglomerative clustering with the compute unified device architecture - CUDA of NVIDIA. Although the advancement in graphics cards has made the gaming industry to flourish,there is a lot more to be gained the field of scientific computing, high performance computing and their applications. Previous works have illustrated considerable speed gains on computing pair wise Euclidean distances between vectors, which is the fundamental operation in hierarchical clustering. We have used CUDA to implement the complete hierarchical agglomerative clustering algorithm and show almost double the speed gain using much cheaper desk top graphics card. In this paper we briefly explain the highly parallel and internally distributed programming structure of CUDA. We explore CUDA capabilities and propose methods to efficiently handle data within the graphics hardware for data intense, data independent, iterative or repetitive general purpose algorithms such as the hierarchical clustering. We achieved results with speed gains of about 30 to 65 times over the CPU implementation using micro array gene expressions.},
author = {Shalom, S. a Arul and Dash, Manoranjan and Tue, Minh and Wilson, Nithin},
doi = {10.1109/ICSPS.2009.167},
file = {:home/chiroptera/Dropbox/mendeley/Shalom et al. - 2009 - Hierarchical agglomerative clustering using graphics processor with compute unified device architecture.pdf:pdf},
isbn = {9780769536545},
journal = {2009 International Conference on Signal Processing Systems, ICSPS 2009},
keywords = {Acceleration of computations,CUDA hierarchical clustering,GPGPU,High performance computing,Parallel computing},
pages = {556--561},
title = {{Hierarchical agglomerative clustering using graphics processor with compute unified device architecture}},
year = {2009}
}
@article{Fred2006,
abstract = {Each clustering algorithm induces a similarity between given data points, according to the underlying clustering criteria. Given the large number of available clustering techniques, one is faced with the following questions: (a) Which measure of similarity should be used in a given clustering problem? (b) Should the same similarity measure be used throughout the d-dimensional feature space? In other words, are the underlying clusters in given data of similar shape? Our goal is to learn the pairwise similarity between points in order to facilitate a proper partitioning of the data without the a priori knowledge of k, the number of clusters, and of the shape of these clusters. We explore a clustering ensemble approach combined with cluster stability criteria to selectively learn the similarity from a collection of different clustering algorithms with various parameter configurations},
author = {Fred, Ana L N and Jain, Anil K.},
doi = {10.1109/ICPR.2006.754},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2006 - Learning pairwise similarity for data clustering.pdf:pdf},
isbn = {0769525210},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {925--928},
title = {{Learning pairwise similarity for data clustering}},
volume = {1},
year = {2006}
}
@article{Bader2006,
abstract = {Minimum spanning tree (MST) is one of the most studied combinatorial problems with practical applications in VLSI layout, wireless communication, and distributed networks, recent problems in biology and medicine such as cancer detection, medical imaging, and proteomics, and national security and bioterrorism such as detecting the spread of toxins through populations in the case of biological/chemical warfare. Most of the previous attempts for improving the speed of MST using parallel computing are too complicated to implement or perform well only on special graphs with regular structure. In this paper we design and implement four parallel MST algorithms (three variations of Bor??vka plus our new approach) for arbitrary sparse graphs that for the first time give speedup when compared with the best sequential algorithm. In fact, our algorithms also solve the minimum spanning forest problem. We provide an experimental study of our algorithms on symmetric multiprocessors such as IBMs pSeries and Sun's Enterprise servers. Our new implementation achieves good speedups over a wide range of input graphs with regular and irregular structures, including the graphs used by previous parallel MST studies. For example, on an arbitrary random graph with 1 M vertices and 20 M edges, our new approach achieves a speedup of 5 using 8 processors. The source code for these algorithms is freely available from our web site. ?? 2006 Elsevier Inc. All rights reserved.},
author = {Bader, David a. and Cong, Guojing},
doi = {10.1016/j.jpdc.2006.06.001},
file = {:home/chiroptera/Dropbox/mendeley/Bader, Cong - 2006 - Fast shared-memory algorithms for computing the minimum spanning forest of sparse graphs.pdf:pdf},
isbn = {0-7695-2132-0},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Connectivity,High-performance algorithm engineering,Parallel graph algorithms},
number = {11},
pages = {1366--1378},
title = {{Fast shared-memory algorithms for computing the minimum spanning forest of sparse graphs}},
volume = {66},
year = {2006}
}
@article{Fred2009a,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt.2 Clustering Algorithms.pdf:pdf},
number = {April},
pages = {1--42},
title = {{Tutorial Pt.2: Clustering Algorithms}},
year = {2009}
}
@article{Wang2014,
author = {Wang, Wei},
doi = {10.1109/ICSC.2014.65},
file = {:home/chiroptera/Dropbox/mendeley/Wang - 2014 - Big Data, Big Challenges.pdf:pdf},
isbn = {978-1-4799-4003-5},
journal = {2014 IEEE International Conference on Semantic Computing},
pages = {6--6},
title = {{Big Data, Big Challenges}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6881994},
year = {2014}
}
@article{Jamsek2009a,
abstract = {The complexity of modern microprocessor design involving billions of transistors at increasingly denser scales creates many challenges particularly in the area of design reliability and predictable yields. Researchers at IBM's Austin Research Lab have increasingly depended on software based simulation of various aspects of the design and manufacturing process to help address these challenges. The computational complexity and sheer scale of these simulations have lead to the exploration of the application of high-performance hybrid computing clusters to accelerate the design process. Currently, the hybrid clusters in use are composed primarily of commodity workstations and servers incorporating commodity NVIDIA-based GPU graphics cards and TESLA GPU computational accelerators. We have also been experimenting with blade clusters composed of both general purpose servers and PowerXcell accelerators leveraging the computational throughput of the Cell processor. In this paper we will detail our experiences with accelerating our workloads on these hybrid cluster platforms. We will discuss our initial approach of combining hybrid runtimes such as CUDA with MPI to address cluster computation. We will also describe a custom cluster hybrid infrastructure we are developing to deal with some of the perceived shortcomings of MPI and other traditional cluster tools when dealing with hybrid computing environments.},
author = {Jamsek, Damir and {Van Hensbergen}, Eric},
doi = {10.1109/CLUSTR.2009.5289126},
file = {:home/chiroptera/Dropbox/mendeley/Jamsek, Van Hensbergen - 2009 - Experiences with hybrid clusters.pdf:pdf},
isbn = {9781424450121},
issn = {15525244},
journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
keywords = {cluster,gpu,mpi},
mendeley-tags = {cluster,gpu,mpi},
pages = {1--4},
title = {{Experiences with hybrid clusters}},
year = {2009}
}
@article{Wiebe2014,
abstract = {We present several quantum algorithms for performing nearest-neighbor learning. At the core of our algorithms are fast and coherent quantum methods for computing distance metrics such as the inner product and Euclidean distance. We prove upper bounds on the number of queries to the input data required to compute these metrics. In the worst case, our quantum algorithms lead to polynomial reductions in query complexity relative to the corresponding classical algorithm. In certain cases, we show exponential or even super-exponential reductions over the classical analog. We study the performance of our quantum nearest-neighbor algorithms on several real-world binary classification tasks and find that the classification accuracy is competitive with classical methods.},
archivePrefix = {arXiv},
arxivId = {1401.2142},
author = {Wiebe, Nathan and Kapoor, Ashish and Svore, Krysta},
eprint = {1401.2142},
file = {:home/chiroptera/Dropbox/mendeley/Wiebe, Kapoor, Svore - 2014 - Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning.pdf:pdf},
pages = {31},
title = {{Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning}},
url = {http://arxiv.org/abs/1401.2142},
year = {2014}
}
@article{Weinstein2009,
abstract = {A given set of data points in some feature space may be associated with a Schr\"{o}dinger equation whose potential is determined by the data. This is known to lead to good clustering solutions. Here we extend this approach into a full-fledged dynamical scheme using a time-dependent Schr\"{o}dinger equation. Moreover, we approximate this Hamiltonian formalism by a truncated calculation within a set of Gaussian wave functions (coherent states) centered around the original points. This allows for analytic evaluation of the time evolution of all such states opening up the possibility of exploration of relationships among data points through observation of varying dynamical distances among points and convergence of points into clusters. This formalism may be further supplemented by preprocessing such as dimensional reduction through singular-value decomposition or feature filtering.},
annote = {From Duplicate 2 (Dynamic quantum clustering: a method for visual exploration of structures in data - Weinstein, Marvin; Horn, David)},
archivePrefix = {arXiv},
arxivId = {arXiv:0908.2644v1},
author = {Weinstein, Marvin and Horn, David},
doi = {10.1103/PhysRevE.80.066117},
eprint = {arXiv:0908.2644v1},
file = {:home/chiroptera/Dropbox/mendeley//Weinstein, Horn - 2009 - Dynamic quantum clustering A method for visual exploration of structures in data.pdf:pdf},
issn = {1539-3755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
keywords = {clustering,quantum},
mendeley-tags = {clustering,quantum},
month = dec,
number = {6},
pages = {1--15},
title = {{Dynamic quantum clustering: a method for visual exploration of structures in data}},
url = {http://link.aps.org/doi/10.1103/PhysRevE.80.066117},
volume = {80},
year = {2009}
}
@article{Asanovic2006,
abstract = {The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation. A multidisciplinary group of Berkeley researchers met nearly two years to discuss this change. Our view is that this evolutionary approach to parallel hardware and software may work from 2 or 8 processor systems, but is likely to face diminishing returns as 16 and 32 processor systems are realized, just as returns fell with greater instruction-level parallelism. We believe that much can be learned by examining the success of parallelism at the extremes of the computing spectrum, namely embedded computing and high performance computing. This led us to frame the parallel landscape with seven questions, and to recommend the following: The overarching goal should be to make it easy to write programs that execute efficiently on highly parallel computing systems The target should be 1000s of cores per chip, as these chips are built from processing elements that are the most efficient in MIPS (Million Instructions per Second) per watt, MIPS per area of silicon, and MIPS per development dollar. Instead of traditional benchmarks, use 13 "Dwarfs" to design and evaluate parallel programming models and architectures. (A dwarf is an algorithmic method that captures a pattern of computation and communication.) "Autotuners" should play a larger role than conventional compilers in translating parallel programs. To maximize programmer productivity, future programming models must be more human-centric than the conventional focus on hardware or applications. To be successful, programming models should be independent of the number of processors. To maximize application efficiency, programming models should support a wide range of data types and successful models of parallelism: task-level parallelism, word-level parallelism, and bit-level parallelism. Architects should not include features that significantly affect performance or energy if programmers cannot accurately measure their impact via performance counters and energy counters. Traditional operating systems will be deconstructed and operating system functionality will be orchestrated using libraries and virtual machines. To explore the design space rapidly, use system emulators based on Field Programmable Gate Arrays (FPGAs) that are highly scalable and low cost. Since real world applications are naturally parallel and hardware is naturally parallel, what we need is a programming model, system software, and a supporting architecture that are naturally parallel. Researchers have the rare opportunity to re-invent these cornerstones of computing, provided they simplify the efficient programming of highly parallel systems.},
author = {Asanovic, Krste and Catanzaro, Bryan Christopher and Patterson, David a and Yelick, Katherine a},
doi = {10.1145/1562764.1562783},
file = {:home/chiroptera/Dropbox/mendeley/Asanovic et al. - 2006 - The Landscape of Parallel Computing Research A View from Berkeley.pdf:pdf},
isbn = {UCB/EECS-2006-183},
issn = {00010782},
journal = {EECS Department University of California Berkeley Tech Rep UCBEECS2006183},
pages = {19},
pmid = {8429457},
title = {{The Landscape of Parallel Computing Research : A View from Berkeley}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.8705\&amp;rep=rep1\&amp;type=pdf},
volume = {18},
year = {2006}
}
@article{Chang2009,
abstract = {Graphics processing units (GPUs) are powerful computational devices tailored towards the needs of the 3-D gaming industry for high-performance, real-time graphics engines. Nvidia Corporation released a new generation of GPUs designed for general-purpose computing in 2006, and it released a GPU programming language called CUDA in 2007. The DNA microarray technology is a high throughput tool for assaying mRNA abundance in cell samples. In data analysis, scientists often apply hierarchical clustering of the genes, where a fundamental operation is to calculate all pairwise distances. If there are n genes, it takes O(n\^{}2) time. In this work, GPUs and the CUDA language are used to calculate pairwise distances. For Manhattan distance, GPU/CUDA achieves a 40 to 90 times speed-up compared to the central processing unit implementation; for Pearson correlation coefficient, the speed-up is 28 to 38 times.},
author = {Chang, Dar J. and Desoky, Ahmed H. and Ouyang, Ming and Rouchka, Eric C.},
doi = {10.1109/SNPD.2009.34},
file = {:home/chiroptera/Dropbox/mendeley/Chang et al. - 2009 - Compute pairwise Manhattan distance and Pearson correlation coefficient of data points with GPU.pdf:pdf},
isbn = {9780769536422},
issn = {0769536425},
journal = {10th ACIS Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2009, In conjunction with IWEA 2009 and WEACR 2009},
keywords = {Hierarchical clustering,Parallel and distributed computation,Similarity and dissimilarity metrics},
pages = {501--506},
title = {{Compute pairwise Manhattan distance and Pearson correlation coefficient of data points with GPU}},
year = {2009}
}
@article{Walter2008,
abstract = {Hierarchical representations of large data sets, such as binary cluster trees, are a crucial component in many scalable algorithms used in various fields. Two major approaches for building these trees are agglomerative, or bottom-up, clustering and divisive, or top-down, clustering. The agglomerative approach offers some real advantages such as more flexible clustering and often produces higher quality trees, but has been little used in graphics because it is frequently assumed to be prohibitively expensive (O(N<sup>2</sup>) or worse). In this paper we show that agglomerative clustering can be done efficiently even for very large data sets. We introduce a novel locally-ordered algorithm that is faster than traditional heap-based agglomerative clustering and show that the complexity of the tree build time is much closer to linear than quadratic. We also evaluate the quality of the agglomerative clustering trees compared to the best known divisive clustering strategies in two sample applications: bounding volume hierarchies for ray tracing and light trees in the Lightcuts rendering algorithm. Tree quality is highly application, data set, and dissimilarity function specific. In our experiments the agglomerative-built tree quality is consistently higher by margins ranging from slight to significant, with up to 35\% reduction in tree query times.},
author = {Walter, Bruce and Bala, Kavita and Kulkarni, Milind and Pingali, Keshav},
doi = {10.1109/RT.2008.4634626},
file = {:home/chiroptera/Dropbox/mendeley/Walter et al. - 2008 - Fast agglomerative clustering for rendering.pdf:pdf},
isbn = {9781424427413},
journal = {RT'08 - IEEE/EG Symposium on Interactive Ray Tracing 2008, Proceedings},
keywords = {Agglomerative clustering,Bottom-up tree construction,Bounding volume hierarchy,Dendogram,Lightcuts rendering},
pages = {81--86},
title = {{Fast agglomerative clustering for rendering}},
year = {2008}
}
@article{Karger1995,
author = {Karger, David R. and Klein, Philip N. and Tarjan, Robert E.},
doi = {10.1145/201019.201022},
file = {:home/chiroptera/Dropbox/mendeley/Karger, Klein, Tarjan - 1995 - A randomized linear-time algorithm to find minimum spanning trees.pdf:pdf},
issn = {00045411},
journal = {Journal of the ACM},
number = {2},
pages = {321--328},
title = {{A randomized linear-time algorithm to find minimum spanning trees}},
volume = {42},
year = {1995}
}
@book{Pedersen2008,
abstract = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
author = {Pedersen, Michael Syskind and Baxter, Bill and Templeton, Brian and Rish\o j, Christian and Theobald, Douglas L and Hoegh-rasmussen, Esben and Casteel, Glynne and Gao, Jun Bin and Dedecius, Kamil and Strim, Korbinian and Christiansen, Lars and Hansen, Lars Kai and Wilkinson, Leland and He, Liguo and Bar, Miguel and Winther, Ole and Sakov, Pavel and Hattinger, Stephan and Petersen, Kaare Breandt and Rish\o j, Christian},
booktitle = {Matrix},
doi = {10.1111/j.1365-294X.2006.03161.x},
editor = {Bloom, B R},
institution = {Technical University of Denmark},
issn = {09621083},
keywords = {acknowledgements,bill baxter,brian templeton,christian,christian rish\o j,contributions,derivative,derivative inverse matrix,determinant,differentiate a matrix,matrix algebra,matrix identities,matrix relations,suggestions,thank following,we would like},
number = {1},
pages = {1--71},
pmid = {17284204},
publisher = {Citeseer},
title = {{The Matrix Cookbook}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.3165\&amp;rep=rep1\&amp;type=pdf},
volume = {M},
year = {2008}
}
@article{Faro2012,
author = {Faro, Alberto and Giordano, Daniela and Palazzo, Simone},
file = {:home/chiroptera/Dropbox/mendeley/Faro, Giordano, Palazzo - 2012 - Integrating Unsupervised and Supervised Clustering Methods on a GPU platform for Fast Image Segmentatio.pdf:pdf},
isbn = {9781467325844},
keywords = {gpu,image segmentation,parallel clustering},
title = {{Integrating Unsupervised and Supervised Clustering Methods on a GPU platform for Fast Image Segmentation}},
year = {2012}
}
@book{Moler2008,
abstract = {This chapter is about eigenvalues and singular values of matrices. Computational algorithms and sensitivity to perturbations are both discussed.},
author = {Moler, Cleve},
booktitle = {Numerical Computing with MATLAB, Revised Reprint},
doi = {10.1016/0377-0427(90)90025-U},
file = {:home/chiroptera/Dropbox/mendeley/Moler - 2008 - Eigenvalues and Singular Values.pdf:pdf},
isbn = {978-0-898716-60-3},
issn = {1550-2376},
pages = {39},
pmid = {21230651},
title = {{Eigenvalues and Singular Values}},
url = {http://www.mathworks.nl/moler/chapters.html},
year = {2008}
}
@article{Hou2013,
author = {Hou, Rui and Jiang, Tao and Zhang, Liuhang and Qi, Pengfei and Dong, Jianbo and Wang, Haibin and Gu, Xiongli and Zhang, Shujie},
doi = {10.1109/HPCA.2013.6522317},
file = {:home/chiroptera/Dropbox/mendeley/Hou et al. - 2013 - Cost effective data center servers.pdf:pdf},
isbn = {978-1-4673-5587-2},
journal = {2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)},
pages = {179--187},
title = {{Cost effective data center servers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6522317},
year = {2013}
}
@article{He2012,
author = {He, Lifeng},
file = {:home/chiroptera/Dropbox/mendeley/He - 2012 - A new Algorithm for Labeling Connected-Components and Calculating the Euler Number , Connected-Component Number , and Hole.pdf:pdf},
isbn = {9784990644116},
issn = {10514651},
journal = {International Conference on Pattern Recognition},
keywords = {Features and Image Descriptors,Image and Video Processing,Image and Video Understanding},
number = {Icpr},
pages = {3099--3102},
title = {{A new Algorithm for Labeling Connected-Components and Calculating the Euler Number , Connected-Component Number , and Hole Number  The University of Chicago}},
year = {2012}
}
@article{Karimi2010,
abstract = {CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is an open standard that can be used to program CPUs, GPUs, and other devices from different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL promises a portable language for GPU programming, its generality may entail a performance penalty. In this paper, we use complex, near-identical kernels from a Quantum Monte Carlo application to compare the performance of CUDA and OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel to an OpenCL kernel involves minimal modifications. Making such a kernel compile with ATI's build tools involves more modifications. Our performance tests measure and compare data transfer times to and from the GPU, kernel execution times, and end-to-end application execution times for both CUDA and OpenCL.},
archivePrefix = {arXiv},
arxivId = {1005.2581},
author = {Karimi, Kamran and Dickson, Neil G. and Hamze, Firas},
doi = {10.1109/ICPP.2011.45},
eprint = {1005.2581},
file = {:home/chiroptera/Dropbox/mendeley/Karimi, Dickson, Hamze - 2010 - A Performance Comparison of CUDA and OpenCL.pdf:pdf},
isbn = {978-1-4577-1336-1},
keywords = {comparison,cuda,opencl},
mendeley-tags = {comparison,cuda,opencl},
number = {1},
pages = {12},
title = {{A Performance Comparison of CUDA and OpenCL}},
url = {http://arxiv.org/abs/1005.2581},
year = {2010}
}
@article{Liu2010,
abstract = {By reviewing the original INIQGA algorithm, an improved algorithm (IINIQGA) is put forward by revising the lookup table. In addition, By introducing the variable angle-distance rotation method into the update Q(t) procedure, a novel quantum-inspired evolutionary algorithm, QEA-VAR, was proposed. Compared with previous algorithms, our update Q(t) procedure is more simple and feasible. Finally, the corresponding experiments on the 0-1 knapsack problem were carried out, and the results show that our improvement is efficient, and comparing with IINIQGA, QEA, and CGA, QEA-VAR has a faster convergence and better profits than other algorithms.},
author = {Liu, Wenjie and Chen, Hanwu and Yan, Qiaoqiao and Liu, Zhihao and Xu, Juan and Zheng, Yu},
doi = {10.1109/CEC.2010.5586281},
file = {:home/chiroptera/Dropbox/mendeley/Liu et al. - 2010 - A novel quantum-inspired evolutionary algorithm based on variable angle-distance rotation.pdf:pdf},
isbn = {9781424469109},
journal = {2010 IEEE World Congress on Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation, CEC 2010},
keywords = {0/1 knapsack problem,quantum-inspired evolutionary algorithm,qubit,variable angle-distance,variable angle-distance rotation},
mendeley-tags = {qubit,variable angle-distance},
title = {{A novel quantum-inspired evolutionary algorithm based on variable angle-distance rotation}},
year = {2010}
}
@article{Xiao2008,
abstract = {The conventional k-means clustering algorithm must know the number of clusters in advance and the clustering result is sensitive to the selection of the initial cluster centroids. The sensitivity may make the algorithm converge to the local optima. This paper proposes an improved k-means clustering algorithm based on quantum-inspired genetic algorithm (KMQGA). In KMQGA, Q-bit based representation is employed for exploration and exploitation in discrete 0-1 hyperspace by using rotation operation of quantum gate as well as three genetic algorithm operations (selection, crossover and mutation) of Q-bit. Without knowing the exact number of clusters beforehand, the KMQGA can get the optimal number of clusters as well as providing the optimal cluster centroids after several iterations of the four operations (selection, crossover, mutation, and rotation). The simulated datasets and the real datasets are used to validate KMQGA and to compare KMQGA with an improved k-means clustering algorithm based on the famous variable string length genetic algorithm (KMVGA) respectively. The experimental results show that KMQGA is promising and the effectiveness and the search quality of KMQGA is better than those of KMVGA.},
author = {Xiao, Jing Xiao Jing and Yan, YuPing Yan YuPing and Lin, Ying Lin Ying and Yuan, Ling Yuan Ling and Zhang, Jun Zhang Jun},
journal = {2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)},
title = {{A Quantum-inspired Genetic Algorithm for data clustering}},
year = {2008}
}
@article{Arefin2012a,
abstract = {BACKGROUND: The analysis of biological networks has become a major challenge due to the recent development of high-throughput techniques that are rapidly producing very large data sets. The exploding volumes of biological data are craving for extreme computational power and special computing facilities (i.e. super-computers). An inexpensive solution, such as General Purpose computation based on Graphics Processing Units (GPGPU), can be adapted to tackle this challenge, but the limitation of the device internal memory can pose a new problem of scalability. An efficient data and computational parallelism with partitioning is required to provide a fast and scalable solution to this problem.$\backslash$n$\backslash$nRESULTS: We propose an efficient parallel formulation of the k-Nearest Neighbour (kNN) search problem, which is a popular method for classifying objects in several fields of research, such as pattern recognition, machine learning and bioinformatics. Being very simple and straightforward, the performance of the kNN search degrades dramatically for large data sets, since the task is computationally intensive. The proposed approach is not only fast but also scalable to large-scale instances. Based on our approach, we implemented a software tool GPU-FS-kNN (GPU-based Fast and Scalable k-Nearest Neighbour) for CUDA enabled GPUs. The basic approach is simple and adaptable to other available GPU architectures. We observed speed-ups of 50-60 times compared with CPU implementation on a well-known breast microarray study and its associated data sets.$\backslash$n$\backslash$nCONCLUSION: Our GPU-based Fast and Scalable k-Nearest Neighbour search technique (GPU-FS-kNN) provides a significant performance improvement for nearest neighbour computation in large-scale networks. Source code and the software tool is available under GNU Public License (GPL) at https://sourceforge.net/p/gpufsknn/.},
author = {Arefin, Ahmed Shamsul and Riveros, Carlos and Berretta, Regina and Moscato, Pablo},
doi = {10.1371/journal.pone.0044000},
file = {:home/chiroptera/Dropbox/mendeley/Arefin et al. - 2012 - GPU-FS-kNN A Software Tool for Fast and Scalable kNN Computation Using GPUs.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
pmid = {22937144},
title = {{GPU-FS-kNN: A Software Tool for Fast and Scalable kNN Computation Using GPUs}},
volume = {7},
year = {2012}
}
@article{Jung2010,
abstract = {This paper proposes a new connected component labeling algorithm for GPGPU applications based on NVIDIA's CUDA. Various approaches and algorithms for connected component labeling with minimal execution time were designed, but the most of them have been focused on optimizing CPU algorithm. Therefore it is hard to apply these approaches to GPGPU programming models such as NVIDIA's CUDA. Today, GPGPU (General Purpose Graphic Processing Unit) technologies offer dedicated parallel hardware and programming model, and many applications are being moved onto the GPGPU. This algorithm is a multi-pass algorithm to utilize for GPGPU applications, and evaluation results show that maximum speedup is more than double compared with conventional CPU algorithms.},
author = {Jung, In Yong and Jeong, Chang Sung},
doi = {10.1109/ISCIT.2010.5665161},
file = {:home/chiroptera/Dropbox/mendeley/Jung, Jeong - 2010 - Parallel connected-component labeling algorithm for GPGPU applications.pdf:pdf},
isbn = {9781424470105},
journal = {ISCIT 2010 - 2010 10th International Symposium on Communications and Information Technologies},
pages = {1149--1153},
title = {{Parallel connected-component labeling algorithm for GPGPU applications}},
year = {2010}
}
@article{Merrill2012,
abstract = {Breadth-first search (BFS) is a core primitive for graph traversal and a basis for many higher-level graph analysis algorithms. It is also representative of a class of parallel computations whose memory accesses and work distribution are both irregular and data-dependent. Recent work has demonstrated the plausibility of GPU sparse graph traversal, but has tended to focus on asymptotically inefficient algorithms that perform poorly on graphs with non-trivial diameter. We present a BFS parallelization focused on fine-grained task management constructed from efficient prefix sum that achieves an asymptotically optimal O(|V|+|E|) work complexity. Our implementation delivers excellent performance on diverse graphs, achieving traversal rates in excess of 3.3 billion and 8.3 billion traversed edges per second using single and quad-GPU configurations, respectively. This level of performance is several times faster than state-of-the-art implementations both CPU and GPU platforms.},
author = {Merrill, Duane and Garland, Michael and Grimshaw, Andrew},
doi = {10.1145/2370036.2145832},
file = {:home/chiroptera/Dropbox/mendeley/Merrill, Garland, Grimshaw - 2012 - Scalable GPU graph traversal.pdf:pdf},
isbn = {9781450311601},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {breadth-first search,gpu,graph algorithms,graph traversal,parallel algorithms,prefix sum,sparse graph},
number = {8},
pages = {117},
title = {{Scalable GPU graph traversal}},
volume = {47},
year = {2012}
}
@article{Rosenbaum2011,
abstract = {We consider a generalization of the standard oracle model in which the oracle acts on the target with a permutation which is selected according to internal random coins. We show new exponential quantum speedups which may be obtained over classical algorithms in this oracle model. Even stronger, we describe several problems which are impossible to solve classically but can be solved by a quantum algorithm using a single query; we show that such infinity-vs-one separations between classical and quantum query complexities can be constructed from any separation between classical and quantum query complexities (in the unbounded-error regime).   We also give conditions to determine when oracle problems---either in the standard model, or in any of the generalizations we consider---cannot be solved with success probability better than random guessing would achieve. In the oracle model with internal randomness where the goal is to gain any nonzero advantage over guessing, we prove (roughly speaking) that (k) quantum queries are equivalent in power to (2k) classical queries, thus extending results of Meyer and Pommersheim, and Montanaro, Nishimura and Raymond.},
archivePrefix = {arXiv},
arxivId = {1111.1462v1},
author = {Rosenbaum, David and Harrow, Aram W.},
eprint = {1111.1462v1},
file = {:home/chiroptera/Dropbox/mendeley/Rosenbaum, Harrow - 2011 - Uselessness for an Oracle Model with Internal Randomness.pdf:pdf},
issn = {15337146},
journal = {arXiv:1111.1462},
pages = {1--23},
title = {{Uselessness for an Oracle Model with Internal Randomness}},
year = {2011}
}
@article{Owens2008,
abstract = {The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in general-purpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.},
annote = {Excellent survey (albeit old) of GPU, GPU architecture, GPU programming flow, GPGPU, examples and trends.},
author = {Owens, Jd and Houston, M},
doi = {10.1109/JPROC.2008.917757},
file = {:home/chiroptera/Dropbox/mendeley/Owens, Houston - 2008 - GPU computing.pdf:pdf},
isbn = {0769527000},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {gpgpu,gpu,survey,topnotch},
mendeley-tags = {gpgpu,gpu,survey,topnotch},
number = {5},
pages = {879 -- 899},
pmid = {21776805},
title = {{GPU computing}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4490127},
volume = {96},
year = {2008}
}
@article{Lourenco2010,
abstract = {This work focuses on the scalability of the Evidence Accumulation Clustering (EAC) method. We first address the space complexity of the co-association matrix. The sparseness of the matrix is related to the construction of the clustering ensemble. Using a split and merge strategy combined with a sparse matrix representation, we empirically show that a linear space complexity is achievable in this framework, leading to the scalability of EAC method to clustering large data-sets.},
author = {Louren\c{c}o, Andr\'{e} and Fred, Ana L N and Jain, Anil K.},
doi = {10.1109/ICPR.2010.197},
file = {:home/chiroptera/Dropbox/mendeley/Louren\c{c}o, Fred, Jain - 2010 - On the scalability of evidence accumulation clustering.pdf:pdf},
isbn = {9780769541099},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
keywords = {Cluster analysis,Cluster fusion,Combining clustering partitions,Evidence accumulation,Large data-sets},
pages = {782--785},
pmid = {5596045},
title = {{On the scalability of evidence accumulation clustering}},
volume = {0},
year = {2010}
}
@article{Casper2013,
abstract = {The ability to cluster data accurately is essential to applications such as image segmentation. Therefore, techniques that enhance accuracy are of keen interest. One such technique involves applying a quantum mechanical system model, such as that of the quantum bit, to generate probabilistic numerical output to be used as variable input for clustering algorithms. This work demonstrates that applying a quantum bit model to data clustering algorithms can increase clustering accuracy, as a result of simulating superposition as well as possessing both guaranteed and controllable convergence properties. For accuracy assessment purposes, four quantum-modeled clustering algorithms for multi-band image segmentation are explored and evaluated. The clustering algorithms of choice consist of quantum variants of K-Means, Fuzzy C-Means, New Weighted Fuzzy C- Means, and the Artificial Bee Colony. Data sets of interest include multi-band imagery, which subsequent to classification are analyzed and assessed for accuracy. Results demonstrate that these algorithms exhibit improved accuracy, when compared to classical counterparts. Moreover, solutions are enhanced via introduction of the quantum state machine, which provides random initial centroid and variable input values to the various clustering algorithms, and quantum operators, which bring about convergence and maximize local search space exploration. Typically, the algorithms have shown to produce better solutions. Keywords:},
author = {Casper, Ellis and Hung, Chih-cheng},
doi = {10.4156/pica.vol2.issue1.1},
file = {:home/chiroptera/Dropbox/mendeley/Casper, Hung - 2013 - Quantum Modeled Clustering Algorithms for Image Segmentation 1.pdf:pdf},
keywords = {artificial bee colony,clustering,clustering algorithms,fuzzy c-means,image segmentation,k-means,quantum computing,quantum mechanics,qubit,weighted fuzzy c-means},
mendeley-tags = {artificial bee colony,clustering,fuzzy c-means,image segmentation,k-means,quantum computing,quantum mechanics,qubit,weighted fuzzy c-means},
number = {March},
pages = {1--21},
title = {{Quantum Modeled Clustering Algorithms for Image Segmentation 1}},
volume = {2},
year = {2013}
}
@article{Zechner2009a,
abstract = {In this paper an optimized k-means implementation on the graphics processing unit (GPU) is presented. NVIDIApsilas compute unified device architecture (CUDA), available from the G80 GPU family onwards, is used as the programming environment. Emphasis is placed on optimizations directly targeted at this architecture to best exploit the computational capabilities available. Additionally drawbacks and limitations of previous related work, e.g. maximum instance, dimension and centroid count are addressed. The algorithm is realized in a hybrid manner, parallelizing distance calculations on the GPU while sequentially updating cluster centroids on the CPU based on the results from the GPU calculations. An empirical performance study on synthetic data is given, demonstrating a maximum 14times speed increase to a fully SIMD optimized CPU implementation.},
author = {Zechner, Mario and Granitzer, Michael},
doi = {10.1109/INTENSIVE.2009.19},
file = {:home/chiroptera/Dropbox/mendeley/Zechner, Granitzer - 2009 - Accelerating k-means on the graphics processor via CUDA(2).pdf:pdf},
isbn = {9780769535852},
journal = {Proceedings of the 1st International Conference on Intensive Applications and Services, INTENSIVE 2009},
pages = {7--15},
title = {{Accelerating k-means on the graphics processor via CUDA}},
year = {2009}
}
@article{Raymond2013,
author = {Raymond, The and Sackler, Beverly},
file = {:home/chiroptera/Dropbox/mendeley/Raymond, Sackler - 2013 - Quantum Clustering of Large Data Sets.pdf:pdf},
title = {{Quantum Clustering of Large Data Sets}},
year = {2013}
}
@article{Cui2011,
abstract = {Analyzing and clustering large scale data set is a complex problem. One explored method of solving this problem borrows from nature, imitating the flocking behavior of birds. One limitation of this method of data clustering is its complexity \$O(n\^{}2)\$. As the number of data and feature dimensions grows, it becomes increasingly difficult to generate results in a reasonable amount of time. In the last few years, the graphics processing unit (GPU) has received attention for its ability to solve highly-parallel and semi-parallel problems much faster than the traditional sequential processor. In this chapter, we have conducted research to exploit this architecture and apply its strengths to the flocking based data clustering problem. Using the CUDA platform from NVIDIA, we developed a Multiple Species Data Flocking implementation to be run on the NVIDIA GPU. Performance gains ranged from \$30\$ to \$60\$ times improvement of the GPU over the CPU implementation.},
author = {Cui, Xiaohui and Charles, Jesse St. and Potok, Thomas E.},
doi = {10.1109/CyberC.2011.44},
file = {:home/chiroptera/Dropbox/mendeley/Cui, Charles, Potok - 2011 - The GPU Enhanced Parallel Computing for Large Scale Data Clustering.pdf:pdf},
isbn = {978-0-7695-4557-8},
journal = {2011 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery},
keywords = {GPU,clustering,flocking,large scale},
pages = {220--225},
title = {{The GPU Enhanced Parallel Computing for Large Scale Data Clustering}},
year = {2011}
}
@article{Manju2014,
abstract = {This paper makes an exhaustive survey of various applications of Quantum inspired computational intelligence (QCI) techniques proposed till date. Definition, categorization and motivation for QCI techniques are stated clearly. Major Drawbacks and challenges are discussed. The significance of this work is that it presents an overview on applications of QCI in solving various problems in engineering, which will be very much useful for researchers on Quantum computing in exploring this upcoming and young discipline.[PUBLICATION ABSTRACT]},
author = {Manju, a. and Nigam, M. J.},
doi = {10.1007/s10462-012-9330-6},
file = {:home/chiroptera/Dropbox/mendeley/Manju, Nigam - 2014 - Applications of quantum inspired computational intelligence A survey.pdf:pdf},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Computational intelligence,Quantum computing,Quantum mechanics},
pages = {79--156},
title = {{Applications of quantum inspired computational intelligence: A survey}},
volume = {42},
year = {2014}
}
@article{ArulShalom2011,
abstract = {We explore the capabilities of today's high-end Graphics processing units (GPU) on desktops to efficiently perform hierarchical agglomerative clustering (HAC) through partitioning of data. Traditional HAC has high time and memory complexities leading to low clustering efficiencies. We reduce time and memory bottlenecks of the traditional HAC algorithm by exploring the performance capabilities of the GPU, significantly accelerating the computations without compromising the accuracy of clusters. We implement the traditional HAC and the Partially Overlapping Partitioning (PoP) on GPU using Compute Unified Device Architecture (CUDA) and compare the computational performance with CPU using micro array data. The result shows that the PoP HAC and traditional HAC are up to 442 times and 66 times faster on the GPU respectively than the time taken by CPU. The PoP-enabled HAC on GPU requires only a fraction of the memory required by traditional HAC both on the CPU and GPU.},
author = {{Arul Shalom}, S. a. and Dash, Manoranjan},
doi = {10.1109/PDCAT.2011.38},
file = {:home/chiroptera/Dropbox/mendeley/Arul Shalom, Dash - 2011 - Efficient hierarchical agglomerative clustering algorithms on GPU using data partitioning.pdf:pdf},
isbn = {9780769545646},
journal = {Parallel and Distributed Computing, Applications and Technologies, PDCAT Proceedings},
keywords = {Computational speed-ups,Efficient partitioning,GPGPU,GPU clustering,GPU computing,GPU for acceleration,Hierarchical agglomerative clustering},
pages = {134--139},
title = {{Efficient hierarchical agglomerative clustering algorithms on GPU using data partitioning}},
year = {2011}
}
@article{Han2000,
abstract = {This paper proposes a novel evolutionary computing method called a
genetic quantum algorithm (GQA). GQA is based on the concept and
principles of quantum computing such as qubits and superposition of
states. Instead of binary, numeric, or symbolic representation, by
adopting qubit chromosome as a representation GQA can represent a linear
superposition of solutions due to its probabilistic representation. As
genetic operators, quantum gates are employed for the search of the best
solution. Rapid convergence and good global search capability
characterize the performance of GQA. The effectiveness and the
applicability of GQA are demonstrated by experimental results on the
knapsack problem, which is a well-known combinatorial optimization
problem. The results show that GQA is superior to other genetic
algorithms using penalty functions, repair methods and decoders},
author = {Han, Kuk-Hyun Han Kuk-Hyun and Kim, Jong-Hwan Kim Jong-Hwan},
doi = {10.1109/CEC.2000.870809},
file = {:home/chiroptera/Dropbox/mendeley/Han, Kim - 2000 - Genetic quantum algorithm and its application to combinatorial optimization problem.pdf:pdf},
isbn = {0-7803-6375-2},
journal = {Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)},
title = {{Genetic quantum algorithm and its application to combinatorial
optimization problem}},
volume = {2},
year = {2000}
}
@article{Wang2002,
abstract = {Clustering is the process of grouping a set of objects into classes of similar objects. Although definitions of similarity vary from one clustering model to another, in most of these models the concept of similarity is based on distances, e.g., Euclidean distance or cosine distance. In other words, similar objects are required to have close values on at least a set of dimensions. In this paper, we explore a more general type of similarity. Under the pCluster model we proposed, two objects are similar if they exhibit a coherent pattern on a subset of dimensions. For instance, in DNA microarray analysis, the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very much alike. Discovery of such clusters of genes is essential in revealing significant connections in gene regulatory networks. E-commerce applications, such as collaborative filtering, can also benefit from the new model, which captures not only the closeness of values of certain leading indicators but also the closeness of (purchasing, browsing, etc.) patterns exhibited by the customers. Our paper introduces an effective algorithm to detect such clusters, and we perform tests on several real and synthetic data sets to show its effectiveness.},
author = {Wang, H and Wang, H and Wang, W and Wang, W and Yang, H and Yang, H and Yu, P S and Yu, P S},
doi = {10.1145/564691.564737},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2002 - Clustering by pattern similarity in large data sets.pdf:pdf},
isbn = {1581134975},
issn = {07308078},
journal = {2002 ACM SIGMOD international conference on Management of Data},
pages = {394},
title = {{Clustering by pattern similarity in large data sets}},
url = {http://portal.acm.org/citation.cfm?doid=564691.564737},
volume = {2},
year = {2002}
}
@article{Mokhtari2014,
author = {Mokhtari, Reza and Stumm, Michael},
doi = {10.1109/IPDPS.2014.89},
file = {:home/chiroptera/Dropbox/mendeley/Mokhtari, Stumm - 2014 - BigKernel -- High Performance CPU-GPU Communication Pipelining for Big Data-Style Applications.pdf:pdf},
isbn = {978-1-4799-3800-1},
issn = {23321237},
journal = {Proceedings of the 2014 IEEE 28th International Parallel and Distributed Processing Symposium},
keywords = {CPU,GPU,communication,cpu-gpu,gpu,memory management,optimization,programming model,stream processing},
mendeley-tags = {cpu-gpu,gpu,memory management,programming model},
pages = {819--828},
title = {{BigKernel -- High Performance CPU-GPU Communication Pipelining for Big Data-Style Applications}},
url = {http://dx.doi.org/10.1109/IPDPS.2014.89},
year = {2014}
}
@article{Wu2009,
abstract = {In this paper, we report our research on using GPUs as accelerators for Business Intelligence(BI) analytics. We are particularly interested in analytics on very large data sets, which are common in today's real world BI applications. While many published works have shown that GPUs can be used to accelerate various general purpose applications with respectable performance gains, few attempts have been made to tackle very large problems. Our goal here is to investigate if the GPUs can be useful accelerators for BI analytics with very large data sets that cannot fit into GPUs onboard memory. Using a popular clustering algorithm, K-Means, as an example, our results have been very positive. For data sets smaller than GPU's onboard memory, the GPU-accelerated version is 6-12x faster than our highly optimized CPU-only version running on an 8-core workstation, or 200-400x faster than the popular benchmark program, MineBench, running on a single core. This is also 2-4x faster than the best reported work. For large data sets which cannot fit in GPU's memory, we further show that with a design which allows the computation on both CPU and GPU, as well as data transfers between them, to proceed in parallel, the GPU-accelerated version can still offer a dramatic performance boost. For example, for a data set with 100 million 2-d data points and 2,000 clusters, the GPU-accelerated version took about 6 minutes, while the CPU-only version running on an 8-core workstation took about 58 minutes. Compared to other approaches, GPU-accelerated implementations of analytics potentially provide better raw performance, better cost-performance ratios, and better energy performance ratios.},
author = {Wu, Ren and Zhang, Bin and Hsu, Meichun},
file = {:home/chiroptera/Dropbox/mendeley/Wu, Zhang, Hsu - 2009 - GPU-Accelerated Large Scale Analytics.pdf:pdf},
isbn = {HPL-2009-38},
journal = {Development},
keywords = {algorithm,big data,clustering,data mining,data mining clustering parallel algorithm gpu gpgp,gpgpu,gpu,k means,many core,multi core,parallel,s},
mendeley-tags = {big data,gpu},
number = {HPL-2009-38},
pages = {10},
title = {{GPU-Accelerated Large Scale Analytics}},
url = {http://www.hpl.hp.com/techreports/2009/HPL-2009-38.pdf},
year = {2009}
}
@article{Ghorpade2012,
abstract = {The future of computation is the Graphical Processing Unit, i.e. the GPU. The promise that the graphics cards have shown in the field of image processing and accelerated rendering of 3D scenes, and the computational capability that these GPUs possess, they are developing into great parallel computing units. It is quite simple to program a graphics processor to perform general parallel tasks. But after understanding the various architectural aspects of the graphics processor, it can be used to perform other taxing tasks as well. In this paper, we will show how CUDA can fully utilize the tremendous power of these GPUs. CUDA is NVIDIA’s parallel computing architecture. It enables dramatic increases in computing performance, by harnessing the power of the GPU. This paper talks about CUDA and its architecture. It takes us through a comparison of CUDA C/C++ with other parallel programming languages like OpenCL and DirectCompute. The paper also lists out the common myths about CUDA and how the future seems to be promising for CUDA.},
archivePrefix = {arXiv},
arxivId = {1202.4347},
author = {Ghorpade, Jayshree},
doi = {10.5121/acij.2012.3109},
eprint = {1202.4347},
file = {:home/chiroptera/Dropbox/mendeley/Ghorpade - 2012 - GPGPU Processing in CUDA Architecture.pdf:pdf},
issn = {2229726X},
journal = {Advanced Computing: An International Journal},
keywords = {gpgpu,gpu},
mendeley-tags = {gpgpu,gpu},
number = {1},
pages = {105--120},
title = {{GPGPU Processing in CUDA Architecture}},
volume = {3},
year = {2012}
}
@article{Shuai2006,
abstract = {This paper presents a generalized quantum particle model to greatly quicken and improve data clustering. The proposed model uses the random dynamics and quantum entanglement of quantum particles on a particle array. In comparison with classical nonquantum methods, the quantum particle model not only clusters much faster, but also has better clustering quality for multi-shape multi-distribution high-dimensional large-scale data sets with noise. The simulations and comparisons show the effectiveness of the quantum particle model},
author = {Shuai, Dianxun and Lu, Cunpai and Zhang, Bin},
doi = {10.1109/COMPSAC.2006.131},
file = {:home/chiroptera/Dropbox/mendeley/Shuai, Lu, Zhang - 2006 - Entanglement partitioning of quantum particles for data clustering.pdf:pdf},
isbn = {0769526551},
issn = {07303157},
journal = {Proceedings - International Computer Software and Applications Conference},
number = {2},
pages = {285--290},
title = {{Entanglement partitioning of quantum particles for data clustering}},
volume = {2},
year = {2006}
}
@article{Wang2011,
author = {Wang, Wei},
file = {:home/chiroptera/Dropbox/mendeley/Wang - 2011 - Design and Implementation of GPU-Based Prim ' s Algorithm.pdf:pdf},
journal = {International Journal of Modern Education and Computer Science},
number = {July},
pages = {55--62},
title = {{Design and Implementation of GPU-Based Prim ' s Algorithm}},
volume = {4},
year = {2011}
}
@article{Rajaraman2011a,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
author = {Rajaraman, Anand and Ullman, Jeffrey D},
doi = {10.1017/CBO9781139058452},
file = {:home/chiroptera/Dropbox/mendeley/Rajaraman, Ullman - 2011 - Mining of Massive Datasets(2).pdf:pdf},
isbn = {9781139058452},
issn = {01420615},
journal = {Lecture Notes for Stanford CS345A Web Mining},
pages = {328},
title = {{Mining of Massive Datasets}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139058452},
volume = {67},
year = {2011}
}
@article{Weinstein2013,
abstract = {How does one search for a needle in a multi-dimensional haystack without knowing what a needle is and without knowing if there is one in the haystack? This kind of problem requires a paradigm shift - away from hypothesis driven searches of the data - towards a methodology that lets the data speak for itself. Dynamic Quantum Clustering (DQC) is such a methodology. DQC is a powerful visual method that works with big, high-dimensional data. It exploits variations of the density of the data (in feature space) and unearths subsets of the data that exhibit correlations among all the measured variables. The outcome of a DQC analysis is a movie that shows how and why sets of data-points are eventually classified as members of simple clusters or as members of - what we call - extended structures. This allows DQC to be successfully used in a non-conventional exploratory mode where one searches data for unexpected information without the need to model the data. We show how this works for big, complex, real-world datasets that come from five distinct fields: i.e., x-ray nano-chemistry, condensed matter, biology, seismology and finance. These studies show how DQC excels at uncovering unexpected, small - but meaningful - subsets of the data that contain important information. We also establish an important new result: namely, that big, complex datasets often contain interesting structures that will be missed by many conventional clustering techniques. Experience shows that these structures appear frequently enough that it is crucial to know they can exist, and that when they do, they encode important hidden information. In short, we not only demonstrate that DQC can be flexibly applied to datasets that present significantly different challenges, we also show how a simple analysis can be used to look for the needle in the haystack, determine what it is, and find what this means.},
archivePrefix = {arXiv},
arxivId = {1310.2700},
author = {Weinstein, M and Meirer, F and Hume, A},
eprint = {1310.2700},
file = {:home/chiroptera/Dropbox/mendeley/Weinstein, Meirer, Hume - 2013 - Analyzing Big Data with Dynamic Quantum Clustering.pdf:pdf},
journal = {arXiv preprint arXiv: \ldots},
pages = {1--37},
title = {{Analyzing Big Data with Dynamic Quantum Clustering}},
url = {http://arxiv.org/abs/1310.2700 http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:No+Title\#0},
year = {2013}
}
@article{Xin2012,
author = {Xin, Miao and Li, Hao},
doi = {10.1109/IJCSS.2012.22},
file = {:home/chiroptera/Dropbox/mendeley/Xin, Li - 2012 - An implementation of GPU accelerated MapReduce Using Hadoop with OpenCL for data- and compute-intensive jobs.pdf:pdf},
isbn = {9780769547312},
journal = {Proceedings - 2012 International Joint Conference on Service Sciences, Service Innovation in Emerging Economy: Cross-Disciplinary and Cross-Cultural Perspective, IJCSS 2012},
keywords = {GPU acceleration,Hadoop,MapReduce,OpenCL},
pages = {6--11},
title = {{An implementation of GPU accelerated MapReduce: Using Hadoop with OpenCL for data- and compute-intensive jobs}},
year = {2012}
}
@book{Wittek,
author = {Wittek, Peter},
file = {:home/chiroptera/Dropbox/mendeley/Wittek - Unknown - Quantum Machine Learning What Quantum Computing Means to Data Mining.pdf:pdf},
title = {{Quantum Machine Learning: What Quantum Computing Means to Data Mining}}
}
@article{Li2007,
author = {Li, Zhi-hua and Wang, Shi-tong and Wuxi, Jiangsu},
file = {:home/chiroptera/Dropbox/mendeley/Li, Wang, Wuxi - 2007 - Quantum TheoryThe Unified Framework for FCM and QC Algorithm.pdf:pdf},
isbn = {1424410665},
keywords = {2-,algorithm,function,implemented by quantum clustering,interpretation,qc,quantum clustering,quantum potential,quantum theory,wave},
pages = {2--4},
title = {{Quantum Theory:The Unified Framework for FCM and QC Algorithm}},
year = {2007}
}
@article{Solan2005,
abstract = {We address the problem, fundamental to linguistics, bioinformatics, and certain other disciplines, of using corpora of raw symbolic sequential data to infer underlying rules that govern their production. Given a corpus of strings (such as text, transcribed speech, chromosome or protein sequence data, sheet music, etc.), our unsupervised algorithm recursively distills from it hierarchically structured patterns. The adios (automatic distillation of structure) algorithm relies on a statistical method for pattern extraction and on structured generalization, two processes that have been implicated in language acquisition. It has been evaluated on artificial context-free grammars with thousands of rules, on natural languages as diverse as English and Chinese, and on protein data correlating sequence with function. This unsupervised algorithm is capable of learning complex syntax, generating grammatical novel sentences, and proving useful in other fields that call for structure discovery from raw data, such as bioinformatics.},
author = {Solan, Zach and Horn, David and Ruppin, Eytan and Edelman, Shimon},
doi = {10.1073/pnas.0409746102},
file = {:home/chiroptera/Dropbox/mendeley/Solan et al. - 2005 - Unsupervised learning of natural languages.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {33},
pages = {11629--11634},
pmid = {16087885},
title = {{Unsupervised learning of natural languages.}},
volume = {102},
year = {2005}
}
@article{Baldini2014a,
author = {Baldini, Ioana and Fink, Stephen J. and Altman, Erik},
doi = {10.1109/SBAC-PAD.2014.30},
file = {:home/chiroptera/Dropbox/mendeley/Baldini, Fink, Altman - 2014 - Predicting GPU Performance from CPU Runs Using Machine Learning.pdf:pdf},
isbn = {978-1-4799-6905-0},
journal = {2014 IEEE 26th International Symposium on Computer Architecture and High Performance Computing},
pages = {254--261},
title = {{Predicting GPU Performance from CPU Runs Using Machine Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6970672},
year = {2014}
}
@article{Lopes2011,
abstract = {Graphics Processing Units (GPUs) placed at our disposal an unprecedented computational-power, largely surpassing the performance of cutting-edge CPUs (Central Processing Units). The high-parallelism inherent to the GPU makes this device especially well-suited to address Machine Learning (ML) problems with prohibitively computational intensive tasks. Nevertheless, few ML algorithms have been implemented on the GPU and most are not openly shared, posing difficulties for researchers and engineers aiming to develop GPU-based systems. To mitigate this problem, we propose the creation of an open source GPU Machine Learning Library (GPUMLib) that aims to provide the building blocks for the development of efficient GPU ML software. Experimental results on benchmark datasets show that the algorithms already implemented yield significant time savings over the CPU counterparts.},
author = {Lopes, Noel and Ribeiro, Bernardete},
file = {:home/chiroptera/Dropbox/mendeley/Lopes, Ribeiro - 2011 - GPUMLib An efficient open-source GPU machine learning library.pdf:pdf},
journal = {\ldots Journal of Computer Information Systems and \ldots},
keywords = {gpu,gpu computing,machine learning,machine learning algorithms},
mendeley-tags = {gpu,machine learning},
pages = {355--362},
title = {{GPUMLib: An efficient open-source GPU machine learning library}},
url = {http://www.ece.neu.edu/groups/nucar/NUCARTALKS/GPUMLib.pdf},
volume = {3},
year = {2011}
}
@article{Owens2006,
author = {Owens, John D and Luebke, David and Govindraju, Naga and Harris, Mark and Kruger, Jens and Lefohn, Aaron E and Purcell, Timothy J},
file = {:home/chiroptera/Dropbox/mendeley/Owens et al. - 2006 - A Survey of General Purpose Computation on Graphics Hardware.pdf:pdf},
journal = {Computer Graphics Forum},
number = {August},
pages = {21--51},
title = {{A Survey of General Purpose Computation on Graphics Hardware}},
url = {http://www.cs.virginia.edu/papers/ASurveyofGeneralPurposeComputationonGraphicsHardware.pdf},
year = {2006}
}
@article{Horn2001,
abstract = {We discuss novel clustering methods that are based on mapping data points to a Hilbert space by means of a Gaussian kernel. The first method, support vector clustering (SVC), searches for the smallest sphere enclosing data images in Hilbert space. The second, quantum clustering (QC), searches for the minima of a potential function defined in such a Hilbert space. In SVC, the minimal sphere, when mapped back to data space, separates into several components, each enclosing a separate cluster of points. A soft margin constant helps in coping with outliers and overlapping clusters. In QC, minima of the potential define cluster centers, and equipotential surfaces are used to construct the clusters. In both methods, the width of the Gaussian kernel controls the scale at which the data are probed for cluster formations. We demonstrate the performance of the algorithms on several data sets.},
author = {Horn, David},
doi = {10.1016/S0378-4371(01)00442-3},
file = {:home/chiroptera/Dropbox/mendeley/Horn - 2001 - Clustering via Hilbert space.pdf:pdf},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
keywords = {Clustering,Hilbert space,Kernel methods,Scale-space clustering,Schr\"{o}dinger equation,Support vector clustering},
month = dec,
number = {1-4},
pages = {70--79},
title = {{Clustering via Hilbert space}},
url = {http://www.sciencedirect.com/science/article/pii/S0378437101004423},
volume = {302},
year = {2001}
}
@article{Lourenco2007,
abstract = {We address the problem of clustering of contour images from hardware tools based on string descriptions, in a comparative study of cluster combination techniques. Several clustering algorithms are addressed using both the hierarchical agglomerative concept and partitional approaches. In the later class of algorithms, we explore: an adaptation of the K-means algorithm to string patterns using the median string as cluster representative; the error-correcting parsing approach by Fu; and the very recent spectral clustering approach. These algorithms are applied using several dissimilarity measures, namely: minimum code length based measures; dissimilarity based on the concept of reduction in grammatical complexity; and error-correcting parsing. In a first instance, clustering algorithms are applied individually to the image data set, and results are evaluated in terms of the error rate, taking as ground truth known labeling of the data. In a second step, we combine multiple data partitions, that we call a clustering ensemble, using three state-of-the-art clustering combination techniques. Results show that combination methods lead in general to better data partitioning, as compared to ground truth information.},
author = {Louren\c{c}o, Andr\'{e} and Fred, Ana},
doi = {10.1109/ACVMOT.2005.46},
file = {:home/chiroptera/Dropbox/mendeley/Louren\c{c}o, Fred - 2007 - Ensemble methods in the clustering of string patterns.pdf:pdf},
isbn = {0769522718},
journal = {Proceedings - Seventh IEEE Workshop on Applications of Computer Vision, WACV 2005},
pages = {143--148},
title = {{Ensemble methods in the clustering of string patterns}},
year = {2007}
}
@article{Wang2007,
abstract = {A new hybrid fuzzy clustering algorithm that incorporates the fuzzy c-means (FCM) into the quantum-behaved particle swarm optimization (QPSO) algorithm is proposed in this paper (QPSO+FCM). The QPSO has less parameters and higher convergent capability of the global optimizing than particle swarm optimization algorithm (PSO). So the iteration algorithm is replaced by the QPSO based on the gradient descent of FCM, which makes the algorithm have a strong global searching capacity and avoids the local minimum problems of FCM and in a large degree avoids depending on the initialization values. This paper also investigates the ability of FCM algorithm, PSO+FCM algorithm and GA+FCM algorithm with Iris testing data and Wine testing data. The simulation result proves that compared with other algorithms, the new algorithm not only has the favorable convergence but also has been obviously improved the clustering effect.},
author = {Wang, Hao Wang Hao and Yang, Shiqin Yang Shiqin and Xu, Wenbo Xu Wenbo and Sun, Jun Sun Jun},
doi = {10.1109/FSKD.2007.507},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2007 - Scalability of Hybrid Fuzzy C-Means Algorithm Based on Quantum-Behaved PSO.pdf:pdf},
isbn = {978-0-7695-2874-8},
journal = {Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)},
number = {Fskd},
title = {{Scalability of Hybrid Fuzzy C-Means Algorithm Based on Quantum-Behaved PSO}},
volume = {2},
year = {2007}
}
@misc{Guerra2009,
abstract = {The integration of usable and flexible analysis support in modelling environments is a key success factor in Model-Driven Development. In this paradigm, models are the core asset from which code is automatically generated, and thus ensuring model correctness is a fundamental quality control activity. For this purpose, a common approach is to transform the system models into formal semantic domains for verification. However, if the analysis results are not shown in a proper way to the end-user (e.g. in terms of the original language) they may become useless. In this paper we present a novel DSVL called BaVeL that facilitates the flexible annotation of verification results obtained in semantic domains to different formats, including the context of the original language. BaVeL is used in combination with a consistency framework, providing support for all steps in a verification process: acquisition of additional input data, transformation of the system models into semantic domains, verification, and flexible annotation of analysis results. The approach has been validated analytically by the cognitive dimensions framework, and empirically by its implementation and application to several DSVLs. Here we present a case study of a notation in the area of Digital Libraries, where the analysis is performed by transformations into Petri nets and a process algebra. © 2008 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Guerra, Esther and de Lara, Juan and Malizia, Alessio and D\'{\i}az, Paloma},
booktitle = {Information and Software Technology},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {0402594v3},
file = {:home/chiroptera/Dropbox/mendeley/Guerra et al. - 2009 - Supporting user-oriented analysis for multi-view domain-specific visual languages.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
keywords = {Back-annotation,Consistency,Domain-specific visual languages,Formal methods,Model transformation,Modelling environments},
number = {4},
pages = {769--784},
primaryClass = {arXiv:cond-mat},
title = {{Supporting user-oriented analysis for multi-view domain-specific visual languages}},
volume = {51},
year = {2009}
}
@article{Zhang2008,
abstract = {The principle advantage and shortcoming of quantum clustering algorithm (QC) is analyzed. Based on its shortcomings, an improved algorithm - exponent distance-based quantum clustering algorithm (EQDC) is produced. It improved the iterative procedure of QC algorithm and used exponent distance formula to measure the distance between data points and the cluster centers. Experimental results demonstrate that the cluster accuracy of EDQC outperforms that of QC, and the exponent distance formula used in the clustering process performs better than the Euclidean distance in data preprocessing. What's more, the IRIS dataset can come to a satisfied result without preprocessing.},
author = {Zhang, Yao and Wang, Peng and Chen, Gao Yun and Chen, Dong Dong and Ding, Rui and Zhang, Yan},
doi = {10.1109/KAMW.2008.4810518},
file = {:home/chiroptera/Dropbox/mendeley/Zhang et al. - 2008 - Quantum clustering algorithm based on exponent measuring distance.pdf:pdf},
isbn = {9781424435296},
journal = {2008 IEEE International Symposium on Knowledge Acquisition and Modeling Workshop Proceedings, KAM 2008},
keywords = {Clustering accuracy,Data preprocessing,Exponent distance-based quantum clustering algorit,Measuring formula,Quantum clustering algorithm,Quantum potential},
number = {1},
pages = {436--439},
title = {{Quantum clustering algorithm based on exponent measuring distance}},
year = {2008}
}
@article{Blekas2007a,
abstract = {Given a data set, a dynamical procedure is applied to the data points in order to shrink and separate, possibly overlapping clusters. Namely, Newton's equations of motion are employed to concentrate the data points around their cluster centers, using an attractive potential, constructed specially for this purpose. During this process, important information is gathered concerning the spread of each cluster. In succession this information is used to create an objective function that maps each cluster to a local maximum. Global optimization is then used to retrieve the positions of the maxima that correspond to the locations of the cluster centers. Further refinement is achieved by applying the EM-algorithm to a Gaussian mixture model whose construction and initialization is based on the acquired information. To assess the effectiveness of our method, we have conducted experiments on a plethora of benchmark data sets. In addition we have compared its performance against four clustering techniques that are well established in the literature. [All rights reserved Elsevier]},
author = {Blekas, K. and Lagaris, I.E.},
doi = {10.1016/j.patcog.2006.07.012},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Lagaris - 2007 - Newtonian clustering An approach based on molecular dynamics and global optimization(2).pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {clustering,global optimization,molecular dynamics,order statistics},
pages = {1734--1744},
title = {{Newtonian clustering: An approach based on molecular dynamics and global optimization}},
volume = {40},
year = {2007}
}
@article{Li2008,
abstract = {The enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum random walk (QRW) with the problem of data clustering, and develop two clustering algorithms based on the one dimensional QRW. Then, the probability distributions on the positions induced by QRW in these algorithms are investigated, which also indicates the possibility of obtaining better results. Consequently, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms are of fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.},
archivePrefix = {arXiv},
arxivId = {0812.1357},
author = {Li, Qiang and He, Yan and Jiang, Jing-ping},
eprint = {0812.1357},
file = {:home/chiroptera/Dropbox/mendeley/Li, He, Jiang - 2008 - A Novel Clustering Algorithm Based on Quantum Random Walk.pdf:pdf},
keywords = {data clustering,quantum compu-,quantum game,tation,unsupervised learning},
pages = {14},
title = {{A Novel Clustering Algorithm Based on Quantum Random Walk}},
url = {http://arxiv.org/abs/0812.1357},
year = {2008}
}
@article{Zentall2014,
abstract = {Learning by rats was facilitated when response-relevant cues were provided by other rats; learning increased as a fzlnction of number of cues provided. These results suggest that rats can learn by imitation. Learning by rats that observed conspeclifics not esnitting response-relevant cues was retarded compared to learning by rats that did not observe conspecifics. This indicates that a conspecific's presence can also inhibit learning, a result consistent with socia facilitation theory.},
author = {Zentall, Thomas R. and Levine, John M.},
file = {:home/chiroptera/Dropbox/mendeley/Zentall, Levine - 2014 - American Association for the Advancement of Science.pdf:pdf},
journal = {Science},
number = {5278},
pages = {1220--1221},
title = {{American Association for the Advancement of Science}},
volume = {178},
year = {2014}
}
@article{El-sherbiny,
abstract = {Quantum computing proved good results and performance when applied to solving optimization problems. This paper proposes a quantum crossover-based quantum genetic algorithm (QXQGA) for solving non-linear programming. Due to the significant role of mutation function on the QXQGA's quality, a number of quantum crossover and quantum mutation operators are presented for improving the capabilities of searching, overcoming premature convergence, and keeping diversity of population. For calibrating the QXQGA, the quantum crossover and mutation operators are evaluated using relative percentage deviation for selecting the best combination. In addition, a set of non-linear problems is used as benchmark functions to illustrate the effectiveness of optimizing the complexities with different dimensions, and the performance of the proposed QXQGA algorithm is compared with the quantum inspired evolutionary algorithm to demonstrate its superiority.},
author = {El-sherbiny, Mahmoud M},
file = {:home/chiroptera/Dropbox/mendeley/El-sherbiny - Unknown - Quantum Crossover Based Quantum Genetic Algorithm for Solving Non-linear Programming Quantum Rotational gate.pdf:pdf},
keywords = {-component,quantum},
title = {{Quantum Crossover Based Quantum Genetic Algorithm for Solving Non-linear Programming Quantum Rotational gate}}
}
@misc{Weinstein,
author = {Weinstein, Marvin},
title = {{DQClib: A Maple package implementing Dynamic Quantum Clustering (DQC)}},
url = {http://www.slac.stanford.edu/~niv/index\_files/DQCOverview1.html}
}
@article{Marinelli2009,
author = {Marinelli, E. E.},
file = {:home/chiroptera/Dropbox/mendeley/Marinelli - 2009 - Hyrax Cloud computing on mobile devices using MapReduce.pdf:pdf},
number = {September},
title = {{Hyrax: Cloud computing on mobile devices using MapReduce}},
volume = {0389},
year = {2009}
}
@article{Evans,
author = {Evans, Michael R},
file = {:home/chiroptera/Dropbox/mendeley/Evans - Unknown - Spatial Big Data Case Studies on Volume , Velocity , and Variety What is Spatial Big Data.pdf:pdf},
pages = {1--16},
title = {{Spatial Big Data : Case Studies on Volume , Velocity , and Variety What is Spatial Big Data ?}}
}
@article{Wittek2013,
abstract = {Clustering methods in machine learning may benefit from borrowing metaphors from physics. Dynamic quantum clustering associates a Gaussian wave packet with the multidimensional data points and regards them as eigenfunctions of the Schr??dinger equation. The clustering structure emerges by letting the system evolve and the visual nature of the algorithm has been shown to be useful in a range of applications. Furthermore, the method only uses matrix operations, which readily lend themselves to parallelization. In this paper, we develop an implementation on graphics hardware and investigate how this approach can accelerate the computations. We achieve a speedup of up to two magnitudes over a multicore CPU implementation, which proves that quantum-like methods and acceleration by graphics processing units have a great relevance to machine learning. ?? 2012 Elsevier Inc.},
author = {Wittek, Peter},
doi = {10.1016/j.jcp.2012.08.048},
file = {:home/chiroptera/Dropbox/mendeley/Wittek - 2013 - High-performance dynamic quantum clustering on graphics processors(2).pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
keywords = {Clustering,GPU computing,Quantum-like learning,Time-dependent schr??dinger equation},
pages = {262--271},
publisher = {Elsevier Inc.},
title = {{High-performance dynamic quantum clustering on graphics processors}},
url = {http://dx.doi.org/10.1016/j.jcp.2012.08.048},
volume = {233},
year = {2013}
}
@article{Sirotkovi2012,
author = {Sirotkovi, J and Dujmi, H and Papi, V},
file = {:home/chiroptera/Dropbox/mendeley/Sirotkovi, Dujmi, Papi - 2012 - K-Means Image Segmentation on Massively Parallel GPU Architecture.pdf:pdf},
isbn = {9789532330724},
pages = {489--494},
title = {{K-Means Image Segmentation on Massively Parallel GPU Architecture}},
year = {2012}
}
@article{Topics2010,
author = {Topics, Advanced and Topic, Machine Learning and Scribe, Dimensionality Reduction and Lecturer, Matt Faulkner and Date, Andreas Krause},
file = {:home/chiroptera/Dropbox/mendeley/Topics et al. - 2010 - Dimensionality reduction.pdf:pdf},
pages = {1--6},
title = {{Dimensionality reduction}},
year = {2010}
}
@article{Han2011,
abstract = {Graphics Processing Units (GPUs) have become a competitive accelerator for applications outside the graphics domain, mainly driven by the improvements inGPUprogrammability. Although the Compute Unified Device Architecture (CUDA) is a simple C-like interface for programming NVIDIA GPUs, porting applications to CUDA remains a challenge to average programmers. In particular, CUDA places on the programmer the burden of packaging GPU code in separate functions, of explicitly managing data transfer between the host and GPU memories, and of manually optimizing the utilization of the GPU memory. Practical experience shows that the programmer needs to make significant code changes, often tedious and error-prone, before getting an optimized program. We have designed hiCUDA, a high-level directive-based language for CUDA programming. It allows programmers to perform these tedious tasks in a simpler manner and directly to the sequential code, thus speeding up the porting process. In this paper, we describe the hiCUDA directives as well as the design and implementation of a prototype compiler that translates a hiCUDA program to a CUDA program. Our compiler is able to support real-world applications that span multiple procedures and use dynamically allocated arrays. Experiments using nine CUDA benchmarks show that the simplicity hiCUDA provides comes at no expense to performance.},
author = {Han, Tianyi David and Abdelrahman, Tarek S.},
doi = {10.1109/TPDS.2010.62},
file = {:home/chiroptera/Dropbox/mendeley/Han, Abdelrahman - 2011 - HiCUDA High-level GPGPU programming.pdf:pdf},
isbn = {1045-9219 VO - 22},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {CUDA,GPGPU,data-parallel programming,directive-based language,source-to-source compiler},
number = {1},
pages = {78--90},
title = {{HiCUDA: High-level GPGPU programming}},
volume = {23},
year = {2011}
}
@article{Vineet2009,
author = {Vineet, Vibhav},
file = {:home/chiroptera/Dropbox/mendeley/Vineet - 2009 - Fast Minimum Spanning Tree for Large Graphs on the GPU by Fast Minimum Spanning Tree for Large Graphs on the GPU.pdf:pdf},
number = {August},
title = {{Fast Minimum Spanning Tree for Large Graphs on the GPU by Fast Minimum Spanning Tree for Large Graphs on the GPU}},
volume = {2009},
year = {2009}
}
@article{Ji2013,
abstract = {Type Ia supernovae (SNe Ia) play a crucial role as standardizable cosmological candles, though the nature of their progenitors is a subject of active investigation. Recent observational and theoretical work has pointed to merging white dwarf binaries, referred to as the double-degenerate channel, as the possible progenitor systems for some SNe Ia. Additionally, recent theoretical work suggests that mergers which fail to detonate may produce magnetized, rapidly-rotating white dwarfs. In this paper, we present the first multidimensional simulations of the post-merger evolution of white dwarf binaries to include the effect of the magnetic field. In these systems, the two white dwarfs complete a final merger on a dynamical timescale, and are tidally disrupted, producing a rapidly-rotating white dwarf merger surrounded by a hot corona and a thick, differentially-rotating disk. The disk is strongly susceptible to the magnetorotational instability (MRI), and we demonstrate that this leads to the rapid growth of an initially dynamically weak magnetic field in the disk, the spin-down of the white dwarf merger, and to the subsequent central ignition of the white dwarf merger. Additionally, these magnetized models exhibit new features not present in prior hydrodynamic studies of white dwarf mergers, including the development of MRI turbulence in the hot disk, magnetized outflows carrying a significant fraction of the disk mass, and the magnetization of the white dwarf merger to field strengths \$\backslash sim 2 \backslash times 10\^{}8\$ G. We discuss the impact of our findings on the origin and observed properties of SNe Ia and magnetized white dwarfs.},
archivePrefix = {arXiv},
arxivId = {1302.5700},
author = {Bogert, F. Alexander and Smith, Nicholas and Holdener, John and Jong, Eric M. De and Hart, Andrew F. and Shamir, Lior and Allen, Alice and {Luca Cinquini}, Shakeh E. Khudikyan and Thompson, David R. and Mattmann, Chris A. and Wagstaff, Kiri and Lazio, Joseph and Jones, Dayton L. and Teuben, Peter},
doi = {10.1088/0004-637X/773/2/136},
eprint = {1302.5700},
file = {:home/chiroptera/Dropbox/mendeley/Bogert et al. - 2013 - Computing in Astronomy Applications and Examples.pdf:pdf},
issn = {0004-637X},
journal = {The Astrophysical Journal},
pages = {14},
title = {{Computing in Astronomy: Applications and Examples}},
url = {http://arxiv.org/abs/1302.5700},
volume = {submitted},
year = {2013}
}
@article{Zaroliagis1997,
author = {Zaroliagis, Christos D.},
doi = {10.1142/S012962649700005X},
file = {:home/chiroptera/Dropbox/mendeley/Zaroliagis - 1997 - Simple and Work-Efficient Parallel Algorithms for the Minimum Spanning Tree Problem.pdf:pdf},
issn = {0129-6264},
journal = {Parallel Processing Letters},
keywords = {applications 1,g with real,given a connected n,inten-,m -edge undirected graph,minimum spanning tree,network optimization with many,parallel random access machine,problem is one of,sively studied problems in,the minimum spanning tree,the most fundamental and,theoretical and practical,vertex},
number = {01},
pages = {25--37},
title = {{Simple and Work-Efficient Parallel Algorithms for the Minimum Spanning Tree Problem}},
volume = {07},
year = {1997}
}
@article{Weinstein2009b,
author = {Weinstein, Marvin and Horn, David},
doi = {10.1103/PhysRevE.80.066117},
file = {:home/chiroptera/Dropbox/mendeley//Weinstein, Horn - 2009 - Dynamic quantum clustering A method for visual exploration of structures in data.pdf:pdf},
issn = {1539-3755},
journal = {Physical Review E},
keywords = {clustering,quantum},
mendeley-tags = {clustering,quantum},
month = dec,
number = {6},
pages = {066117},
title = {{Dynamic quantum clustering: A method for visual exploration of structures in data}},
url = {http://link.aps.org/doi/10.1103/PhysRevE.80.066117},
volume = {80},
year = {2009}
}
@article{Kijsipongse2012,
abstract = {K-Means is the clustering algorithm which is widely used in many areas such as information retrieval, computer vision and pattern recognition. With the recent advance in General Purpose Graphics Processing Unit (GPGPU), we can use a modern GPU which is capable to do computation up to Tflops to calculate K-Means clustering on average problems. However, due to the exponential growth of data, the K-Means clustering on a single GPU will not be adequate for large datasets in the near future. In this paper, we present the design and implementation of an efficient large-scale parallel K-Means on GPU clusters. We utilize the massive parallelism in GPUs to speed up the most time consuming part of K-Means clustering in each node. We employ the dynamic load balancing to distribute workload equally on different GPUs installed in the clusters so as to improve the performance of the parallel K-Means at the inter-node level. We also take advantage from software distributed shared memory to simplify the communication and collaboration among nodes. The result of the evaluation shows the performance improvement of the parallel K-Means by maintaining load balance on GPU clusters.},
author = {Kijsipongse, Ekasit and U-Ruekolan, Suriya},
doi = {10.1109/JCSSE.2012.6261977},
file = {:home/chiroptera/Dropbox/mendeley/Kijsipongse, U-Ruekolan - 2012 - Dynamic load balancing on GPU clusters for large-scale K-Means clustering.pdf:pdf},
isbn = {9781467319218},
journal = {JCSSE 2012 - 9th International Joint Conference on Computer Science and Software Engineering},
keywords = {gpu,k-means},
mendeley-tags = {gpu,k-means},
pages = {346--350},
title = {{Dynamic load balancing on GPU clusters for large-scale K-Means clustering}},
year = {2012}
}
@article{Horn2001a,
abstract = {We propose a novel clusteringmethod that is an extension of ideas inher- ent to scale-space clustering and support-vector clustering. Like the lat- ter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scale- space probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schr¨ odinger equation of which the probability function is a solution. This Schr¨ odinger equation contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. Themethod has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schr¨ odinger potential to the locations of data points, we can apply this method to problems in high dimensions. 1},
author = {Horn, David and Gottlieb, Assaf},
file = {:home/chiroptera/Dropbox/mendeley/Horn, Gottlieb - 2001 - The Method of Quantum Clustering.pdf:pdf},
journal = {NIPS},
number = {1},
title = {{The Method of Quantum Clustering.}},
url = {http://www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/AA08.ps.gz},
year = {2001}
}
@article{Geras2011,
author = {Geras, Krzysztof Jerzy},
file = {:home/chiroptera/Dropbox/mendeley/Geras - 2011 - Prediction Markets for Machine Learning.pdf:pdf},
number = {248264},
title = {{Prediction Markets for Machine Learning}},
year = {2011}
}
@article{Letham2013,
abstract = {It is easy to find expert knowledge on the Internet on almost any topic, but obtaining a complete overview of a given topic is not always easy: information can be scattered across many sources and must be aggregated to be useful. We introduce a method for intelligently growing a list of relevant items, starting from a small seed of examples. Our algorithm takes advantage of the wisdom of the crowd, in the sense that there are many experts who post lists of things on the Internet. We use a collection of simple machine learning components to find these experts and aggregate their lists to produce a single complete and meaningful list. We use experiments with gold standards and open-ended experiments without gold standards to show that our method significantly outperforms the state of the art. Our method uses the ranking algorithm Bayesian Sets even when its underlying independence assumption is violated, and we provide a theoretical generalization bound to motivate its use.},
author = {Letham, Benjamin and Rudin, Cynthia and Heller, Katherine a.},
doi = {10.1007/s10618-013-0329-7},
file = {:home/chiroptera/Dropbox/mendeley/Letham, Rudin, Heller - 2013 - Growing a list.pdf:pdf},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {Collective intelligence,Internet data mining,Ranking,Set completion},
number = {3},
pages = {372--395},
title = {{Growing a list}},
volume = {27},
year = {2013}
}
@article{Zhang2008a,
abstract = {Affinity propagation (AP) is a clustering algorithm which has much better performance than traditional clustering approach such as k-means algorithm. In this paper, we present an algorithm called voting partition affinity propagation (voting-PAP) which is a method for clustering using evidence accumulation based on AP. Resulting clusters by voting-PAP are not constrained to be hyper-spherically shaped. Voting-PAP consists of three parts: partition affinity propagation (PAP), relaxed multi-root minimum spanning tree (MST) and majority voting. PAP is a method which can produce different exemplar set based on AP. Relaxed multi-root MST is a data point assign algorithm which has better performance than nearest assign rule. Majority voting is a scheme used to find a consistent clustering result of different partitions based on the idea of evidence accumulation. We also discuss how to find an appropriate threshold corresponding to an approximate ideal consistent partition in this paper.},
author = {Zhang, Xuqing Zhang Xuqing and Wu, Fei Wu Fei and Zhuang, Yueting Zhuang Yueting},
doi = {10.1109/ICPR.2008.4761213},
file = {:home/chiroptera/Dropbox/mendeley/Zhang, Wu, Zhuang - 2008 - Clustering by evidence accumulation on affinity propagation.pdf:pdf},
isbn = {978-1-4244-2174-9},
issn = {1051-4651},
journal = {2008 19th International Conference on Pattern Recognition},
title = {{Clustering by evidence accumulation on affinity propagation}},
year = {2008}
}
@article{Arbelaez2013,
author = {Arbelaez, Alejandro and Quesada, Luis},
file = {:home/chiroptera/Dropbox/mendeley/Arbelaez, Quesada - 2013 - Parallelising the k-Medoids Clustering Problem Using Space-Partitioning.pdf:pdf},
journal = {Sixth Annual Symposium on Combinatorial Search},
keywords = {Full Papers,k-medoids,parallel},
mendeley-tags = {k-medoids,parallel},
pages = {20--28},
title = {{Parallelising the k-Medoids Clustering Problem Using Space-Partitioning}},
url = {http://www.aaai.org/ocs/index.php/SOCS/SOCS13/paper/view/7225\&lt;/ee\&gt;},
year = {2013}
}
@article{Sousa2015,
abstract = {Abstract—This paper presents (i) a parallel, platform- independent variant of Bor˚ uvka’s algorithm, an efficient Min- imum Spanning Tree (MST) solver, and (ii) a comprehensive comparison of MST-solver implementations, both on multi-core CPU-chips and GPUs. The core of our variant is an effective and explicit contraction of the graph. Our multi-core CPU implementation scales linearly up to 8 threads, whereas the GPU implementation performs considerably better than the optimal number of threads running on the CPU. We also show that our implementations outperform all other parallel MST-solver implementations in (ii), for a broad set of publicly available road- network graphs.},
author = {Sousa, Cristiano da Silva and Mariano, Artur and Proen\c{c}a, Alberto},
file = {:home/chiroptera/Dropbox/mendeley/Sousa, Mariano, Proen\c{c}a - Unknown - A Generic and Highly Efficient Parallel Variant of Boruvka ’s Algorithm.pdf:pdf},
keywords = {cpu,gpu,graph,mst,parallel},
mendeley-tags = {cpu,gpu,graph,mst,parallel},
title = {{A Generic and Highly Efficient Parallel Variant of Boruvka ’s Algorithm}},
url = {https://github.com/Beatgodes/BoruvkaUMinho}
}
@misc{Graham1985,
abstract = {It is standard practice among authors discussing the minimum spanning tree problem to refer to the work of Kruskal(1956) and Prim (1957) as the sources of the problem and its first efficient solutions, despite the citation by both of Boruvka (1926) as a predecessor. In fact, there are several apparently independent sources and algorithmic solutions of the problem. They have appeared in Czechoslovakia, France, and Poland, going back to the beginning of this century. We shall explore and compare these works and their motivations, and relate them to the most recent advances on the minimum spanning tree problem.},
author = {Graham, R.L. and Hell, Pavol},
booktitle = {IEEE Annals of the History of Computing},
doi = {10.1109/MAHC.1985.10011},
file = {:home/chiroptera/Dropbox/mendeley/Graham, Hell - 1985 - On the History of the Minimum Spanning Tree Problem.pdf:pdf},
issn = {1058-6180},
number = {1},
pages = {43--57},
title = {{On the History of the Minimum Spanning Tree Problem}},
volume = {7},
year = {1985}
}
@article{Blekas2007,
abstract = {Given a data set, a dynamical procedure is applied to the data points in order to shrink and separate, possibly overlapping clusters. Namely, Newton's equations of motion are employed to concentrate the data points around their cluster centers, using an attractive potential, constructed specially for this purpose. During this process, important information is gathered concerning the spread of each cluster. In succession this information is used to create an objective function that maps each cluster to a local maximum. Global optimization is then used to retrieve the positions of the maxima that correspond to the locations of the cluster centers. Further refinement is achieved by applying the EM-algorithm to a Gaussian mixture model whose construction and initialization is based on the acquired information. To assess the effectiveness of our method, we have conducted experiments on a plethora of benchmark data sets. In addition we have compared its performance against four clustering techniques that are well established in the literature. [All rights reserved Elsevier]},
author = {Blekas, K. and Lagaris, I.E.},
doi = {10.1016/j.patcog.2006.07.012},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Lagaris - 2007 - Newtonian clustering An approach based on molecular dynamics and global optimization.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {clustering,global optimization,molecular dynamics,order statistics},
pages = {1734--1744},
title = {{Newtonian clustering: An approach based on molecular dynamics and global optimization}},
volume = {40},
year = {2007}
}
@article{Aidos2012,
author = {Aidos, Helena and Fred, Ana},
doi = {10.1016/j.patcog.2011.12.009},
file = {:home/chiroptera/Dropbox/mendeley/Aidos, Fred - 2012 - Statistical modeling of dissimilarity increments for d-dimensional data Application in partitional clustering.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Dissimilarity increments,Gaussian mixture decomposition,Likelihood-ratio test,Minimum description length,Partitional clustering,dissimilarity increments,likelihood-ratio test,partitional clustering},
number = {9},
pages = {3061--3071},
publisher = {Elsevier},
title = {{Statistical modeling of dissimilarity increments for d-dimensional data Application in partitional clustering}},
url = {http://dx.doi.org/10.1016/j.patcog.2011.12.009},
volume = {45},
year = {2012}
}
@article{DiMarco2013,
author = {DiMarco, Jeffrey and Taufer, Michela},
doi = {10.1117/12.2018069},
file = {:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms(3).pdf:pdf},
isbn = {9780819495433},
issn = {0277786X},
journal = {Spie},
keywords = {0,cuda,cuda 5,divisive hierarchical clustering,k-means},
mendeley-tags = {cuda,k-means},
pages = {87520E},
title = {{Performance impact of dynamic parallelism on different clustering algorithms}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2018069},
year = {2013}
}
@article{CalatravaMoreno2013,
author = {{Calatrava Moreno}, Maria Del Carmen and Auzinger, Thomas},
doi = {10.1109/SOCA.2013.15},
file = {:home/chiroptera/Dropbox/mendeley/Calatrava Moreno, Auzinger - 2013 - General-Purpose Graphics Processing Units in Service-Oriented Architectures.pdf:pdf},
isbn = {978-1-4799-2702-9},
journal = {2013 IEEE 6th International Conference on Service-Oriented Computing and Applications},
pages = {260--267},
title = {{General-Purpose Graphics Processing Units in Service-Oriented Architectures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6717316},
year = {2013}
}
@article{Rajaraman2011,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
author = {Rajaraman, Anand and Ullman, Jeffrey D},
doi = {10.1017/CBO9781139058452},
file = {:home/chiroptera/Dropbox/mendeley/Rajaraman, Ullman - 2011 - Mining of Massive Datasets.pdf:pdf},
isbn = {9781139058452},
issn = {01420615},
journal = {Lecture Notes for Stanford CS345A Web Mining},
pages = {328},
title = {{Mining of Massive Datasets}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139058452},
volume = {67},
year = {2011}
}
@article{Nadungodage2013,
abstract = {Recommendation systems are a popular marketing strategy for online service providers. These systems predict a customer's future preferences from the past behaviors of that customer and the other customers. Most of the popular online stores process millions of transactions per day; therefore, providing quick and quality recommendations using the large amount of data collected from past transactions can be challenging. Parallel processing power of GPUs can be used to accelerate the recommendation process. However, the amount of memory available on a GPU card is limited; thus, a number of passes may be required to completely process a large-scale dataset. This paper proposes two parallel, item-based recommendation algorithms implemented using the CUDA platform. Considering the high sparsity of the user-item data, we utilize two compression techniques to reduce the required number of passes and increase the speedup. The experimental results on synthetic and real-world datasets show that our algorithms outperform the respective CPU implementations and also the na\~{A}¯ve GPU implementation which does not use compression. \^{A}© 2013 IEEE.},
author = {Nadungodage, Chandima Hewa and Xia, Yuni and Lee, John Jaehwan and Lee, Myungcheol and Park, Choon Seo},
doi = {10.1109/BigData.2013.6691571},
file = {:home/chiroptera/Dropbox/mendeley/Nadungodage et al. - 2013 - GPU accelerated item-based collaborative filtering for big-data applications.pdf:pdf},
isbn = {9781479912926},
journal = {Proceedings - 2013 IEEE International Conference on Big Data, Big Data 2013},
keywords = {CUDA,big-data,collaborative filtering,recommendation systems GPU},
pages = {175--180},
title = {{GPU accelerated item-based collaborative filtering for big-data applications}},
year = {2013}
}
@article{Chen2014,
abstract = {In this paper, we review the background and state-of-the-art of big data. We first introduce the general background of big data and review related technologies, such as could computing, Internet of Things, data centers, and Hadoop.We then focus on the four phases of the value chain of big data, i.e., data generation, data acquisition, data storage, and data analysis. For each phase, we introduce the general background, discuss the technical challenges, and review the latest advances. We finally examine the several representative applications of big data, including enterprise management, Internet of Things, online social networks, medial applications, collective intelligence, and smart grid. These discussions aimto provide a comprehensive overview and big-picture to readers of this exciting area. This survey is concluded with a discussion of open problems and future directions.},
author = {Chen, Min and Mao, Shiwen and Liu, Yunhao},
doi = {10.1007/s11036-013-0489-0},
file = {:home/chiroptera/Dropbox/mendeley/Chen, Mao, Liu - 2014 - Big data A survey.pdf:pdf},
isbn = {1383-469X},
issn = {1383469X},
journal = {Mobile Networks and Applications},
keywords = {Big data,Big data analysis,Cloud computing,Data center,Hadoop,Internet of things,Smart grid},
number = {2},
pages = {171--209},
title = {{Big data: A survey}},
volume = {19},
year = {2014}
}
@article{DiMarco2013a,
abstract = {Discover and quantify the performance gains of dynamic parallelism for clustering algorithms on GPUs. Dynamic parallelism effectively eliminates the superfluous back and forth communication between the GPU and CPU through nested kernel computations. The change in performance is measured using two well-known clustering algorithms that exhibit data dependencies: the K-means clustering and the hierarchical clustering. K-means has a sequential data dependence wherein iterations occur in a linear fashion, while the hierarchical clustering has a tree-like dependence that produces split tasks. Analyzing the performance of these data-dependent algorithms gives us a better understanding of the benefits or potential drawbacks of CUDA 5's new dynamic parallelism feature.},
author = {DiMarco, Jeffrey and Taufer, Michela},
doi = {10.1117/12.2018069},
file = {:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms.pdf:pdf;:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms(2).pdf:pdf},
isbn = {9780819495433},
issn = {0277786X},
journal = {Spie},
keywords = {0,cuda 5,divisive hierarchical clustering,k-means},
pages = {87520E},
title = {{Performance impact of dynamic parallelism on different clustering algorithms}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2018069},
year = {2013}
}
@article{Woong-KeeLoh2014,
author = {Woong-KeeLoh and Kim, Young-Kuk},
doi = {10.1109/BDCloud.2014.130},
file = {:home/chiroptera/Dropbox/mendeley/Woong-KeeLoh, Kim - 2014 - A GPU-accelerated Density-Based Clustering Algorithm.pdf:pdf},
isbn = {978-1-4799-6719-3},
journal = {2014 IEEE Fourth International Conference on Big Data and Cloud Computing},
keywords = {12,3,and there have been,bohm et al,clustering,cuda,density-based clustering,divide-and-conquer,gpu,many approaches to improve,parallel algorithm,performance,proposed an algorithm,their},
mendeley-tags = {clustering,gpu},
pages = {775--776},
title = {{A GPU-accelerated Density-Based Clustering Algorithm}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7034874},
year = {2014}
}
@article{Aimeur2013,
abstract = {We show how the quantum paradigm can be used to speed up unsupervised learning algorithms. More precisely, we explain how it is possible to accelerate learning algorithms by quantizing some of their subroutines. Quantization refers to the process that partially or totally converts a classical algorithm to its quantum counterpart in order to improve performance. In particular, we give quantized versions of clustering via minimum spanning tree, divisive clustering and k-medians that are faster than their classical analogues. We also describe a distributed version of k-medians that allows the participants to save on the global communication cost of the protocol compared to the classical version. Finally, we design quantum algorithms for the construction of a neighbourhood graph, outlier detection as well as smart initialization of the cluster centres.},
author = {A\"{\i}meur, Esma and Brassard, Gilles and Gambs, S\'{e}bastien},
doi = {10.1007/s10994-012-5316-5},
file = {:home/chiroptera/Dropbox/mendeley/A\"{\i}meur, Brassard, Gambs - 2013 - Quantum speed-up for unsupervised learning.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Clustering,Grover's algorithm,Quantum information processing,Quantum learning,Unsupervised learning},
number = {February 2012},
pages = {261--287},
title = {{Quantum speed-up for unsupervised learning}},
volume = {90},
year = {2013}
}
@article{NVIDIACorporation2006,
author = {{NVIDIA Corporation}},
file = {:home/chiroptera/Dropbox/mendeley/NVIDIA Corporation - 2006 - CUDA Programming Model Overview.pdf:pdf},
title = {{CUDA Programming Model Overview}},
year = {2006}
}
@article{Horn2001b,
author = {Horn, David and Gottlieb, Assaf},
doi = {10.1103/PhysRevLett.88.018702},
file = {:home/chiroptera/Dropbox/mendeley//Horn, Gottlieb - 2001 - Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics.ps:ps;:home/chiroptera/Dropbox/mendeley/Horn, Gottlieb - 2001 - Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics.pdf:pdf},
issn = {0031-9007},
journal = {Physical Review Letters},
number = {1},
pages = {1--4},
title = {{Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics}},
url = {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.88.018702},
volume = {88},
year = {2001}
}
@article{Su2012,
abstract = {GPU (Graphics Processing Unit) has a great impact on computing field. To enhance the performance of computing systems, researchers and developers use the parallel computing architecture of GPU. On the other hand, to reduce the development time of new products, two programming models are included in GPU, which are OpenCL (Open Computing Language) and CUDA (Compute Unified Device Architecture). The benefit of involving the two programming models in GPU is that researchers and developers don't have to understand OpenGL, DirectX or other program design, but can use GPU through simple programming language. OpenCL is an open standard API, which has the advantage of cross-platform. CUDA is a parallel computer architecture developed by NVIDIA, which includes Runtime API and Driver API. Compared with OpenCL, CUDA is with better performance. In this paper, we used plenty of similar kernels to compare the computing performance of C, OpenCL and CUDA, the two kinds of API's on NVIDIA Quadro 4000 GPU. The experimental result showed that, the executive time of CUDA Driver API was 94.9\%\~{}99.0\% faster than that of C, while and the executive time of CUDA Driver API was 3.8\%\~{}5.4\% faster than that of OpenCL. Accordingly, the cross-platform characteristic of OpenCL did not affect the performance of GPU.},
annote = {Comparison between OpenCL and CUDA.},
author = {Su, Ching Lung and Chen, Po Yu and Lan, Chun Chieh and Huang, Long Sheng and Wu, Kuo Hsuan},
doi = {10.1109/APCCAS.2012.6419068},
file = {:home/chiroptera/Dropbox/mendeley/Su et al. - 2012 - Overview and comparison of OpenCL and CUDA technology for GPGPU.pdf:pdf},
isbn = {9781457717291},
journal = {IEEE Asia-Pacific Conference on Circuits and Systems, Proceedings, APCCAS},
keywords = {comparison,cuda,opencl},
mendeley-tags = {comparison,cuda,opencl},
pages = {448--451},
title = {{Overview and comparison of OpenCL and CUDA technology for GPGPU}},
year = {2012}
}
@misc{Casper,
abstract = {A Quantum-Modeled K-Means clustering algorithm for multi- band image segmentation is explored and evaluated. Data sets of interest include multi-band RGB imagery, which subsequent to classification is analyzed and assessed for accuracy. Results demonstrate that under specific conditions the algorithm exhibits improved accuracy, when compared to its classical counterpart. Categories},
author = {Casper, Ellis and Hung, Chih-Cheng and Jung, Edward and Yang, Ming},
file = {:home/chiroptera/Dropbox/mendeley/Casper et al. - Unknown - A Quantum-Modeled K-Means Clustering Algorithm for Multi-band Image Segmentation.pdf:pdf},
keywords = {K-Means,QK-Means,Quantum computing,image,image segmentation,k-means,qk-means,quantum computing,qubit},
mendeley-tags = {image segmentation,k-means,qk-means,qubit},
number = {3},
pages = {158--163},
title = {{A Quantum-Modeled K-Means Clustering Algorithm for Multi-band Image Segmentation}},
url = {http://delivery.acm.org/10.1145/2410000/2401639/p158-casper.pdf?ip=193.136.132.10\&id=2401639\&acc=ACTIVE SERVICE\&key=2E5699D25B4FE09E.F7A57B2C5B227641.4D4702B0C3E38B35.4D4702B0C3E38B35\&CFID=476955365\&CFTOKEN=55494231\&\_\_acm\_\_=1423057410\_0d77d9b5028cb3},
urldate = {2015-02-04},
volume = {1}
}
@article{Jiang2009,
abstract = {Labeling the connected components in the feature space is an important step in grid based clustering algorithms in data mining. Although connected components labeling algorithms have been highly improved in image processing domain, there is little progress in grid based clustering in data mining domain. Two problems exist in transplanting these algorithms from image processing to data mining. One is how to process multi-dimensional dataset. The other is how to reduce the cost of auxiliary space. This paper describes an optimal two-scan Connected Components Labeling algorithm based that in image processing domain. It does not need auxiliary space, and easy to be extended to multi-dimension data set.},
author = {Jiang, Tao and Qiu, Ming and Chen, Jie and Cao, Xue},
doi = {10.1109/DBTA.2009.144},
file = {:home/chiroptera/Dropbox/mendeley/Jiang et al. - 2009 - LILA A connected components labeling algorithm in grid-based clustering.pdf:pdf},
isbn = {9780769536040},
journal = {Proceedings - 2009 1st International Workshop on Database Technology and Applications, DBTA 2009},
keywords = {Connected components labeling,Grid-based clustering,Multi-dimensional dataset},
pages = {213--216},
title = {{LILA: A connected components labeling algorithm in grid-based clustering}},
year = {2009}
}
@article{Horn2004,
abstract = {We describe results of a novel algorithm for grammar induction from a large corpus. The ADIOS (Automatic DIstillation of Structure) algorithm searches for significant patterns, chosen according to context dependent statistical criteria, and builds a hierarchy of such patterns according to a set of rules leading to structured generalization. The corpus is thus generalized into a context free grammar (CFG), composed of patterns, equivalence classes and words of the initial lexicon. We have evaluated our method both on corpora generated by CFG and on natural language ones. The performance of ADIOS is judged by searching for both good recall (acceptance of correct novel sentences) and good precision (production of correct novel sentences). The results are very encouraging.},
author = {Horn, David and Solan, Zach and Ruppin, Eytan and Edelman, Shimon},
file = {:home/chiroptera/Dropbox/mendeley/Horn et al. - 2004 - Unsupervised language acquisition syntax from plain corpus.pdf:pdf},
journal = {\ldots on Human Language},
title = {{Unsupervised language acquisition: syntax from plain corpus}},
url = {http://horn.tau.ac.il/~horn/publications/newcastle.pdf},
year = {2004}
}
@article{Kindratenko2009a,
abstract = {Large-scale GPU clusters are gaining popularity in the scientific computing community. However, their deployment and production use are associated with a number of new challenges. In this paper, we present our efforts to address some of the challenges with building and running GPU clusters in HPC environments. We touch upon such issues as balanced cluster architecture, resource sharing in a cluster environment, programming models, and applications for GPU clusters.},
author = {Kindratenko, Volodymyr V. and Enos, Jeremy J. and Shi, Guochun and Showerman, Michael T. and Arnold, Galen W. and Stone, John E. and Phillips, James C. and Hwu, Wen Mei},
doi = {10.1109/CLUSTR.2009.5289128},
file = {:home/chiroptera/Dropbox/mendeley/Kindratenko et al. - 2009 - GPU clusters for high-performance computing.pdf:pdf},
isbn = {9781424450121},
issn = {15525244},
journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
title = {{GPU clusters for high-performance computing}},
year = {2009}
}
@article{Kaldewey2011,
author = {Kaldewey, Tim},
file = {:home/chiroptera/Dropbox/mendeley/Kaldewey - 2011 - Large-Scale GPU programming.pdf:pdf},
keywords = {gpu},
mendeley-tags = {gpu},
title = {{Large-Scale GPU programming}},
year = {2011}
}
@article{Misi2012,
author = {Mi\v{s}i, Marko J and \^{C}, M and Toma\v{s}evi, Milo V},
file = {:home/chiroptera/Dropbox/mendeley/Mi\v{s}i, \^{C}, Toma\v{s}evi - 2012 - Evolution and Trends in GPU Computing.pdf:pdf},
isbn = {9789532330724},
journal = {MIPRO, 2012 Proceedings of the 35th International Convention},
keywords = {API,CPU,CUDA,GPU computing,NVIDIA compute unified device architecture,application program interfaces,application programming interface,central processing units,commercial applications,general purpose computation,graphics coprocessors,graphics hardware,graphics processing units,latency-oriented processors,parallel architectures,task-parallel processors,throughput oriented processors},
pages = {289--294},
title = {{Evolution and Trends in GPU Computing}},
year = {2012}
}
@article{Karantasis2010,
abstract = {Many-core graphics processors are playing today an important role in the advancements of modern highly concurrent processors. Their ability to accelerate computation is being explored under several scientific fields. In the current paper we present the acceleration of a widely used data clustering algorithm, K-means, in the context of high performance GPU clusters. As opposed to most related implementation efforts that use MPI to port their target applications on a GPU cluster, our implementation follows the Software Distributed Shared Memory (SDSM) paradigm in order to distribute information and computation across the accelerator cluster. In order to investigate the efficiency of a programming model that offers shared memory abstraction on GPU clusters we present two implementations, one that is based on a SDSM implementation of OpenMP and another that utilizes the Pleiad cluster middleware on top of the Java platform. The first results show that such an implementation is feasible in order to accelerate a broad category of large scale, data intensive applications, among which K-means is a characteristic case.},
author = {Karantasis, Konstantinos I. and Polychronopoulos, Eleftherios D. and Dimitrakopoulos, George N.},
doi = {10.1109/CLUSTERWKSP.2010.5613079},
file = {:home/chiroptera/Dropbox/mendeley/Karantasis, Polychronopoulos, Dimitrakopoulos - 2010 - Accelerating data clustering on GPU-based clusters under shared memory abstractio.pdf:pdf},
isbn = {9781424483969},
journal = {2010 IEEE International Conference on Cluster Computing Workshops and Posters, Cluster Workshops 2010},
keywords = {GPU clusters, SDSM, Pleiad, CUDA, K-means},
title = {{Accelerating data clustering on GPU-based clusters under shared memory abstraction}},
year = {2010}
}
@article{Papenhausen2013,
abstract = {Clustering is an important preparation step in big data processing. It may even be used to detect redundant data points as well as outliers. Elimination of redundant data and duplicates can serve as a viable means for data reduction and it can also aid in sampling. Visual feedback is very valuable here to give users confidence in this process. Furthermore, big data preprocessing is seldom interactive, which stands at conflict with users who seek answers immediately. The best one can do is incremental preprocessing in which partial and hopefully quite accurate results become available relatively quickly and are then refined over time. We propose a correlation clustering framework which uses MDS for layout and GPU-acceleration to accomplish these goals. Our domain application is the correlation clustering of atmospheric mass spectrum data with 8 million data points of 450 dimensions each. © 2013 IEEE.},
author = {Papenhausen, Eric and Wang, Bing and Ha, Sungsoo and Zelenyuk, Alla and Imre, Dan and Mueller, Klaus},
doi = {10.1109/BigData.2013.6691716},
file = {:home/chiroptera/Dropbox/mendeley/Papenhausen et al. - 2013 - GPU-accelerated incremental correlation clustering of large data with visual feedback.pdf:pdf},
isbn = {9781479912926},
journal = {Proceedings - 2013 IEEE International Conference on Big Data, Big Data 2013},
keywords = {GPU,big data,clustering,correlation,visual analytics,visualization},
pages = {63--70},
title = {{GPU-accelerated incremental correlation clustering of large data with visual feedback}},
year = {2013}
}
@article{Slota2014,
author = {Slota, George M. and Rajamanickam, Sivasankaran and Madduri, Kamesh},
doi = {10.1109/IPDPS.2014.64},
file = {:home/chiroptera/Dropbox/mendeley/Slota, Rajamanickam, Madduri - 2014 - BFS and Coloring-based Parallel Algorithms for Strongly Connected Components and Related Problems.pdf:pdf},
isbn = {1530-2075 VO -},
keywords = {-strongly connected components,7,bfs,coloring,its use,most parallel,multicore algorithms,often met with limited,performance analysis,scc algorithms have avoided,success,therefore},
pages = {1--22},
title = {{BFS and Coloring-based Parallel Algorithms for Strongly Connected Components and Related Problems}},
year = {2014}
}
@article{Wu2011,
abstract = {The \$k\$-means algorithm is widely used for unsupervised clustering. This paper describes an efficient CUDA-based \$k\$-means algorithm. Different from existing GPU-based k-means algorithms, our algorithm achieves better efficiency by utilizing the triangle inequality. Our algorithm explores the trade-off between load balance and memory access coalescing through data layout management. Because the effectiveness of the triangle inequity depends on the input data, we further propose a hybrid algorithm that adaptively determines whether to apply the triangle inequality. The efficiency of our algorithm is validated through extensive experiments, which demonstrate improved performance over existing CPU-based and CUDA-based k-means algorithms, in terms of both speed and scalability.},
author = {Wu, Jiadong and Hong, Bo},
doi = {10.1109/IPDPS.2011.331},
file = {:home/chiroptera/Dropbox/mendeley/Wu, Hong - 2011 - An efficient k-means algorithm on CUDA.pdf:pdf},
isbn = {9780769543857},
issn = {1530-2075},
journal = {IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum},
keywords = {CUDA,GPU,K-means},
pages = {1740--1749},
title = {{An efficient k-means algorithm on CUDA}},
year = {2011}
}
@article{Fred2001,
abstract = {Given an arbitrary data set, to which no particular paramet- rical, statistical or geometrical structure can be assumed, different clus- tering algorithms will in general produce different data partitions. In fact, several partitions can also be obtained by using a single clustering algo- rithm due to dependencies on initialization or the selection of the value of some design parameter. This paper addresses the problem of finding consistent clusters in data partitions, proposing the analysis of the most common associations performed in a majority voting scheme. Combina- tion of clustering results are performed by transforming data partitions into a co-association sample matrix, which maps coherent associations. This matrix is then used to extract the underlying consistent clusters. The proposed methodology is evaluated in the context of k-means clus- tering, a new clustering algorithm - voting-k-means, being presented. Examples, using both simulated and real data, show how this major- ity voting combination scheme simultaneously handles the problems of selecting the number of clusters, and dependency on initialization. Fur- thermore, resulting clusters are not constrained to be hyper-spherically shaped.},
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2001 - Finding consistent clusters in data partitions.pdf:pdf},
isbn = {3540422846},
journal = {Multiple classifier systems},
pages = {309--318},
title = {{Finding consistent clusters in data partitions}},
url = {http://link.springer.com/chapter/10.1007/3-540-48219-9\_31},
year = {2001}
}
@article{Arefin2012,
author = {Arefin, Ahmed Shamsul and Riveros, Carlos and Berretta, Regina and Moscato, Pablo},
doi = {10.1109/ICCSE.2012.6295143},
file = {:home/chiroptera/Dropbox/mendeley/Arefin et al. - 2012 - kNN-MST-Agglomerative A fast and scalable graph-based data clustering approach on GPU.pdf:pdf},
isbn = {9781467302425},
journal = {ICCSE 2012 - Proceedings of 2012 7th International Conference on Computer Science and Education},
keywords = {Computer Application},
number = {Iccse},
pages = {585--590},
title = {{kNN-MST-Agglomerative: A fast and scalable graph-based data clustering approach on GPU}},
year = {2012}
}
@article{Jain2010,
abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Jain, Anil K},
doi = {10.1016/j.patrec.2009.09.011},
file = {:home/chiroptera/Dropbox/mendeley/Jain - 2010 - Data clustering 50 years beyond K-means.pdf:pdf},
isbn = {9781424417360},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Data clustering,Historical developments,King-Sun Fu prize,Perspectives on clustering,User's dilemma},
number = {8},
pages = {651--666},
publisher = {Elsevier B.V.},
title = {{Data clustering: 50 years beyond K-means}},
url = {http://dx.doi.org/10.1016/j.patrec.2009.09.011},
volume = {31},
year = {2010}
}
@article{Varshavsky2007a,
abstract = {Motivation: Feature selection methods aim to reduce the complexity of data and to uncover the most relevant biological variables. In reality, information in biological datasets is often incomplete as a result of untrustworthy samples and missing values. The reliability of selection methods may therefore be questioned.  Method: Information loss is incorporated into a perturbation scheme, testing which features are stable under it. This method is applied to data analysis by unsupervised feature filtering (UFF). The latter has been shown to be a very successful method in analysis of gene-expression data.  Results: We find that the UFF quality degrades smoothly with information loss. It remains successful even under substantial damage. Our method allows for selection of a best imputation method on a dataset treated by UFF. More importantly, scoring features according to their stability under information loss is shown to be correlated with biological importance in cancer studies. This scoring may lead to novel biological insights.  Contact: royke@cs.huji.ac.il  Supplementary information and code availability: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btm528},
author = {Varshavsky, Roy and Gottlieb, Assaf and Horn, David and Linial, Michal},
doi = {10.1093/bioinformatics/btm528},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky et al. - 2007 - Unsupervised feature selection under perturbations Meeting the challenges of biological data.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {24},
pages = {3343--3349},
title = {{Unsupervised feature selection under perturbations: Meeting the challenges of biological data}},
volume = {23},
year = {2007}
}
@article{Hursky,
author = {Hursky, Paul and Porter, Michael B},
file = {:home/chiroptera/Dropbox/mendeley/Hursky, Porter - Unknown - Accelerating underwater acoustic propagation modeling using general purpose graphic processing units.pdf:pdf},
keywords = {Split-step Fourier parabolic equation, high-perfor,general purpose graphic processing,high-,performance computing,split-step fourier parabolic equation,unit},
title = {{Accelerating underwater acoustic propagation modeling using general purpose graphic processing units}}
}
@article{Fraire2013,
author = {Fraire, Juan a. and Ferreyra, Alejandro and Marques, Carlos},
doi = {10.1109/TLA.2013.6502816},
file = {:home/chiroptera/Dropbox/mendeley/Fraire, Ferreyra, Marques - 2013 - OpenCL Overview, Implementation, and Performance Comparison.pdf:pdf},
issn = {1548-0992},
journal = {IEEE Latin America Transactions},
keywords = {heterogeneous},
number = {1},
pages = {274--280},
title = {{OpenCL Overview, Implementation, and Performance Comparison}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6502816},
volume = {11},
year = {2013}
}
@article{Fred2009b,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt. 4 - Clustering Ensemble Methods.pdf:pdf},
journal = {Methods},
number = {April},
pages = {1--27},
title = {{Tutorial Pt. 4 - Clustering Ensemble Methods}},
year = {2009}
}
@article{Malakar2013,
author = {Malakar, Ranajoy and Vydyanathan, Naga},
doi = {10.1109/ParCompTech.2013.6621392},
file = {:home/chiroptera/Dropbox/mendeley/Malakar, Vydyanathan - 2013 - A CUDA-enabled hadoop cluster for fast distributed image processing.pdf:pdf},
isbn = {9781479915910},
journal = {2013 National Conference on Parallel Computing Technologies, PARCOMPTECH 2013},
keywords = {CUDA,GPGPU,Hadoop,Map-reduce},
title = {{A CUDA-enabled hadoop cluster for fast distributed image processing}},
year = {2013}
}
@article{Che2008,
abstract = {Graphics processors (GPUs) provide a vast number of simple, data-parallel, deeply multithreaded cores and high memory bandwidths. GPU architectures are becoming increasingly programmable, offering the potential for dramatic speedups for a variety of general-purpose applications compared to contemporary general-purpose processors (CPUs). This paper uses NVIDIA's C-like CUDA language and an engineering sample of their recently introduced GTX 260 GPU to explore the effectiveness of GPUs for a variety of application types, and describes some specific coding idioms that improve their performance on the GPU. GPU performance is compared to both single-core and multicore CPU performance, with multicore CPU implementations written using OpenMP. The paper also discusses advantages and inefficiencies of the CUDA programming model and some desirable features that might allow for greater ease of use and also more readily support a larger body of applications. © 2008 Elsevier Inc. All rights reserved.},
author = {Che, Shuai and Boyer, Michael and Meng, Jiayuan and Tarjan, David and Sheaffer, Jeremy W. and Skadron, Kevin},
doi = {10.1016/j.jpdc.2008.05.014},
file = {:home/chiroptera/Dropbox/mendeley/Che et al. - 2008 - A performance study of general-purpose applications on graphics processors using CUDA.pdf:pdf},
isbn = {0743-7315},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {CUDA,GPGPU,GPU,Graphics processors,Heterogeneous computing organizations,Manycore,Multicore,OpenMP,Parallel programming},
number = {10},
pages = {1370--1380},
title = {{A performance study of general-purpose applications on graphics processors using CUDA}},
volume = {68},
year = {2008}
}
@article{Ji2011,
abstract = {Modern General Purpose Graphics Processing Units (GPGPUs) provide high degrees of parallelism in computation and memory access, making them suitable for data parallel applications such as those using the elastic MapReduce model. Yet designing a MapReduce framework for GPUs faces significant challenges brought by their multi-level memory hierarchy. Due to the absence of atomic operations in the earlier generations of GPUs, existing GPU MapReduce frameworks have problems in handling input/output data with varied or unpredictable sizes. Also, existing frameworks utilize mostly a single level of memory, $\backslash$emph\{i.e.\}, the relatively spacious yet slow global memory. In this work, we attempt to explore the potential benefit of enabling a GPU MapReduce framework to use multiple levels of the GPU memory hierarchy. We propose a novel GPU data staging scheme for MapReduce workloads, tailored toward the GPU memory hierarchy. Centering around the efficient utilization of the fast but very small shared memory, we designed and implemented a GPU MapReduce framework, whose key techniques include (1) shared memory staging area management, (2) thread-role partitioning, and (3) intra-block thread synchronization. We carried out evaluation with five popular MapReduce workloads and studied their performance under different GPU memory usage choices. Our results reveal that exploiting GPU shared memory is highly promising for the Map phase (with an average 2.85x speedup over using global memory only), while in the Reduce phase the benefit of using shared memory is much less pronounced, due to the high input-to-output ratio. In addition, when compared to Mars, an existing GPU MapReduce framework, our system is shown to bring a significant speedup in Map/Reduce phases.},
author = {Ji, Feng and Ma, Xiaosong},
doi = {10.1109/IPDPS.2011.80},
file = {:home/chiroptera/Dropbox/mendeley/Ji, Ma - 2011 - Using Shared Memory to Accelerate MapReduce on Graphics Processing Units.pdf:pdf},
isbn = {978-0-7695-4385-7},
issn = {1530-2075},
journal = {2011 IEEE International Parallel \& Distributed Processing Symposium},
keywords = {MapReduce,gpu},
mendeley-tags = {MapReduce,gpu},
pages = {805--816},
title = {{Using Shared Memory to Accelerate MapReduce on Graphics Processing Units}},
year = {2011}
}
