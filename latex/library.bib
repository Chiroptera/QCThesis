Automatically generated by Mendeley Desktop 1.13.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Jain2010,
abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Jain, Anil K},
doi = {10.1016/j.patrec.2009.09.011},
file = {:home/chiroptera/Dropbox/mendeley/Jain - 2010 - Data clustering 50 years beyond K-means.pdf:pdf},
isbn = {9781424417360},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Data clustering,Historical developments,King-Sun Fu prize,Perspectives on clustering,User's dilemma},
number = {8},
pages = {651--666},
publisher = {Elsevier B.V.},
title = {{Data clustering: 50 years beyond K-means}},
url = {http://dx.doi.org/10.1016/j.patrec.2009.09.011},
volume = {31},
year = {2010}
}
@article{Geras2011,
author = {Geras, Krzysztof Jerzy},
file = {:home/chiroptera/Dropbox/mendeley/Geras - 2011 - Prediction Markets for Machine Learning.pdf:pdf},
number = {248264},
title = {{Prediction Markets for Machine Learning}},
year = {2011}
}
@article{Chen2014,
abstract = {In this paper, we review the background and state-of-the-art of big data. We first introduce the general background of big data and review related technologies, such as could computing, Internet of Things, data centers, and Hadoop.We then focus on the four phases of the value chain of big data, i.e., data generation, data acquisition, data storage, and data analysis. For each phase, we introduce the general background, discuss the technical challenges, and review the latest advances. We finally examine the several representative applications of big data, including enterprise management, Internet of Things, online social networks, medial applications, collective intelligence, and smart grid. These discussions aimto provide a comprehensive overview and big-picture to readers of this exciting area. This survey is concluded with a discussion of open problems and future directions.},
author = {Chen, Min and Mao, Shiwen and Liu, Yunhao},
doi = {10.1007/s11036-013-0489-0},
file = {:home/chiroptera/Dropbox/mendeley/Chen, Mao, Liu - 2014 - Big data A survey.pdf:pdf},
isbn = {1383-469X},
issn = {1383469X},
journal = {Mobile Networks and Applications},
keywords = {Big data,Big data analysis,Cloud computing,Data center,Hadoop,Internet of things,Smart grid},
number = {2},
pages = {171--209},
title = {{Big data: A survey}},
volume = {19},
year = {2014}
}
@article{Meila2003,
abstract = {This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information (VI), measures the amount of information lost and gained in changing from clustering \$\{\backslash cal C\}\$ to clustering \$\{\backslash cal C\}'\$ . The criterion makes no assumptions about how the clusterings were generated and applies to both soft and hard clusterings. The basic properties of VI are presented and discussed from the point of view of comparing clusterings. In particular, the VI is positive, symmetric and obeys the triangle inequality. Thus, surprisingly enough, it is a true metric on the space of clusterings. Keywords: Clustering; Comparing partitions; Measures of agreement; Information theory; Mutual information},
author = {Meila, Marina},
doi = {10.1007/978-3-540-45167-9\_14},
file = {:home/chiroptera/Dropbox/mendeley/Meila - 2003 - Comparing clusterings by the variation of information.pdf:pdf},
isbn = {978-3-540-40720-1, 978-3-540-45167-9},
issn = {03029743},
journal = {Learning theory and Kernel machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003: proceedings},
keywords = {clustering,comparing partitions,information theory,measures of agreement,mutual information},
pages = {173},
title = {{Comparing clusterings by the variation of information}},
url = {http://books.google.com/books?hl=en\&amp;lr=\&amp;id=hk1dqsM0XF4C\&amp;oi=fnd\&amp;pg=PA173\&amp;dq=Comparing+Clusterings+by+the+Variation+of+Information\&amp;ots=7rcmrLpFV1\&amp;sig=P-AXGQnlenPfAlSb3fdhphYv6dI},
year = {2003}
}
@article{Zhang2008,
abstract = {The principle advantage and shortcoming of quantum clustering algorithm (QC) is analyzed. Based on its shortcomings, an improved algorithm - exponent distance-based quantum clustering algorithm (EQDC) is produced. It improved the iterative procedure of QC algorithm and used exponent distance formula to measure the distance between data points and the cluster centers. Experimental results demonstrate that the cluster accuracy of EDQC outperforms that of QC, and the exponent distance formula used in the clustering process performs better than the Euclidean distance in data preprocessing. What's more, the IRIS dataset can come to a satisfied result without preprocessing.},
author = {Zhang, Yao and Wang, Peng and Chen, Gao Yun and Chen, Dong Dong and Ding, Rui and Zhang, Yan},
doi = {10.1109/KAMW.2008.4810518},
file = {:home/chiroptera/Dropbox/mendeley/Zhang et al. - 2008 - Quantum clustering algorithm based on exponent measuring distance.pdf:pdf},
isbn = {9781424435296},
journal = {2008 IEEE International Symposium on Knowledge Acquisition and Modeling Workshop Proceedings, KAM 2008},
keywords = {Clustering accuracy,Data preprocessing,Exponent distance-based quantum clustering algorit,Measuring formula,Quantum clustering algorithm,Quantum potential},
number = {1},
pages = {436--439},
title = {{Quantum clustering algorithm based on exponent measuring distance}},
year = {2008}
}
@article{Blekas2009,
author = {Blekas, Konstantinos and Christodoulidou, K and Lagaris, I E},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Christodoulidou, Lagaris - 2009 - LNCS 5769 - Newtonian Spectral Clustering.pdf:pdf},
pages = {145--154},
title = {{LNCS 5769 - Newtonian Spectral Clustering}},
year = {2009}
}
@article{Rajaraman2011,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
author = {Rajaraman, Anand and Ullman, Jeffrey D},
doi = {10.1017/CBO9781139058452},
file = {:home/chiroptera/Dropbox/mendeley/Rajaraman, Ullman - 2011 - Mining of Massive Datasets.pdf:pdf},
isbn = {9781139058452},
issn = {01420615},
journal = {Lecture Notes for Stanford CS345A Web Mining},
pages = {328},
title = {{Mining of Massive Datasets}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139058452},
volume = {67},
year = {2011}
}
@article{Brassard,
author = {Brassard, Gilles and Centre-ville, Succursale},
file = {:home/chiroptera/Dropbox/mendeley/Brassard, Centre-ville - Unknown - Quantum Clustering Algorithms(2).pdf:pdf;:home/chiroptera/Dropbox/mendeley/Brassard, Centre-ville - Unknown - Quantum Clustering Algorithms.pdf:pdf},
title = {{Quantum Clustering Algorithms}}
}
@misc{Casper,
abstract = {A Quantum-Modeled K-Means clustering algorithm for multi- band image segmentation is explored and evaluated. Data sets of interest include multi-band RGB imagery, which subsequent to classification is analyzed and assessed for accuracy. Results demonstrate that under specific conditions the algorithm exhibits improved accuracy, when compared to its classical counterpart. Categories},
author = {Casper, Ellis and Hung, Chih-Cheng and Jung, Edward and Yang, Ming},
file = {:home/chiroptera/Dropbox/mendeley/Casper et al. - Unknown - A Quantum-Modeled K-Means Clustering Algorithm for Multi-band Image Segmentation.pdf:pdf},
keywords = {K-Means,QK-Means,Quantum computing,image,image segmentation,k-means,qk-means,quantum computing,qubit},
mendeley-tags = {image segmentation,k-means,qk-means,qubit},
number = {3},
pages = {158--163},
title = {{A Quantum-Modeled K-Means Clustering Algorithm for Multi-band Image Segmentation}},
url = {http://delivery.acm.org/10.1145/2410000/2401639/p158-casper.pdf?ip=193.136.132.10\&id=2401639\&acc=ACTIVE SERVICE\&key=2E5699D25B4FE09E.F7A57B2C5B227641.4D4702B0C3E38B35.4D4702B0C3E38B35\&CFID=476955365\&CFTOKEN=55494231\&\_\_acm\_\_=1423057410\_0d77d9b5028cb3},
urldate = {2015-02-04},
volume = {1}
}
@article{Lloyd2013,
abstract = {Machine-learning tasks frequently involve problems of manipulating and classi- fying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number ofvectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervisedandunsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1307.0411v2},
author = {Lloyd, Seth and Mohseni, Masoud and Rebentrost, Patrick},
eprint = {arXiv:1307.0411v2},
file = {:home/chiroptera/Dropbox/mendeley/Lloyd, Mohseni, Rebentrost - 2013 - Quantum algorithms for supervised and unsupervised machine learning.pdf:pdf},
keywords = {Machine Learning,quantum computers},
mendeley-tags = {quantum computers},
pages = {1--11},
title = {{Quantum algorithms for supervised and unsupervised machine learning}},
url = {http://arxiv.org/pdf/1307.0411.pdf$\backslash$npapers2://publication/uuid/2BDCBB85-812C-46E4-B506-73B9A41EBBC1},
year = {2013}
}
@article{Wittek2013a,
abstract = {Clustering methods in machine learning may benefit from borrowing metaphors from physics. Dynamic quantum clustering associates a Gaussian wave packet with the multidimensional data points and regards them as eigenfunctions of the Schr??dinger equation. The clustering structure emerges by letting the system evolve and the visual nature of the algorithm has been shown to be useful in a range of applications. Furthermore, the method only uses matrix operations, which readily lend themselves to parallelization. In this paper, we develop an implementation on graphics hardware and investigate how this approach can accelerate the computations. We achieve a speedup of up to two magnitudes over a multicore CPU implementation, which proves that quantum-like methods and acceleration by graphics processing units have a great relevance to machine learning. ?? 2012 Elsevier Inc.},
author = {Wittek, Peter},
doi = {10.1016/j.jcp.2012.08.048},
file = {:home/chiroptera/Dropbox/mendeley/Wittek - 2013 - High-performance dynamic quantum clustering on graphics processors.pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
keywords = {Clustering,GPU computing,Quantum-like learning,Time-dependent schr??dinger equation},
month = jan,
number = {1},
pages = {262--271},
title = {{High-performance dynamic quantum clustering on graphics processors}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0021999112005165},
volume = {233},
year = {2013}
}
@book{Pedersen2008,
abstract = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
author = {Pedersen, Michael Syskind and Baxter, Bill and Templeton, Brian and Rish\o j, Christian and Theobald, Douglas L and Hoegh-rasmussen, Esben and Casteel, Glynne and Gao, Jun Bin and Dedecius, Kamil and Strim, Korbinian and Christiansen, Lars and Hansen, Lars Kai and Wilkinson, Leland and He, Liguo and Bar, Miguel and Winther, Ole and Sakov, Pavel and Hattinger, Stephan and Petersen, Kaare Breandt and Rish\o j, Christian},
booktitle = {Matrix},
doi = {10.1111/j.1365-294X.2006.03161.x},
editor = {Bloom, B R},
institution = {Technical University of Denmark},
issn = {09621083},
keywords = {acknowledgements,bill baxter,brian templeton,christian,christian rish\o j,contributions,derivative,derivative inverse matrix,determinant,differentiate a matrix,matrix algebra,matrix identities,matrix relations,suggestions,thank following,we would like},
number = {1},
pages = {1--71},
pmid = {17284204},
publisher = {Citeseer},
title = {{The Matrix Cookbook}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.3165\&amp;rep=rep1\&amp;type=pdf},
volume = {M},
year = {2008}
}
@article{Nvidia2009,
abstract = {The lethal outcome of high-dose pulmonary virus infection is thought to reflect high-level, sustained virus replication and associated lung inflammation prior to development of an adaptive immune response. Herein, we demonstrate that the outcome of lethal/sublethal influenza infection instead correlates with the initial virus replication tempo. Furthermore, the magnitude of early lung antiviral CD8+ T cell responses varies inversely with inoculum dose and is controlled by lymph-node-resident dendritic cells (LNDC) through IL-12p40-regulated FasL-dependent T cell apoptosis. These results suggest that the inoculum dose and replication rate of a pathogen entering the respiratory tract may regulate the strength of the adaptive immune response, and the subsequent outcome of infection and that LNDC may serve as regulators (gatekeepers) in the development of CD8+ T cell responses.},
author = {Nvidia, Whitepaper and Generation, Next and Compute, Cuda},
doi = {10.1016/j.immuni.2005.11.006},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia, Generation, Compute - 2009 - Whitepaper NVIDIA’s Next Generation CUDA Compute Architecture.pdf:pdf},
issn = {10747613},
journal = {ReVision},
number = {6},
pages = {1--22},
pmid = {16356862},
title = {{Whitepaper NVIDIA’s Next Generation CUDA Compute Architecture}},
url = {http://www.nvidia.com/content/PDF/fermi\_white\_papers/NVIDIA\_Fermi\_Compute\_Architecture\_Whitepaper.pdf},
volume = {23},
year = {2009}
}
@article{Shell2008,
abstract = {Despite growing evidence that increased brain-derived neurotrophic factor (BDNF) and hippocampal adult neurogenesis are necessary for the behavioral actions of antidepressants in rodents, the cellular mechanisms involved in these effects are still unknown. Li et al. in this issue of Neuron demonstrate that the presence of TrkB, the high-affinity receptor for BDNF, in hippocampal neural progenitor cells is required for the neurogenic and behavioral actions of antidepressant treatments.},
author = {Shell, M.},
doi = {10.1016/j.neuron.2008.07.028},
issn = {1097-4199},
journal = {Neuron},
keywords = {Adult Stem Cells,Adult Stem Cells: drug effects,Animals,Antidepressive Agents,Antidepressive Agents: pharmacology,Cell Proliferation,Cell Proliferation: drug effects,Mice,Neurons,Neurons: drug effects,Receptor,trkB,trkB: metabolism},
number = {3},
pages = {349--51},
pmid = {18701059},
title = {{How to Use the IEEEtran L TEX Class}},
url = {http://www.cs.northwestern.edu/~sle841/papers/Arch\_TVCG/misc/tex files/original\_tvcg\_files/IEEEtran\_HOWTO.pdf$\backslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/18701059},
volume = {59},
year = {2008}
}
@misc{Weinstein,
author = {Weinstein, Marvin},
title = {{DQClib: A Maple package implementing Dynamic Quantum Clustering (DQC)}},
url = {http://www.slac.stanford.edu/~niv/index\_files/DQCOverview1.html}
}
@article{Shuai2006,
abstract = {This paper presents a generalized quantum particle model to greatly quicken and improve data clustering. The proposed model uses the random dynamics and quantum entanglement of quantum particles on a particle array. In comparison with classical nonquantum methods, the quantum particle model not only clusters much faster, but also has better clustering quality for multi-shape multi-distribution high-dimensional large-scale data sets with noise. The simulations and comparisons show the effectiveness of the quantum particle model},
author = {Shuai, Dianxun and Lu, Cunpai and Zhang, Bin},
doi = {10.1109/COMPSAC.2006.131},
file = {:home/chiroptera/Dropbox/mendeley/Shuai, Lu, Zhang - 2006 - Entanglement partitioning of quantum particles for data clustering.pdf:pdf},
isbn = {0769526551},
issn = {07303157},
journal = {Proceedings - International Computer Software and Applications Conference},
number = {2},
pages = {285--290},
title = {{Entanglement partitioning of quantum particles for data clustering}},
volume = {2},
year = {2006}
}
@article{Raymond2013,
author = {Raymond, The and Sackler, Beverly},
file = {:home/chiroptera/Dropbox/mendeley/Raymond, Sackler - 2013 - Quantum Clustering of Large Data Sets.pdf:pdf},
title = {{Quantum Clustering of Large Data Sets}},
year = {2013}
}
@article{Wiebe2014,
abstract = {We present several quantum algorithms for performing nearest-neighbor learning. At the core of our algorithms are fast and coherent quantum methods for computing distance metrics such as the inner product and Euclidean distance. We prove upper bounds on the number of queries to the input data required to compute these metrics. In the worst case, our quantum algorithms lead to polynomial reductions in query complexity relative to the corresponding classical algorithm. In certain cases, we show exponential or even super-exponential reductions over the classical analog. We study the performance of our quantum nearest-neighbor algorithms on several real-world binary classification tasks and find that the classification accuracy is competitive with classical methods.},
archivePrefix = {arXiv},
arxivId = {1401.2142},
author = {Wiebe, Nathan and Kapoor, Ashish and Svore, Krysta},
eprint = {1401.2142},
file = {:home/chiroptera/Dropbox/mendeley/Wiebe, Kapoor, Svore - 2014 - Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning.pdf:pdf},
pages = {31},
title = {{Quantum Algorithms for Nearest-Neighbor Methods for Supervised and Unsupervised Learning}},
url = {http://arxiv.org/abs/1401.2142},
year = {2014}
}
@article{Winterstein1997,
author = {Winterstein, Felix and Bayliss, Samuel and Constantinides, Geoge a.},
file = {:home/chiroptera/Dropbox/mendeley/Winterstein, Bayliss, Constantinides - 1997 - FPGA-Based K-Means Clustering Using Tree-Based Data Structures.pdf:pdf},
pages = {1450--1455},
title = {{FPGA-Based K-Means Clustering Using Tree-Based Data Structures}},
volume = {101},
year = {1997}
}
@article{Owens2008,
abstract = {The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in general-purpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.},
author = {Owens, Jd and Houston, M},
doi = {10.1109/JPROC.2008.917757},
file = {:home/chiroptera/Dropbox/mendeley/Owens, Houston - 2008 - GPU computing.pdf:pdf},
isbn = {0769527000},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {5},
pages = {879 -- 899},
pmid = {21776805},
title = {{GPU computing}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4490127},
volume = {96},
year = {2008}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, CJC Christopher J C},
doi = {10.1023/A:1009715923555},
editor = {Fayyad, Usama},
eprint = {1111.6189v1},
institution = {Bell Laboratories, Lucent Technologies},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
publisher = {Springer},
series = {NetGames '06},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf$\backslash$nhttp://link.springer.com/article/10.1023/A:1009715923555},
volume = {2},
year = {1998}
}
@article{Bai2009,
abstract = {K-means algorithm is one of the most famous unsupervised clustering algorithms. Many theoretical improvements for the performance of original algorithms have been put forward, while almost all of them are based on single instruction single data (SISD) architecture processors (GPUs), which partly ignored the inherent paralleled characteristic of the algorithms. In this paper, a novel single instruction multiple data (SIMD) architecture processors (GPUs) based k-means algorithm is proposed. In this algorithm, in order to accelerate compute-intensive portions of traditional k-means, both data objects assignment and k-centroids recalculation are offloaded to the GPU in parallel. We have implemented this GPU-based k-means on the newest generation GPU with compute unified device architecture(CUDA). The numerical experiments demonstrated that the speed of GPU-based k-means could reach as high as 40 times of the CPU-based k-means.},
annote = {Doesn't say which tools were used (C, Matlab, Fortran???).},
author = {Bai, Hong Tao and He, Li Li and Ouyang, Dan Tong and Li, Zhan Shan and Li, He},
doi = {10.1109/CSIE.2009.491},
file = {:home/chiroptera/Dropbox/mendeley/Bai et al. - 2009 - K-means on commodity GPUs with CUDA.pdf:pdf},
isbn = {9780769535074},
journal = {2009 WRI World Congress on Computer Science and Information Engineering, CSIE 2009},
pages = {651--655},
title = {{K-means on commodity GPUs with CUDA}},
volume = {3},
year = {2009}
}
@article{Topchy,
author = {Topchy, Alexander and Minaei-bidgoli, Behrouz and Jain, Anil K and Punch, William F and Lansing, E},
file = {:home/chiroptera/Dropbox/mendeley/Topchy et al. - Unknown - Adaptive Clustering Ensembles.pdf:pdf},
number = {i},
title = {{Adaptive Clustering Ensembles}}
}
@article{Wang2007,
abstract = {A new hybrid fuzzy clustering algorithm that incorporates the fuzzy c-means (FCM) into the quantum-behaved particle swarm optimization (QPSO) algorithm is proposed in this paper (QPSO+FCM). The QPSO has less parameters and higher convergent capability of the global optimizing than particle swarm optimization algorithm (PSO). So the iteration algorithm is replaced by the QPSO based on the gradient descent of FCM, which makes the algorithm have a strong global searching capacity and avoids the local minimum problems of FCM and in a large degree avoids depending on the initialization values. This paper also investigates the ability of FCM algorithm, PSO+FCM algorithm and GA+FCM algorithm with Iris testing data and Wine testing data. The simulation result proves that compared with other algorithms, the new algorithm not only has the favorable convergence but also has been obviously improved the clustering effect.},
author = {Wang, Hao Wang Hao and Yang, Shiqin Yang Shiqin and Xu, Wenbo Xu Wenbo and Sun, Jun Sun Jun},
doi = {10.1109/FSKD.2007.507},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2007 - Scalability of Hybrid Fuzzy C-Means Algorithm Based on Quantum-Behaved PSO.pdf:pdf},
isbn = {978-0-7695-2874-8},
journal = {Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)},
number = {Fskd},
title = {{Scalability of Hybrid Fuzzy C-Means Algorithm Based on Quantum-Behaved PSO}},
volume = {2},
year = {2007}
}
@article{Varshavsky2007,
author = {Varshavsky, Roy and Horn, David and Linial, Michal},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky, Horn, Linial - 2007 - Clustering Algorithms Optimizer A Framework for Large Datasets.pdf:pdf},
isbn = {3540720308},
issn = {03029743},
pages = {85--96},
title = {{Clustering Algorithms Optimizer : A Framework for Large Datasets}},
year = {2007}
}
@article{Blekas2007a,
abstract = {Given a data set, a dynamical procedure is applied to the data points in order to shrink and separate, possibly overlapping clusters. Namely, Newton's equations of motion are employed to concentrate the data points around their cluster centers, using an attractive potential, constructed specially for this purpose. During this process, important information is gathered concerning the spread of each cluster. In succession this information is used to create an objective function that maps each cluster to a local maximum. Global optimization is then used to retrieve the positions of the maxima that correspond to the locations of the cluster centers. Further refinement is achieved by applying the EM-algorithm to a Gaussian mixture model whose construction and initialization is based on the acquired information. To assess the effectiveness of our method, we have conducted experiments on a plethora of benchmark data sets. In addition we have compared its performance against four clustering techniques that are well established in the literature. [All rights reserved Elsevier]},
author = {Blekas, K. and Lagaris, I.E.},
doi = {10.1016/j.patcog.2006.07.012},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Lagaris - 2007 - Newtonian clustering An approach based on molecular dynamics and global optimization(2).pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {clustering,global optimization,molecular dynamics,order statistics},
pages = {1734--1744},
title = {{Newtonian clustering: An approach based on molecular dynamics and global optimization}},
volume = {40},
year = {2007}
}
@article{Fred2009,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt. 3 Validation of Custering Solutions.pdf:pdf},
number = {April},
pages = {1--15},
title = {{Tutorial Pt. 3: Validation of Custering Solutions}},
year = {2009}
}
@article{Horn2001,
abstract = {We discuss novel clustering methods that are based on mapping data points to a Hilbert space by means of a Gaussian kernel. The first method, support vector clustering (SVC), searches for the smallest sphere enclosing data images in Hilbert space. The second, quantum clustering (QC), searches for the minima of a potential function defined in such a Hilbert space. In SVC, the minimal sphere, when mapped back to data space, separates into several components, each enclosing a separate cluster of points. A soft margin constant helps in coping with outliers and overlapping clusters. In QC, minima of the potential define cluster centers, and equipotential surfaces are used to construct the clusters. In both methods, the width of the Gaussian kernel controls the scale at which the data are probed for cluster formations. We demonstrate the performance of the algorithms on several data sets.},
author = {Horn, David},
doi = {10.1016/S0378-4371(01)00442-3},
file = {:home/chiroptera/Dropbox/mendeley/Horn - 2001 - Clustering via Hilbert space.pdf:pdf},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
keywords = {Clustering,Hilbert space,Kernel methods,Scale-space clustering,Schr\"{o}dinger equation,Support vector clustering},
month = dec,
number = {1-4},
pages = {70--79},
title = {{Clustering via Hilbert space}},
url = {http://www.sciencedirect.com/science/article/pii/S0378437101004423},
volume = {302},
year = {2001}
}
@article{Manju2014,
abstract = {This paper makes an exhaustive survey of various applications of Quantum inspired computational intelligence (QCI) techniques proposed till date. Definition, categorization and motivation for QCI techniques are stated clearly. Major Drawbacks and challenges are discussed. The significance of this work is that it presents an overview on applications of QCI in solving various problems in engineering, which will be very much useful for researchers on Quantum computing in exploring this upcoming and young discipline.[PUBLICATION ABSTRACT]},
author = {Manju, a. and Nigam, M. J.},
doi = {10.1007/s10462-012-9330-6},
file = {:home/chiroptera/Dropbox/mendeley/Manju, Nigam - 2014 - Applications of quantum inspired computational intelligence A survey.pdf:pdf},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Computational intelligence,Quantum computing,Quantum mechanics},
pages = {79--156},
title = {{Applications of quantum inspired computational intelligence: A survey}},
volume = {42},
year = {2014}
}
@article{Horn2001b,
author = {Horn, David and Gottlieb, Assaf},
doi = {10.1103/PhysRevLett.88.018702},
file = {:home/chiroptera/Dropbox/mendeley//Horn, Gottlieb - 2001 - Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics.ps:ps;:home/chiroptera/Dropbox/mendeley/Horn, Gottlieb - 2001 - Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics.pdf:pdf},
issn = {0031-9007},
journal = {Physical Review Letters},
number = {1},
pages = {1--4},
title = {{Algorithm for Data Clustering in Pattern Recognition Problems Based on Quantum Mechanics}},
url = {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.88.018702},
volume = {88},
year = {2001}
}
@article{DiMarco2013b,
abstract = {Discover and quantify the performance gains of dynamic parallelism for clustering algorithms on GPUs. Dynamic parallelism effectively eliminates the superfluous back and forth communication between the GPU and CPU through nested kernel computations. The change in performance is measured using two well-known clustering algorithms that exhibit data dependencies: the K-means clustering and the hierarchical clustering. K-means has a sequential data dependence wherein iterations occur in a linear fashion, while the hierarchical clustering has a tree-like dependence that produces split tasks. Analyzing the performance of these data-dependent algorithms gives us a better understanding of the benefits or potential drawbacks of CUDA 5's new dynamic parallelism feature.},
author = {DiMarco, Jeffrey and Taufer, Michela},
doi = {10.1117/12.2018069},
file = {:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms.pdf:pdf},
isbn = {9780819495433},
issn = {0277786X},
journal = {Spie},
keywords = {0,cuda 5,divisive hierarchical clustering,k-means},
pages = {87520E},
title = {{Performance impact of dynamic parallelism on different clustering algorithms}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2018069},
year = {2013}
}
@article{DiMarco2013a,
author = {DiMarco, Jeffrey and Taufer, Michela},
doi = {10.1117/12.2018069},
file = {:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms(2).pdf:pdf},
isbn = {9780819495433},
issn = {0277786X},
journal = {Spie},
keywords = {0,cuda 5,divisive hierarchical clustering,k-means},
pages = {87520E},
title = {{Performance impact of dynamic parallelism on different clustering algorithms}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2018069},
year = {2013}
}
@misc{Horn2010,
author = {Horn, David and Aviv, Tel and Gottlieb, Assaf and HaSharon, Hod and Axel, Inon and Gan, Ramat},
file = {:home/chiroptera/Dropbox/mendeley/Horn et al. - 2010 - Method and Apparatus for Quantum Clustring.pdf:pdf},
number = {12},
title = {{Method and Apparatus for Quantum Clustring}},
volume = {2},
year = {2010}
}
@article{Liu2010,
abstract = {By reviewing the original INIQGA algorithm, an improved algorithm (IINIQGA) is put forward by revising the lookup table. In addition, By introducing the variable angle-distance rotation method into the update Q(t) procedure, a novel quantum-inspired evolutionary algorithm, QEA-VAR, was proposed. Compared with previous algorithms, our update Q(t) procedure is more simple and feasible. Finally, the corresponding experiments on the 0-1 knapsack problem were carried out, and the results show that our improvement is efficient, and comparing with IINIQGA, QEA, and CGA, QEA-VAR has a faster convergence and better profits than other algorithms.},
author = {Liu, Wenjie and Chen, Hanwu and Yan, Qiaoqiao and Liu, Zhihao and Xu, Juan and Zheng, Yu},
doi = {10.1109/CEC.2010.5586281},
file = {:home/chiroptera/Dropbox/mendeley/Liu et al. - 2010 - A novel quantum-inspired evolutionary algorithm based on variable angle-distance rotation.pdf:pdf},
isbn = {9781424469109},
journal = {2010 IEEE World Congress on Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation, CEC 2010},
keywords = {0/1 knapsack problem,quantum-inspired evolutionary algorithm,qubit,variable angle-distance,variable angle-distance rotation},
mendeley-tags = {qubit,variable angle-distance},
title = {{A novel quantum-inspired evolutionary algorithm based on variable angle-distance rotation}},
year = {2010}
}
@article{Fred2009c,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt.1 Basic Concepts of data clustering.pdf:pdf},
number = {April},
pages = {1--17},
title = {{Tutorial Pt.1: Basic Concepts of data clustering}},
year = {2009}
}
@article{Jarvis1973,
abstract = {A nonparametric clustering technique incorporating the concept of similarity based on the sharing of near neighbors is pre- sented. In addition to being an essentially paraliel approach, the com- putational elegance of the method is such that the scheme is applicable to a wide class of practical problems involving large sample size and high dimensionality. No attempt is made to show how a priori problem knowledge can be introduced into the procedure.},
author = {Jarvis, R a and Patrick, Edward a},
doi = {10.1109/T-C.1973.223640},
file = {:home/chiroptera/Dropbox/mendeley/Jarvis, Patrick - 1973 - Clustering Using a Similarity Measure Based on Shared Near Neighbors.pdf:pdf},
isbn = {00189340 (ISSN)},
issn = {0018-9340},
journal = {Ieee Transactions on Computers},
keywords = {Clustering,nonparametric,pattern recognition,shared near neighbors,similarity measure.},
number = {11},
pages = {1025--1034},
title = {{Clustering Using a Similarity Measure Based on Shared Near Neighbors}},
volume = {C-22},
year = {1973}
}
@article{Horn2001a,
abstract = {We propose a novel clusteringmethod that is an extension of ideas inher- ent to scale-space clustering and support-vector clustering. Like the lat- ter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scale- space probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schr¨ odinger equation of which the probability function is a solution. This Schr¨ odinger equation contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. Themethod has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schr¨ odinger potential to the locations of data points, we can apply this method to problems in high dimensions. 1},
author = {Horn, David and Gottlieb, Assaf},
file = {:home/chiroptera/Dropbox/mendeley/Horn, Gottlieb - 2001 - The Method of Quantum Clustering.pdf:pdf},
journal = {NIPS},
number = {1},
title = {{The Method of Quantum Clustering.}},
url = {http://www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/AA08.ps.gz},
year = {2001}
}
@article{Shuai2006a,
abstract = {This paper presents a new generalized quantum particle model for data self-organizing clustering. The stochas- tic motion and collision of quantum particles give rise to a stochastic process of quantum entanglement of particles. The stationary probability distribution over the configuration space of entangled particles results in the optimally clustering solution of the given data set. The quantum particle model has advantages in terms of the insensitivity to noise, the quality robustness to clustered data, the learning ability, and the suitability for high-dimensional multi-shape large-scale data sets. In comparison with the classical version of particle model and the cellular automata, the quantum particle mode has much faster speed and higher quality for clustering. The simulation and comparison show the effectiveness and good performance of the proposed quantum particle approach to data clustering.},
author = {Shuai, Dianxun and Zhang, Bin and Dong, Yumin},
file = {:home/chiroptera/Dropbox/mendeley/Shuai, Zhang, Dong - 2006 - Quantum Particles Model for Data Clustering in Enterprise Computing.pdf:pdf},
isbn = {1424401003},
number = {2},
pages = {4602--4607},
title = {{Quantum Particles Model for Data Clustering in Enterprise Computing}},
year = {2006}
}
@article{Mullner2013,
abstract = {The fastcluster package is a C++ library for hierarchical, agglomerative clustering. It provides a fast implementation of the most efficient, current algorithms when the input is a dissimilarity index. Moreover, it features memory-saving routines for hierarchical clustering of vector data. It improves both asymptotic time complexity (in most cases) and practical performance (in all cases) compared to the existing implementations in standard software: several R packages, MATLAB, Mathematica, Python with SciPy.},
archivePrefix = {arXiv},
arxivId = {1109.2378},
author = {M\"{u}llner, Daniel},
eprint = {1109.2378},
file = {:home/chiroptera/Dropbox/mendeley/M\"{u}llner - 2013 - fastcluster Fast Hierarchical , Agglomerative.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {age,agglomerative,algorithm,aver-,c,centroid,clustering,complete,hierarchical,linkage,mathematica,matlab,mcquitty,median,python,scipy,single,upgma,upgmc,ward,weighted,wpgma,wpgmc},
number = {9},
pages = {1--18},
title = {{fastcluster : Fast Hierarchical , Agglomerative}},
url = {http://www.jstatsoft.org/v53/i09},
volume = {53},
year = {2013}
}
@article{NVIDIACorporation2006,
author = {{NVIDIA Corporation}},
file = {:home/chiroptera/Dropbox/mendeley/NVIDIA Corporation - 2006 - CUDA Programming Model Overview.pdf:pdf},
title = {{CUDA Programming Model Overview}},
year = {2006}
}
@article{Fred2003,
author = {Fred, Ana L N},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2003 - A New Cluster Isolation Criterion Based on Dissimilarity Increments ´.pdf:pdf},
number = {8},
pages = {1--15},
title = {{A New Cluster Isolation Criterion Based on Dissimilarity Increments ´}},
volume = {25},
year = {2003}
}
@article{Li2007,
author = {Li, Zhi-hua and Wang, Shi-tong and Wuxi, Jiangsu},
file = {:home/chiroptera/Dropbox/mendeley/Li, Wang, Wuxi - 2007 - Quantum TheoryThe Unified Framework for FCM and QC Algorithm.pdf:pdf},
isbn = {1424410665},
keywords = {2-,algorithm,function,implemented by quantum clustering,interpretation,qc,quantum clustering,quantum potential,quantum theory,wave},
pages = {2--4},
title = {{Quantum Theory:The Unified Framework for FCM and QC Algorithm}},
year = {2007}
}
@article{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, ran- domized seeding technique, we obtain an algorithm that is $\Theta$(log k)-competitive with the optimal clustering. Prelim- inary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, D. and Arthur, D. and Vassilvitskii, S. and Vassilvitskii, S.},
doi = {10.1145/1283383.1283494},
file = {:home/chiroptera/Dropbox/mendeley/Arthur et al. - 2007 - k-means The advantages of careful seeding.pdf:pdf},
isbn = {978-0-898716-24-5},
journal = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
pages = {1027--1035},
title = {{k-means++: The advantages of careful seeding}},
url = {http://portal.acm.org/citation.cfm?id=1283494},
volume = {8},
year = {2007}
}
@inproceedings{Li2010,
abstract = {Based on the concepts and principles of quantum computing, a novel clustering algorithm, called a quantum-inspired immune clonal clustering algorithm based on watershed (QICW), is proposed to deal with the problem of image segmentation. In QICW, antibody is proliferated and divided into a set of subpopulation groups. Antibodies in a subpopulation group are represented by multi-state gene quantum bits. In the antibody's updating, the quantum mutation operator is applied to accelerate convergence. The quantum recombination realizes the information communication between the subpopulation groups so as to avoid premature convergences. In this paper, the segmentation problem is viewed as a combinatorial optimization problem, the original image is partitioned into small blocks by watershed algorithm, and the quantum-inspired immune clonal algorithm is used to search the optimal clustering centre, and make the sequence of maximum affinity function as clustering result, and finally obtain the segmentation result. Experimental results show that the proposed method is effective for texture image and SAR image segmentation, compared with the genetic clustering algorithm based on watershed (W-GAC), and the k-means algorithm based on watershed (W-KM).},
author = {Li, Yangyang and Wu, Nana and Ma, Jingjing and Jiao, Licheng},
booktitle = {IEEE Congress on Evolutionary Computation},
doi = {10.1109/CEC.2010.5586362},
file = {:home/chiroptera/Dropbox/mendeley/Li et al. - 2010 - Quantum-inspired immune clonal clustering algorithm based on watershed.pdf:pdf},
isbn = {978-1-4244-6909-3},
keywords = {Clustering algorithms,Error analysis,Feature extraction,Image segmentation,Partitioning algorithms,Radiative recombination,SAR image segmentation,Wavelet transforms,combinatorial mathematics,combinatorial optimization problem,genetic algorithms,genetic clustering algorithm,image segmentation,image texture,k-means algorithm,maximum affinity function,multistate gene quantum bits,pattern clustering,quantum computing,quantum mutation operator,quantum-inspired immune clonal clustering algorith,synthetic aperture radar,texture image,watershed algorithm},
month = jul,
pages = {1--7},
publisher = {IEEE},
shorttitle = {Evolutionary Computation (CEC), 2010 IEEE Congress},
title = {{Quantum-inspired immune clonal clustering algorithm based on watershed}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5586362},
year = {2010}
}
@article{Singh1977,
author = {Singh, Sarabjeet},
file = {:home/chiroptera/Dropbox/mendeley/Singh - 1977 - CUDA for GPGPU Applications – A Survey.pdf:pdf},
pages = {1--4},
title = {{CUDA for GPGPU Applications – A Survey}},
year = {1977}
}
@article{Solan2005,
abstract = {We address the problem, fundamental to linguistics, bioinformatics, and certain other disciplines, of using corpora of raw symbolic sequential data to infer underlying rules that govern their production. Given a corpus of strings (such as text, transcribed speech, chromosome or protein sequence data, sheet music, etc.), our unsupervised algorithm recursively distills from it hierarchically structured patterns. The adios (automatic distillation of structure) algorithm relies on a statistical method for pattern extraction and on structured generalization, two processes that have been implicated in language acquisition. It has been evaluated on artificial context-free grammars with thousands of rules, on natural languages as diverse as English and Chinese, and on protein data correlating sequence with function. This unsupervised algorithm is capable of learning complex syntax, generating grammatical novel sentences, and proving useful in other fields that call for structure discovery from raw data, such as bioinformatics.},
author = {Solan, Zach and Horn, David and Ruppin, Eytan and Edelman, Shimon},
doi = {10.1073/pnas.0409746102},
file = {:home/chiroptera/Dropbox/mendeley/Solan et al. - 2005 - Unsupervised learning of natural languages.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {33},
pages = {11629--11634},
pmid = {16087885},
title = {{Unsupervised learning of natural languages.}},
volume = {102},
year = {2005}
}
@article{Kumar2004,
author = {Kumar, Nimit and Behera, Laxmidhar},
doi = {10.1023/B:NEPL.0000039429.89321.07},
issn = {1370-4621},
journal = {Neural Processing Letters},
keywords = {clustering,quantum},
mendeley-tags = {clustering,quantum},
month = aug,
number = {1},
pages = {11--22},
title = {{Visual–Motor Coordination Using a Quantum Clustering Based Neural Control Scheme}},
url = {http://link.springer.com/10.1023/B:NEPL.0000039429.89321.07},
volume = {20},
year = {2004}
}
@article{Weinstein2009b,
author = {Weinstein, Marvin and Horn, David},
doi = {10.1103/PhysRevE.80.066117},
file = {:home/chiroptera/Dropbox/mendeley//Weinstein, Horn - 2009 - Dynamic quantum clustering A method for visual exploration of structures in data.pdf:pdf},
issn = {1539-3755},
journal = {Physical Review E},
keywords = {clustering,quantum},
mendeley-tags = {clustering,quantum},
month = dec,
number = {6},
pages = {066117},
title = {{Dynamic quantum clustering: A method for visual exploration of structures in data}},
url = {http://link.aps.org/doi/10.1103/PhysRevE.80.066117},
volume = {80},
year = {2009}
}
@article{Harvard2005,
author = {Harvard},
file = {:home/chiroptera/Dropbox/mendeley/Harvard - 2005 - The Assignment Problem and the Hungarian Method.pdf:pdf},
journal = {Introduction to Linear Algebra and Multivariable Calculus},
title = {{The Assignment Problem and the Hungarian Method}},
year = {2005}
}
@article{Bennett,
author = {Bennett, Charles},
file = {:home/chiroptera/Dropbox/mendeley/Bennett - Unknown - Is quantum search practical.pdf:pdf},
pages = {22--30},
title = {{Is quantum search practical?}}
}
@article{Li2008,
abstract = {The enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum random walk (QRW) with the problem of data clustering, and develop two clustering algorithms based on the one dimensional QRW. Then, the probability distributions on the positions induced by QRW in these algorithms are investigated, which also indicates the possibility of obtaining better results. Consequently, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms are of fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.},
archivePrefix = {arXiv},
arxivId = {0812.1357},
author = {Li, Qiang and He, Yan and Jiang, Jing-ping},
eprint = {0812.1357},
file = {:home/chiroptera/Dropbox/mendeley/Li, He, Jiang - 2008 - A Novel Clustering Algorithm Based on Quantum Random Walk.pdf:pdf},
keywords = {data clustering,quantum compu-,quantum game,tation,unsupervised learning},
pages = {14},
title = {{A Novel Clustering Algorithm Based on Quantum Random Walk}},
url = {http://arxiv.org/abs/0812.1357},
year = {2008}
}
@article{Pennycook2013,
abstract = {This paper reports on the development of an MPI/OpenCL implementation of LU, an application-level benchmark from the NAS Parallel Benchmark Suite. An account of the design decisions addressed during the development of this code is presented, demonstrating the importance of memory arrangement and work-item/work-group distribution strategies when applications are deployed on different device types. The resulting platform-agnostic, single source application is benchmarked on a number of different architectures, and is shown to be 1.3-1.5× slower than native FORTRAN 77 or CUDA implementations on a single node and 1.3-3.1× slower on multiple nodes. We also explore the potential performance gains of OpenCL's device fissioning capability, demonstrating up to a 3× speed-up over our original OpenCL implementation. © 2013 Elsevier Ltd. All rights reserved.},
author = {Pennycook, S. J. and Hammond, S. D. and Wright, S. a. and Herdman, J. a. and Miller, I. and Jarvis, S. a.},
doi = {10.1016/j.jpdc.2012.07.005},
file = {:home/chiroptera/Dropbox/mendeley/Pennycook et al. - 2013 - An investigation of the performance portability of OpenCL.pdf:pdf},
isbn = {Pennycook, S.J., Hammond, S.D., Wright, S.A., Herdman, J.A., Miller, I. and Jarvis, S.A. (2012) An Investigation of the Performance Portability of OpenCL. Journal of Parallel and Distributed Computing. ISSN 0743-7315 (In Press)},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {GPU computing,High performance computing,Many-core computing,OpenCL,Optimisation},
number = {11},
pages = {1439--1450},
publisher = {Elsevier Inc.},
title = {{An investigation of the performance portability of OpenCL}},
url = {http://dx.doi.org/10.1016/j.jpdc.2012.07.005},
volume = {73},
year = {2013}
}
@article{Varshavsky2005,
author = {Varshavsky, Roy and Linial, Michal and Horn, David},
doi = {10.1007/11576259\_18},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky, Linial, Horn - 2005 - COMPACT A Comparative Package for Clustering Assessment.pdf:pdf},
isbn = {3540297707},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {159--167},
title = {{COMPACT: A Comparative Package for Clustering Assessment}},
volume = {3759 LNCS},
year = {2005}
}
@article{Fred2009a,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt.2 Clustering Algorithms.pdf:pdf},
number = {April},
pages = {1--42},
title = {{Tutorial Pt.2: Clustering Algorithms}},
year = {2009}
}
@article{Rajaraman2011a,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
author = {Rajaraman, Anand and Ullman, Jeffrey D},
doi = {10.1017/CBO9781139058452},
file = {:home/chiroptera/Dropbox/mendeley/Rajaraman, Ullman - 2011 - Mining of Massive Datasets(2).pdf:pdf},
isbn = {9781139058452},
issn = {01420615},
journal = {Lecture Notes for Stanford CS345A Web Mining},
pages = {328},
title = {{Mining of Massive Datasets}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139058452},
volume = {67},
year = {2011}
}
@article{DiMarco2013,
author = {DiMarco, Jeffrey and Taufer, Michela},
doi = {10.1117/12.2018069},
file = {:home/chiroptera/Dropbox/mendeley/DiMarco, Taufer - 2013 - Performance impact of dynamic parallelism on different clustering algorithms(3).pdf:pdf},
isbn = {9780819495433},
issn = {0277786X},
journal = {Spie},
keywords = {0,cuda,cuda 5,divisive hierarchical clustering,k-means},
mendeley-tags = {cuda,k-means},
pages = {87520E},
title = {{Performance impact of dynamic parallelism on different clustering algorithms}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2018069},
year = {2013}
}
@article{Cs2003,
author = {Cs, Sael Lee and Biology, Computational},
file = {:home/chiroptera/Dropbox/mendeley/Cs, Biology - 2003 - Lecture 16 pca and svd.pdf:pdf},
title = {{Lecture 16: pca and svd}},
year = {2003}
}
@misc{Guerra2009,
abstract = {The integration of usable and flexible analysis support in modelling environments is a key success factor in Model-Driven Development. In this paradigm, models are the core asset from which code is automatically generated, and thus ensuring model correctness is a fundamental quality control activity. For this purpose, a common approach is to transform the system models into formal semantic domains for verification. However, if the analysis results are not shown in a proper way to the end-user (e.g. in terms of the original language) they may become useless. In this paper we present a novel DSVL called BaVeL that facilitates the flexible annotation of verification results obtained in semantic domains to different formats, including the context of the original language. BaVeL is used in combination with a consistency framework, providing support for all steps in a verification process: acquisition of additional input data, transformation of the system models into semantic domains, verification, and flexible annotation of analysis results. The approach has been validated analytically by the cognitive dimensions framework, and empirically by its implementation and application to several DSVLs. Here we present a case study of a notation in the area of Digital Libraries, where the analysis is performed by transformations into Petri nets and a process algebra. © 2008 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Guerra, Esther and de Lara, Juan and Malizia, Alessio and D\'{\i}az, Paloma},
booktitle = {Information and Software Technology},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {0402594v3},
file = {:home/chiroptera/Dropbox/mendeley/Guerra et al. - 2009 - Supporting user-oriented analysis for multi-view domain-specific visual languages.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
keywords = {Back-annotation,Consistency,Domain-specific visual languages,Formal methods,Model transformation,Modelling environments},
number = {4},
pages = {769--784},
primaryClass = {arXiv:cond-mat},
title = {{Supporting user-oriented analysis for multi-view domain-specific visual languages}},
volume = {51},
year = {2009}
}
@article{Karimi2010,
abstract = {CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is an open standard that can be used to program CPUs, GPUs, and other devices from different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL promises a portable language for GPU programming, its generality may entail a performance penalty. In this paper, we use complex, near-identical kernels from a Quantum Monte Carlo application to compare the performance of CUDA and OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel to an OpenCL kernel involves minimal modifications. Making such a kernel compile with ATI's build tools involves more modifications. Our performance tests measure and compare data transfer times to and from the GPU, kernel execution times, and end-to-end application execution times for both CUDA and OpenCL.},
archivePrefix = {arXiv},
arxivId = {1005.2581},
author = {Karimi, Kamran and Dickson, Neil G. and Hamze, Firas},
doi = {10.1109/ICPP.2011.45},
eprint = {1005.2581},
file = {:home/chiroptera/Dropbox/mendeley/Karimi, Dickson, Hamze - 2010 - A Performance Comparison of CUDA and OpenCL.pdf:pdf},
isbn = {978-1-4577-1336-1},
number = {1},
pages = {12},
title = {{A Performance Comparison of CUDA and OpenCL}},
url = {http://arxiv.org/abs/1005.2581},
year = {2010}
}
@article{Weinstein2009,
abstract = {A given set of data points in some feature space may be associated with a Schr\"{o}dinger equation whose potential is determined by the data. This is known to lead to good clustering solutions. Here we extend this approach into a full-fledged dynamical scheme using a time-dependent Schr\"{o}dinger equation. Moreover, we approximate this Hamiltonian formalism by a truncated calculation within a set of Gaussian wave functions (coherent states) centered around the original points. This allows for analytic evaluation of the time evolution of all such states opening up the possibility of exploration of relationships among data points through observation of varying dynamical distances among points and convergence of points into clusters. This formalism may be further supplemented by preprocessing such as dimensional reduction through singular-value decomposition or feature filtering.},
annote = {From Duplicate 2 (Dynamic quantum clustering: a method for visual exploration of structures in data - Weinstein, Marvin; Horn, David)},
archivePrefix = {arXiv},
arxivId = {arXiv:0908.2644v1},
author = {Weinstein, Marvin and Horn, David},
doi = {10.1103/PhysRevE.80.066117},
eprint = {arXiv:0908.2644v1},
file = {:home/chiroptera/Dropbox/mendeley//Weinstein, Horn - 2009 - Dynamic quantum clustering A method for visual exploration of structures in data.pdf:pdf},
issn = {1539-3755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
keywords = {clustering,quantum},
mendeley-tags = {clustering,quantum},
month = dec,
number = {6},
pages = {1--15},
title = {{Dynamic quantum clustering: a method for visual exploration of structures in data}},
url = {http://link.aps.org/doi/10.1103/PhysRevE.80.066117},
volume = {80},
year = {2009}
}
@article{Horn2003,
abstract = {MOTIVATION: This paper introduces the application of a novel clustering method to microarray expression data. Its first stage involves compression of dimensions that can be achieved by applying SVD to the gene-sample matrix in microarray problems. Thus the data (samples or genes) can be represented by vectors in a truncated space of low dimensionality, 4 and 5 in the examples studied here. We find it preferable to project all vectors onto the unit sphere before applying a clustering algorithm. The clustering algorithm used here is the quantum clustering method that has one free scale parameter. Although the method is not hierarchical, it can be modified to allow hierarchy in terms of this scale parameter. RESULTS: We apply our method to three data sets. The results are very promising. On cancer cell data we obtain a dendrogram that reflects correct groupings of cells. In an AML/ALL data set we obtain very good clustering of samples into four classes of the data. Finally, in clustering of genes in yeast cell cycle data we obtain four groups in a problem that is estimated to contain five families. AVAILABILITY: Software is available as Matlab programs at http://neuron.tau.ac.il/\~{}horn/QC.htm.},
author = {Horn, David and Axel, Inon},
doi = {10.1093/bioinformatics/btg053},
file = {:home/chiroptera/Dropbox/mendeley/Horn, Axel - 2003 - Novel clustering algorithm for microarray expression data in a truncated SVD space.ps:ps;:home/chiroptera/Dropbox/mendeley/Horn, Axel - 2003 - Novel clustering algorithm for microarray expression data in a truncated SVD space.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
pages = {1110--1115},
pmid = {12801871},
title = {{Novel clustering algorithm for microarray expression data in a truncated SVD space}},
volume = {19},
year = {2003}
}
@article{Woolley,
author = {Woolley, Cliff},
file = {:home/chiroptera/Dropbox/mendeley/Woolley - Unknown - CUDA Overview GPGPU Revolutionizes Computing.pdf:pdf},
title = {{CUDA Overview GPGPU Revolutionizes Computing}}
}
@book{Lanzagorta2008,
author = {Lanzagorta, Marco and Uhlmann, Jeffrey},
booktitle = {Synthesis Lectures on Quantum Computing},
doi = {10.2200/S00159ED1V01Y200810QMC002},
file = {:home/chiroptera/Dropbox/mendeley/Lanzagorta, Uhlmann - 2008 - Quantum Computer Science.pdf:pdf},
isbn = {9780511813870},
issn = {1945-9726},
pages = {1--124},
title = {{Quantum Computer Science}},
volume = {1},
year = {2008}
}
@article{Weinstein2009a,
abstract = {Last year, in 2008, I gave a talk titled \{$\backslash$it Quantum Calisthenics\}. This year I am going to tell you about how the work I described then has spun off into a most unlikely direction. What I am going to talk about is how one maps the problem of finding clusters in a given data set into a problem in quantum mechanics. I will then use the tricks I described to let quantum evolution lets the clusters come together on their own.},
archivePrefix = {arXiv},
arxivId = {0911.0462},
author = {Weinstein, Marvin},
eprint = {0911.0462},
file = {:home/chiroptera/Dropbox/mendeley/Weinstein - 2009 - Strange Bedfellows Quantum Mechanics and Data Mining.pdf:pdf},
pages = {11},
title = {{Strange Bedfellows: Quantum Mechanics and Data Mining}},
url = {http://arxiv.org/abs/0911.0462},
year = {2009}
}
@article{Wittek2013,
abstract = {Clustering methods in machine learning may benefit from borrowing metaphors from physics. Dynamic quantum clustering associates a Gaussian wave packet with the multidimensional data points and regards them as eigenfunctions of the Schr??dinger equation. The clustering structure emerges by letting the system evolve and the visual nature of the algorithm has been shown to be useful in a range of applications. Furthermore, the method only uses matrix operations, which readily lend themselves to parallelization. In this paper, we develop an implementation on graphics hardware and investigate how this approach can accelerate the computations. We achieve a speedup of up to two magnitudes over a multicore CPU implementation, which proves that quantum-like methods and acceleration by graphics processing units have a great relevance to machine learning. ?? 2012 Elsevier Inc.},
author = {Wittek, Peter},
doi = {10.1016/j.jcp.2012.08.048},
file = {:home/chiroptera/Dropbox/mendeley/Wittek - 2013 - High-performance dynamic quantum clustering on graphics processors(2).pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
keywords = {Clustering,GPU computing,Quantum-like learning,Time-dependent schr??dinger equation},
pages = {262--271},
publisher = {Elsevier Inc.},
title = {{High-performance dynamic quantum clustering on graphics processors}},
url = {http://dx.doi.org/10.1016/j.jcp.2012.08.048},
volume = {233},
year = {2013}
}
@article{Xiao2010,
abstract = {The number of clusters has to be known in advance for the conventional k-means clustering algorithm and moreover the clustering result is sensitive to the selection of the initial cluster centroids. This sensitivity may make the algorithm converge to the local optima. This paper proposes a quantum-inspired genetic algorithm for k-means clustering (KMQGA). In KMQGA, a Q-bit based representation is employed for exploration and exploitation in discrete 0-1 hyperspace using rotation operation of quantum gate as well as the typical genetic algorithm operations (selection, crossover and mutation) of Q-bits. Different from the typical quantum-inspired genetic algorithms (QGA), the length of a Q-bit in KMQGA is variable during evolution. Without knowing the exact number of clusters beforehand, KMQGA can obtain the optimal number of clusters as well as providing the optimal cluster centroids. Both the simulated datasets and the real datasets are used to validate KMQGA, respectively. The experimental results show that KMQGA is promising and effective. ?? 2009 Elsevier Ltd. All rights reserved.},
annote = {From Duplicate 2 (A quantum-inspired genetic algorithm for k-means clustering - Xiao, Jing; Yan, YuPing; Zhang, Jun; Tang, Yong)},
author = {Xiao, Jing and Yan, YuPing and Zhang, Jun and Tang, Yong},
doi = {10.1016/j.eswa.2009.12.017},
file = {:home/chiroptera/Dropbox/mendeley/Xiao et al. - 2010 - A quantum-inspired genetic algorithm for k-means clustering.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Genetic algorithms,Quantum-inspired genetic algorithms,k-means,k-means clustering,qubit},
mendeley-tags = {k-means,qubit},
pages = {4966--4973},
title = {{A quantum-inspired genetic algorithm for k-means clustering}},
url = {http://ac.els-cdn.com/S095741740901063X/1-s2.0-S095741740901063X-main.pdf?\_tid=f303a76c-ac71-11e4-be73-00000aacb35e\&acdnat=1423056793\_66291f279193fa69b86c93aecea405b0},
volume = {37},
year = {2010}
}
@article{Ghorpade2012,
abstract = {The future of computation is the Graphical Processing Unit, i.e. the GPU. The promise that the graphics cards have shown in the field of image processing and accelerated rendering of 3D scenes, and the computational capability that these GPUs possess, they are developing into great parallel computing units. It is quite simple to program a graphics processor to perform general parallel tasks. But after understanding the various architectural aspects of the graphics processor, it can be used to perform other taxing tasks as well. In this paper, we will show how CUDA can fully utilize the tremendous power of these GPUs. CUDA is NVIDIA’s parallel computing architecture. It enables dramatic increases in computing performance, by harnessing the power of the GPU. This paper talks about CUDA and its architecture. It takes us through a comparison of CUDA C/C++ with other parallel programming languages like OpenCL and DirectCompute. The paper also lists out the common myths about CUDA and how the future seems to be promising for CUDA.},
archivePrefix = {arXiv},
arxivId = {1202.4347},
author = {Ghorpade, Jayshree},
doi = {10.5121/acij.2012.3109},
eprint = {1202.4347},
file = {:home/chiroptera/Dropbox/mendeley/Ghorpade - 2012 - GPGPU Processing in CUDA Architecture.pdf:pdf},
issn = {2229726X},
journal = {Advanced Computing: An International Journal},
number = {1},
pages = {105--120},
title = {{GPGPU Processing in CUDA Architecture}},
volume = {3},
year = {2012}
}
@article{Aidos2012,
author = {Aidos, Helena and Fred, Ana},
doi = {10.1016/j.patcog.2011.12.009},
file = {:home/chiroptera/Dropbox/mendeley/Aidos, Fred - 2012 - Statistical modeling of dissimilarity increments for d-dimensional data Application in partitional clustering.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Dissimilarity increments,Gaussian mixture decomposition,Likelihood-ratio test,Minimum description length,Partitional clustering,dissimilarity increments,likelihood-ratio test,partitional clustering},
number = {9},
pages = {3061--3071},
publisher = {Elsevier},
title = {{Statistical modeling of dissimilarity increments for d-dimensional data Application in partitional clustering}},
url = {http://dx.doi.org/10.1016/j.patcog.2011.12.009},
volume = {45},
year = {2012}
}
@article{Aimeur2013,
abstract = {We show how the quantum paradigm can be used to speed up unsupervised learning algorithms. More precisely, we explain how it is possible to accelerate learning algorithms by quantizing some of their subroutines. Quantization refers to the process that partially or totally converts a classical algorithm to its quantum counterpart in order to improve performance. In particular, we give quantized versions of clustering via minimum spanning tree, divisive clustering and k-medians that are faster than their classical analogues. We also describe a distributed version of k-medians that allows the participants to save on the global communication cost of the protocol compared to the classical version. Finally, we design quantum algorithms for the construction of a neighbourhood graph, outlier detection as well as smart initialization of the cluster centres.},
author = {A\"{\i}meur, Esma and Brassard, Gilles and Gambs, S\'{e}bastien},
doi = {10.1007/s10994-012-5316-5},
file = {:home/chiroptera/Dropbox/mendeley/A\"{\i}meur, Brassard, Gambs - 2013 - Quantum speed-up for unsupervised learning.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Clustering,Grover's algorithm,Quantum information processing,Quantum learning,Unsupervised learning},
number = {February 2012},
pages = {261--287},
title = {{Quantum speed-up for unsupervised learning}},
volume = {90},
year = {2013}
}
@article{Asanovic2006,
abstract = {The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation. A multidisciplinary group of Berkeley researchers met nearly two years to discuss this change. Our view is that this evolutionary approach to parallel hardware and software may work from 2 or 8 processor systems, but is likely to face diminishing returns as 16 and 32 processor systems are realized, just as returns fell with greater instruction-level parallelism. We believe that much can be learned by examining the success of parallelism at the extremes of the computing spectrum, namely embedded computing and high performance computing. This led us to frame the parallel landscape with seven questions, and to recommend the following: The overarching goal should be to make it easy to write programs that execute efficiently on highly parallel computing systems The target should be 1000s of cores per chip, as these chips are built from processing elements that are the most efficient in MIPS (Million Instructions per Second) per watt, MIPS per area of silicon, and MIPS per development dollar. Instead of traditional benchmarks, use 13 "Dwarfs" to design and evaluate parallel programming models and architectures. (A dwarf is an algorithmic method that captures a pattern of computation and communication.) "Autotuners" should play a larger role than conventional compilers in translating parallel programs. To maximize programmer productivity, future programming models must be more human-centric than the conventional focus on hardware or applications. To be successful, programming models should be independent of the number of processors. To maximize application efficiency, programming models should support a wide range of data types and successful models of parallelism: task-level parallelism, word-level parallelism, and bit-level parallelism. Architects should not include features that significantly affect performance or energy if programmers cannot accurately measure their impact via performance counters and energy counters. Traditional operating systems will be deconstructed and operating system functionality will be orchestrated using libraries and virtual machines. To explore the design space rapidly, use system emulators based on Field Programmable Gate Arrays (FPGAs) that are highly scalable and low cost. Since real world applications are naturally parallel and hardware is naturally parallel, what we need is a programming model, system software, and a supporting architecture that are naturally parallel. Researchers have the rare opportunity to re-invent these cornerstones of computing, provided they simplify the efficient programming of highly parallel systems.},
author = {Asanovic, Krste and Catanzaro, Bryan Christopher and Patterson, David a and Yelick, Katherine a},
doi = {10.1145/1562764.1562783},
file = {:home/chiroptera/Dropbox/mendeley/Asanovic et al. - 2006 - The Landscape of Parallel Computing Research A View from Berkeley.pdf:pdf},
isbn = {UCB/EECS-2006-183},
issn = {00010782},
journal = {EECS Department University of California Berkeley Tech Rep UCBEECS2006183},
pages = {19},
pmid = {8429457},
title = {{The Landscape of Parallel Computing Research : A View from Berkeley}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.8705\&amp;rep=rep1\&amp;type=pdf},
volume = {18},
year = {2006}
}
@article{Neelima2010,
abstract = {With the growth of Graphics Processor (GPU) programmability and processing power, graphics hardware has become a compelling platform for computationally demanding tasks in a wide variety of application domains. This state of art paper gives the technical motivations that underlie GPU computing and describe the hardware and software developments that have led to the recent interest in this field.},
author = {Neelima, B. and Raghavendra, Prakash S.},
doi = {10.1109/ICIINFS.2010.5578685},
file = {:home/chiroptera/Dropbox/mendeley/Neelima, Raghavendra - 2010 - Recent trends in software and hardware for GPGPU computing A comprehensive survey.pdf:pdf},
isbn = {9781424466535},
journal = {2010 5th International Conference on Industrial and Information Systems, ICIIS 2010},
keywords = {GPU computing,Graphics Processor (GPU)},
pages = {319--324},
title = {{Recent trends in software and hardware for GPGPU computing: A comprehensive survey}},
year = {2010}
}
@article{Zechner2009,
abstract = {In this paper an optimized k-means implementation on the graphics processing unit (GPU) is presented. NVIDIApsilas compute unified device architecture (CUDA), available from the G80 GPU family onwards, is used as the programming environment. Emphasis is placed on optimizations directly targeted at this architecture to best exploit the computational capabilities available. Additionally drawbacks and limitations of previous related work, e.g. maximum instance, dimension and centroid count are addressed. The algorithm is realized in a hybrid manner, parallelizing distance calculations on the GPU while sequentially updating cluster centroids on the CPU based on the results from the GPU calculations. An empirical performance study on synthetic data is given, demonstrating a maximum 14times speed increase to a fully SIMD optimized CPU implementation.},
author = {Zechner, Mario and Granitzer, Michael},
doi = {10.1109/INTENSIVE.2009.19},
file = {:home/chiroptera/Dropbox/mendeley/Zechner, Granitzer - 2009 - Accelerating k-means on the graphics processor via CUDA.pdf:pdf},
isbn = {9780769535852},
journal = {Proceedings of the 1st International Conference on Intensive Applications and Services, INTENSIVE 2009},
pages = {7--15},
title = {{Accelerating k-means on the graphics processor via CUDA}},
year = {2009}
}
@article{Fred2005,
author = {Fred, Ana N L and Jain, Anil K},
file = {:home/chiroptera/Dropbox/mendeley/Fred, Jain - 2005 - Combining multiple clusterings using evidence accumulation.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {K-means,cluster fusion,cluster validity,combining clustering partitions,evidence accumulation,mutual information,robust clustering,single-link},
mendeley-tags = {K-means,cluster fusion,cluster validity,combining clustering partitions,evidence accumulation,mutual information,robust clustering,single-link},
number = {6},
pages = {835--850},
title = {{Combining multiple clusterings using evidence accumulation}},
volume = {27},
year = {2005}
}
@article{Casper2013,
abstract = {The ability to cluster data accurately is essential to applications such as image segmentation. Therefore, techniques that enhance accuracy are of keen interest. One such technique involves applying a quantum mechanical system model, such as that of the quantum bit, to generate probabilistic numerical output to be used as variable input for clustering algorithms. This work demonstrates that applying a quantum bit model to data clustering algorithms can increase clustering accuracy, as a result of simulating superposition as well as possessing both guaranteed and controllable convergence properties. For accuracy assessment purposes, four quantum-modeled clustering algorithms for multi-band image segmentation are explored and evaluated. The clustering algorithms of choice consist of quantum variants of K-Means, Fuzzy C-Means, New Weighted Fuzzy C- Means, and the Artificial Bee Colony. Data sets of interest include multi-band imagery, which subsequent to classification are analyzed and assessed for accuracy. Results demonstrate that these algorithms exhibit improved accuracy, when compared to classical counterparts. Moreover, solutions are enhanced via introduction of the quantum state machine, which provides random initial centroid and variable input values to the various clustering algorithms, and quantum operators, which bring about convergence and maximize local search space exploration. Typically, the algorithms have shown to produce better solutions. Keywords:},
author = {Casper, Ellis and Hung, Chih-cheng},
doi = {10.4156/pica.vol2.issue1.1},
file = {:home/chiroptera/Dropbox/mendeley/Casper, Hung - 2013 - Quantum Modeled Clustering Algorithms for Image Segmentation 1.pdf:pdf},
keywords = {artificial bee colony,clustering,clustering algorithms,fuzzy c-means,image segmentation,k-means,quantum computing,quantum mechanics,qubit,weighted fuzzy c-means},
mendeley-tags = {artificial bee colony,clustering,fuzzy c-means,image segmentation,k-means,quantum computing,quantum mechanics,qubit,weighted fuzzy c-means},
number = {March},
pages = {1--21},
title = {{Quantum Modeled Clustering Algorithms for Image Segmentation 1}},
volume = {2},
year = {2013}
}
@article{El-sherbiny,
abstract = {Quantum computing proved good results and performance when applied to solving optimization problems. This paper proposes a quantum crossover-based quantum genetic algorithm (QXQGA) for solving non-linear programming. Due to the significant role of mutation function on the QXQGA's quality, a number of quantum crossover and quantum mutation operators are presented for improving the capabilities of searching, overcoming premature convergence, and keeping diversity of population. For calibrating the QXQGA, the quantum crossover and mutation operators are evaluated using relative percentage deviation for selecting the best combination. In addition, a set of non-linear problems is used as benchmark functions to illustrate the effectiveness of optimizing the complexities with different dimensions, and the performance of the proposed QXQGA algorithm is compared with the quantum inspired evolutionary algorithm to demonstrate its superiority.},
author = {El-sherbiny, Mahmoud M},
file = {:home/chiroptera/Dropbox/mendeley/El-sherbiny - Unknown - Quantum Crossover Based Quantum Genetic Algorithm for Solving Non-linear Programming Quantum Rotational gate.pdf:pdf},
keywords = {-component,quantum},
title = {{Quantum Crossover Based Quantum Genetic Algorithm for Solving Non-linear Programming Quantum Rotational gate}}
}
@article{Han2000,
abstract = {This paper proposes a novel evolutionary computing method called a
genetic quantum algorithm (GQA). GQA is based on the concept and
principles of quantum computing such as qubits and superposition of
states. Instead of binary, numeric, or symbolic representation, by
adopting qubit chromosome as a representation GQA can represent a linear
superposition of solutions due to its probabilistic representation. As
genetic operators, quantum gates are employed for the search of the best
solution. Rapid convergence and good global search capability
characterize the performance of GQA. The effectiveness and the
applicability of GQA are demonstrated by experimental results on the
knapsack problem, which is a well-known combinatorial optimization
problem. The results show that GQA is superior to other genetic
algorithms using penalty functions, repair methods and decoders},
author = {Han, Kuk-Hyun Han Kuk-Hyun and Kim, Jong-Hwan Kim Jong-Hwan},
doi = {10.1109/CEC.2000.870809},
file = {:home/chiroptera/Dropbox/mendeley/Han, Kim - 2000 - Genetic quantum algorithm and its application to combinatorial optimization problem.pdf:pdf},
isbn = {0-7803-6375-2},
journal = {Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)},
title = {{Genetic quantum algorithm and its application to combinatorial
optimization problem}},
volume = {2},
year = {2000}
}
@article{Fred2001,
abstract = {Given an arbitrary data set, to which no particular paramet- rical, statistical or geometrical structure can be assumed, different clus- tering algorithms will in general produce different data partitions. In fact, several partitions can also be obtained by using a single clustering algo- rithm due to dependencies on initialization or the selection of the value of some design parameter. This paper addresses the problem of finding consistent clusters in data partitions, proposing the analysis of the most common associations performed in a majority voting scheme. Combina- tion of clustering results are performed by transforming data partitions into a co-association sample matrix, which maps coherent associations. This matrix is then used to extract the underlying consistent clusters. The proposed methodology is evaluated in the context of k-means clus- tering, a new clustering algorithm - voting-k-means, being presented. Examples, using both simulated and real data, show how this major- ity voting combination scheme simultaneously handles the problems of selecting the number of clusters, and dependency on initialization. Fur- thermore, resulting clusters are not constrained to be hyper-spherically shaped.},
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2001 - Finding consistent clusters in data partitions.pdf:pdf},
isbn = {3540422846},
journal = {Multiple classifier systems},
pages = {309--318},
title = {{Finding consistent clusters in data partitions}},
url = {http://link.springer.com/chapter/10.1007/3-540-48219-9\_31},
year = {2001}
}
@article{Fred2009b,
author = {Fred, Ana},
file = {:home/chiroptera/Dropbox/mendeley/Fred - 2009 - Tutorial Pt. 4 - Clustering Ensemble Methods.pdf:pdf},
journal = {Methods},
number = {April},
pages = {1--27},
title = {{Tutorial Pt. 4 - Clustering Ensemble Methods}},
year = {2009}
}
@article{Nvidia2014,
author = {Nvidia},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia - 2014 - Cuda c programming guide.pdf:pdf},
journal = {Programming Guides},
number = {August},
title = {{Cuda c programming guide}},
year = {2014}
}
@book{Wittek,
author = {Wittek, Peter},
file = {:home/chiroptera/Dropbox/mendeley/Wittek - Unknown - Quantum Machine Learning What Quantum Computing Means to Data Mining.pdf:pdf},
title = {{Quantum Machine Learning: What Quantum Computing Means to Data Mining}}
}
@article{Weinstein2013,
abstract = {How does one search for a needle in a multi-dimensional haystack without knowing what a needle is and without knowing if there is one in the haystack? This kind of problem requires a paradigm shift - away from hypothesis driven searches of the data - towards a methodology that lets the data speak for itself. Dynamic Quantum Clustering (DQC) is such a methodology. DQC is a powerful visual method that works with big, high-dimensional data. It exploits variations of the density of the data (in feature space) and unearths subsets of the data that exhibit correlations among all the measured variables. The outcome of a DQC analysis is a movie that shows how and why sets of data-points are eventually classified as members of simple clusters or as members of - what we call - extended structures. This allows DQC to be successfully used in a non-conventional exploratory mode where one searches data for unexpected information without the need to model the data. We show how this works for big, complex, real-world datasets that come from five distinct fields: i.e., x-ray nano-chemistry, condensed matter, biology, seismology and finance. These studies show how DQC excels at uncovering unexpected, small - but meaningful - subsets of the data that contain important information. We also establish an important new result: namely, that big, complex datasets often contain interesting structures that will be missed by many conventional clustering techniques. Experience shows that these structures appear frequently enough that it is crucial to know they can exist, and that when they do, they encode important hidden information. In short, we not only demonstrate that DQC can be flexibly applied to datasets that present significantly different challenges, we also show how a simple analysis can be used to look for the needle in the haystack, determine what it is, and find what this means.},
archivePrefix = {arXiv},
arxivId = {1310.2700},
author = {Weinstein, M and Meirer, F and Hume, A},
eprint = {1310.2700},
file = {:home/chiroptera/Dropbox/mendeley/Weinstein, Meirer, Hume - 2013 - Analyzing Big Data with Dynamic Quantum Clustering.pdf:pdf},
journal = {arXiv preprint arXiv: \ldots},
pages = {1--37},
title = {{Analyzing Big Data with Dynamic Quantum Clustering}},
url = {http://arxiv.org/abs/1310.2700 http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:No+Title\#0},
year = {2013}
}
@article{Strehl2000,
author = {Strehl, Alexander and Ghosh, Joydeep},
doi = {10.1162/153244303321897735},
file = {:home/chiroptera/Dropbox/mendeley/Strehl, Ghosh - 2000 - 10.1162153244303321897735.pdf:pdf},
issn = {0003-6951},
journal = {CrossRef Listing of Deleted DOIs},
keywords = {cluster analysis,clustering,consensus functions,ensemble,knowledge reuse,multi-learner systems,mutual information,partitioning,unsupervised learning},
pages = {583--617},
title = {10.1162/153244303321897735},
url = {http://dl.acm.org/citation.cfm?id=944919.944935},
volume = {1},
year = {2000}
}
@article{Xiao2008,
abstract = {The conventional k-means clustering algorithm must know the number of clusters in advance and the clustering result is sensitive to the selection of the initial cluster centroids. The sensitivity may make the algorithm converge to the local optima. This paper proposes an improved k-means clustering algorithm based on quantum-inspired genetic algorithm (KMQGA). In KMQGA, Q-bit based representation is employed for exploration and exploitation in discrete 0-1 hyperspace by using rotation operation of quantum gate as well as three genetic algorithm operations (selection, crossover and mutation) of Q-bit. Without knowing the exact number of clusters beforehand, the KMQGA can get the optimal number of clusters as well as providing the optimal cluster centroids after several iterations of the four operations (selection, crossover, mutation, and rotation). The simulated datasets and the real datasets are used to validate KMQGA and to compare KMQGA with an improved k-means clustering algorithm based on the famous variable string length genetic algorithm (KMVGA) respectively. The experimental results show that KMQGA is promising and the effectiveness and the search quality of KMQGA is better than those of KMVGA.},
author = {Xiao, Jing Xiao Jing and Yan, YuPing Yan YuPing and Lin, Ying Lin Ying and Yuan, Ling Yuan Ling and Zhang, Jun Zhang Jun},
journal = {2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)},
title = {{A Quantum-inspired Genetic Algorithm for data clustering}},
year = {2008}
}
@article{Blekas2007,
abstract = {Given a data set, a dynamical procedure is applied to the data points in order to shrink and separate, possibly overlapping clusters. Namely, Newton's equations of motion are employed to concentrate the data points around their cluster centers, using an attractive potential, constructed specially for this purpose. During this process, important information is gathered concerning the spread of each cluster. In succession this information is used to create an objective function that maps each cluster to a local maximum. Global optimization is then used to retrieve the positions of the maxima that correspond to the locations of the cluster centers. Further refinement is achieved by applying the EM-algorithm to a Gaussian mixture model whose construction and initialization is based on the acquired information. To assess the effectiveness of our method, we have conducted experiments on a plethora of benchmark data sets. In addition we have compared its performance against four clustering techniques that are well established in the literature. [All rights reserved Elsevier]},
author = {Blekas, K. and Lagaris, I.E.},
doi = {10.1016/j.patcog.2006.07.012},
file = {:home/chiroptera/Dropbox/mendeley/Blekas, Lagaris - 2007 - Newtonian clustering An approach based on molecular dynamics and global optimization.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {clustering,global optimization,molecular dynamics,order statistics},
pages = {1734--1744},
title = {{Newtonian clustering: An approach based on molecular dynamics and global optimization}},
volume = {40},
year = {2007}
}
@article{Evans,
author = {Evans, Michael R},
file = {:home/chiroptera/Dropbox/mendeley/Evans - Unknown - Spatial Big Data Case Studies on Volume , Velocity , and Variety What is Spatial Big Data.pdf:pdf},
pages = {1--16},
title = {{Spatial Big Data : Case Studies on Volume , Velocity , and Variety What is Spatial Big Data ?}}
}
@article{Rosenbaum2011,
abstract = {We consider a generalization of the standard oracle model in which the oracle acts on the target with a permutation which is selected according to internal random coins. We show new exponential quantum speedups which may be obtained over classical algorithms in this oracle model. Even stronger, we describe several problems which are impossible to solve classically but can be solved by a quantum algorithm using a single query; we show that such infinity-vs-one separations between classical and quantum query complexities can be constructed from any separation between classical and quantum query complexities (in the unbounded-error regime).   We also give conditions to determine when oracle problems---either in the standard model, or in any of the generalizations we consider---cannot be solved with success probability better than random guessing would achieve. In the oracle model with internal randomness where the goal is to gain any nonzero advantage over guessing, we prove (roughly speaking) that (k) quantum queries are equivalent in power to (2k) classical queries, thus extending results of Meyer and Pommersheim, and Montanaro, Nishimura and Raymond.},
archivePrefix = {arXiv},
arxivId = {1111.1462v1},
author = {Rosenbaum, David and Harrow, Aram W.},
eprint = {1111.1462v1},
file = {:home/chiroptera/Dropbox/mendeley/Rosenbaum, Harrow - 2011 - Uselessness for an Oracle Model with Internal Randomness.pdf:pdf},
issn = {15337146},
journal = {arXiv:1111.1462},
pages = {1--23},
title = {{Uselessness for an Oracle Model with Internal Randomness}},
year = {2011}
}
@article{Dong2010,
abstract = {Building the quantum clustering model by quantum characteristic. It is proved by the Simulation experiment, that It can deal with exceptional, high-dimension complicated data and large-scale data set.},
author = {Dong, Yumin and Jia, Fanghua},
doi = {10.1109/ISIP.2010.111},
file = {:home/chiroptera/Dropbox/mendeley/Dong, Jia - 2010 - A new-style generalized quantum clustering model.pdf:pdf},
isbn = {9780769542614},
journal = {Proceedings - 3rd International Symposium on Information Processing, ISIP 2010},
keywords = {Clustering arithmetic,Quantum clustering model,Quantum entropy},
pages = {519--522},
title = {{A new-style generalized quantum clustering model}},
year = {2010}
}
@article{Varshavsky2007a,
abstract = {Motivation: Feature selection methods aim to reduce the complexity of data and to uncover the most relevant biological variables. In reality, information in biological datasets is often incomplete as a result of untrustworthy samples and missing values. The reliability of selection methods may therefore be questioned.  Method: Information loss is incorporated into a perturbation scheme, testing which features are stable under it. This method is applied to data analysis by unsupervised feature filtering (UFF). The latter has been shown to be a very successful method in analysis of gene-expression data.  Results: We find that the UFF quality degrades smoothly with information loss. It remains successful even under substantial damage. Our method allows for selection of a best imputation method on a dataset treated by UFF. More importantly, scoring features according to their stability under information loss is shown to be correlated with biological importance in cancer studies. This scoring may lead to novel biological insights.  Contact: royke@cs.huji.ac.il  Supplementary information and code availability: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btm528},
author = {Varshavsky, Roy and Gottlieb, Assaf and Horn, David and Linial, Michal},
doi = {10.1093/bioinformatics/btm528},
file = {:home/chiroptera/Dropbox/mendeley/Varshavsky et al. - 2007 - Unsupervised feature selection under perturbations Meeting the challenges of biological data.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {24},
pages = {3343--3349},
title = {{Unsupervised feature selection under perturbations: Meeting the challenges of biological data}},
volume = {23},
year = {2007}
}
@article{Hung2013,
author = {Hung, Chih-cheng and Casper, Ellis and Kuo, Bor-chen and Liu, Wenping and Yu, Xiaoyi and Jung, Edward and Yang, Ming},
file = {:home/chiroptera/Dropbox/mendeley/Hung et al. - 2013 - A QUANTUM-MODELED FUZZY C-MEANS CLUSTERING ALGORITHM FOR REMOTELY SENSED MULTI-BAND IMAGE SEGMENTATION.pdf:pdf},
isbn = {9781479911141},
pages = {2501--2504},
title = {{A QUANTUM-MODELED FUZZY C-MEANS CLUSTERING ALGORITHM FOR REMOTELY SENSED MULTI-BAND IMAGE SEGMENTATION}},
year = {2013}
}
@article{Owens2006,
author = {Owens, John D and Luebke, David and Govindraju, Naga and Harris, Mark and Kruger, Jens and Lefohn, Aaron E and Purcell, Timothy J},
file = {:home/chiroptera/Dropbox/mendeley/Owens et al. - 2006 - A Survey of General Purpose Computation on Graphics Hardware.pdf:pdf},
journal = {Computer Graphics Forum},
number = {August},
pages = {21--51},
title = {{A Survey of General Purpose Computation on Graphics Hardware}},
url = {http://www.cs.virginia.edu/papers/ASurveyofGeneralPurposeComputationonGraphicsHardware.pdf},
year = {2006}
}
@article{Joshi2003,
abstract = {Clustering large data sets can be time consuming and processor intensive. This project is an implementation of the parallel version of a popular clustering algorithm, the k-means algorithm, to provide faster clustering solutions. This algorithm was tested such that 3,4,5,7 clusters were created on a cluster of Sun workstations. Optimal levels of speedup were not achieved; but the benefits of parallelization were observed. This methodology exploits the inherent data- parallelism in the k-means algorithm and makes use of the message-passing model.},
annote = {Results only on very small datasets, only accuracy results and no timings. Speedup purely theoretical.},
author = {Joshi, Manasi N},
file = {:home/chiroptera/Dropbox/mendeley/Joshi - 2003 - Parallel K - Means Algorithm on Distributed Memory Multiprocessors.pdf:pdf},
journal = {Cities},
pages = {12},
title = {{Parallel K - Means Algorithm on Distributed Memory Multiprocessors}},
year = {2003}
}
@article{Fang2011,
abstract = {This paper presents a comprehensive performance comparison between CUDA and OpenCL. We have selected 16 benchmarks ranging from synthetic applications to real-world ones. We make an extensive analysis of the performance gaps taking into account programming models, ptimization strategies, architectural details, and underlying compilers. Our results show that, for most applications, CUDA performs at most 30$\backslash$\&\#x025; better than OpenCL. We also show that this difference is due to unfair comparisons: in fact, OpenCL can achieve similar performance to CUDA under a fair comparison. Therefore, we define a fair comparison of the two types of applications, providing guidelines for more potential analyses. We also investigate OpenCL's portability by running the benchmarks on other prevailing platforms with minor modifications. Overall, we conclude that OpenCL's portability does not fundamentally affect its performance, and OpenCL can be a good alternative to CUDA.},
archivePrefix = {arXiv},
arxivId = {1005.2581},
author = {Fang, Jianbin and Varbanescu, Ana Lucia and Sips, Henk},
doi = {10.1109/ICPP.2011.45},
eprint = {1005.2581},
file = {:home/chiroptera/Dropbox/mendeley/Fang, Varbanescu, Sips - 2011 - A comprehensive performance comparison of CUDA and OpenCL.pdf:pdf},
isbn = {9780769545103},
issn = {01903918},
journal = {Proceedings of the International Conference on Parallel Processing},
keywords = {CUDA,OpenCL,Performance Comparison},
pages = {216--225},
title = {{A comprehensive performance comparison of CUDA and OpenCL}},
year = {2011}
}
@article{Mechanics,
author = {Mechanics, Quantum},
file = {:home/chiroptera/Dropbox/mendeley/Mechanics - Unknown - Strange Bedfellows.pdf:pdf},
title = {{Strange Bedfellows:}}
}
@article{Wang2002,
abstract = {Clustering is the process of grouping a set of objects into classes of similar objects. Although definitions of similarity vary from one clustering model to another, in most of these models the concept of similarity is based on distances, e.g., Euclidean distance or cosine distance. In other words, similar objects are required to have close values on at least a set of dimensions. In this paper, we explore a more general type of similarity. Under the pCluster model we proposed, two objects are similar if they exhibit a coherent pattern on a subset of dimensions. For instance, in DNA microarray analysis, the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very much alike. Discovery of such clusters of genes is essential in revealing significant connections in gene regulatory networks. E-commerce applications, such as collaborative filtering, can also benefit from the new model, which captures not only the closeness of values of certain leading indicators but also the closeness of (purchasing, browsing, etc.) patterns exhibited by the customers. Our paper introduces an effective algorithm to detect such clusters, and we perform tests on several real and synthetic data sets to show its effectiveness.},
author = {Wang, H and Wang, H and Wang, W and Wang, W and Yang, H and Yang, H and Yu, P S and Yu, P S},
doi = {10.1145/564691.564737},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2002 - Clustering by pattern similarity in large data sets.pdf:pdf},
isbn = {1581134975},
issn = {07308078},
journal = {2002 ACM SIGMOD international conference on Management of Data},
pages = {394},
title = {{Clustering by pattern similarity in large data sets}},
url = {http://portal.acm.org/citation.cfm?doid=564691.564737},
volume = {2},
year = {2002}
}
@article{Mechanicsa,
author = {Mechanics, Quantum},
file = {:home/chiroptera/Dropbox/mendeley/Mechanics - Unknown - Strange Bedfellows.pdf:pdf},
title = {{Strange Bedfellows:}}
}
@inproceedings{DiBuccio2011,
abstract = {Dynamic Quantum Clustering is a recent clustering technique which makes use of Parzen window estimator to construct a potential function whose minima are related to the clusters to be found. The dynamic of the system is computed by means of the Schr\"{o}dinger differential equation. In this paper, we apply this technique in the context of Information Retrieval to explore its performance in terms of the quality of clusters and the efficiency of the computation. In particular, we want to analyze the clusters produced by using datasets of relevant and non-relevant documents given a topic. © 2011 Springer-Verlag.},
author = {{Di Buccio}, Emanuele and {Di Nunzio}, Giorgio Maria},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {360--363},
title = {{Distilling relevant documents by means of dynamic quantum clustering}},
volume = {6931 LNCS},
year = {2011}
}
@article{Dianxun2006,
abstract = {A novel generalized quantum particle model (GQPM) is presented for data self-organizing clustering. Using GQPM we transform the data clustering into a stochastic process of equivalence classes of particles under the quantum entanglement relation. The GQPM approach has much faster clustering speed and higher clustering quality than the nonquantum particle model GPM and GCA we proposed before. GQPM is also characterized by the self-organizing clustering and has advantages in terms of the insensitivity to noise, the quality robustness to clustered data, the learning ability, the suitability for high-dimensional multi-shape large-scale data sets. The simulations and comparisons have shown the effectiveness and good performance of the proposed GQPM approach to data clustering},
author = {Dianxun, Shuai and Zhang, Ping and Huang, Liangjun},
doi = {10.1109/ISIE.2006.296087},
file = {:home/chiroptera/Dropbox/mendeley/Dianxun, Zhang, Huang - 2006 - Self-organizing data clustering A novel quantum particle approach.pdf:pdf},
isbn = {1424404975},
journal = {IEEE International Symposium on Industrial Electronics},
number = {2},
pages = {2960--2965},
title = {{Self-organizing data clustering: A novel quantum particle approach}},
volume = {4},
year = {2006}
}
@article{Klipfel1943,
author = {Klipfel, Joel},
file = {:home/chiroptera/Dropbox/mendeley/Klipfel - 1943 - A brief introduction to hilbert space and quantum logic.pdf:pdf},
pages = {1--31},
title = {{A brief introduction to hilbert space and quantum logic}},
year = {1943}
}
@article{Topics2010,
author = {Topics, Advanced and Topic, Machine Learning and Scribe, Dimensionality Reduction and Lecturer, Matt Faulkner and Date, Andreas Krause},
file = {:home/chiroptera/Dropbox/mendeley/Topics et al. - 2010 - Dimensionality reduction.pdf:pdf},
pages = {1--6},
title = {{Dimensionality reduction}},
year = {2010}
}
@book{Moler2008,
abstract = {This chapter is about eigenvalues and singular values of matrices. Computational algorithms and sensitivity to perturbations are both discussed.},
author = {Moler, Cleve},
booktitle = {Numerical Computing with MATLAB, Revised Reprint},
doi = {10.1016/0377-0427(90)90025-U},
file = {:home/chiroptera/Dropbox/mendeley/Moler - 2008 - Eigenvalues and Singular Values.pdf:pdf},
isbn = {978-0-898716-60-3},
issn = {1550-2376},
pages = {39},
pmid = {21230651},
title = {{Eigenvalues and Singular Values}},
url = {http://www.mathworks.nl/moler/chapters.html},
year = {2008}
}
@article{Nvidia2010,
author = {Nvidia},
file = {:home/chiroptera/Dropbox/mendeley/Nvidia - 2010 - Introduction to CUDA C.pdf:pdf},
journal = {Siggraph Asia 2010},
title = {{Introduction to CUDA C}},
year = {2010}
}
@article{Su2012,
abstract = {GPU (Graphics Processing Unit) has a great impact on computing field. To enhance the performance of computing systems, researchers and developers use the parallel computing architecture of GPU. On the other hand, to reduce the development time of new products, two programming models are included in GPU, which are OpenCL (Open Computing Language) and CUDA (Compute Unified Device Architecture). The benefit of involving the two programming models in GPU is that researchers and developers don't have to understand OpenGL, DirectX or other program design, but can use GPU through simple programming language. OpenCL is an open standard API, which has the advantage of cross-platform. CUDA is a parallel computer architecture developed by NVIDIA, which includes Runtime API and Driver API. Compared with OpenCL, CUDA is with better performance. In this paper, we used plenty of similar kernels to compare the computing performance of C, OpenCL and CUDA, the two kinds of API's on NVIDIA Quadro 4000 GPU. The experimental result showed that, the executive time of CUDA Driver API was 94.9\%\~{}99.0\% faster than that of C, while and the executive time of CUDA Driver API was 3.8\%\~{}5.4\% faster than that of OpenCL. Accordingly, the cross-platform characteristic of OpenCL did not affect the performance of GPU.},
author = {Su, Ching Lung and Chen, Po Yu and Lan, Chun Chieh and Huang, Long Sheng and Wu, Kuo Hsuan},
doi = {10.1109/APCCAS.2012.6419068},
file = {:home/chiroptera/Dropbox/mendeley/Su et al. - 2012 - Overview and comparison of OpenCL and CUDA technology for GPGPU.pdf:pdf},
isbn = {9781457717291},
journal = {IEEE Asia-Pacific Conference on Circuits and Systems, Proceedings, APCCAS},
pages = {448--451},
title = {{Overview and comparison of OpenCL and CUDA technology for GPGPU}},
year = {2012}
}
@article{Han2011,
abstract = {Graphics Processing Units (GPUs) have become a competitive accelerator for applications outside the graphics domain, mainly driven by the improvements inGPUprogrammability. Although the Compute Unified Device Architecture (CUDA) is a simple C-like interface for programming NVIDIA GPUs, porting applications to CUDA remains a challenge to average programmers. In particular, CUDA places on the programmer the burden of packaging GPU code in separate functions, of explicitly managing data transfer between the host and GPU memories, and of manually optimizing the utilization of the GPU memory. Practical experience shows that the programmer needs to make significant code changes, often tedious and error-prone, before getting an optimized program. We have designed hiCUDA, a high-level directive-based language for CUDA programming. It allows programmers to perform these tedious tasks in a simpler manner and directly to the sequential code, thus speeding up the porting process. In this paper, we describe the hiCUDA directives as well as the design and implementation of a prototype compiler that translates a hiCUDA program to a CUDA program. Our compiler is able to support real-world applications that span multiple procedures and use dynamically allocated arrays. Experiments using nine CUDA benchmarks show that the simplicity hiCUDA provides comes at no expense to performance.},
author = {Han, Tianyi David and Abdelrahman, Tarek S.},
doi = {10.1109/TPDS.2010.62},
file = {:home/chiroptera/Dropbox/mendeley/Han, Abdelrahman - 2011 - HiCUDA High-level GPGPU programming.pdf:pdf},
isbn = {1045-9219 VO - 22},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {CUDA,GPGPU,data-parallel programming,directive-based language,source-to-source compiler},
number = {1},
pages = {78--90},
title = {{HiCUDA: High-level GPGPU programming}},
volume = {23},
year = {2011}
}
@article{Zentall2014,
abstract = {Learning by rats was facilitated when response-relevant cues were provided by other rats; learning increased as a fzlnction of number of cues provided. These results suggest that rats can learn by imitation. Learning by rats that observed conspeclifics not esnitting response-relevant cues was retarded compared to learning by rats that did not observe conspecifics. This indicates that a conspecific's presence can also inhibit learning, a result consistent with socia facilitation theory.},
author = {Zentall, Thomas R. and Levine, John M.},
file = {:home/chiroptera/Dropbox/mendeley/Zentall, Levine - 2014 - American Association for the Advancement of Science.pdf:pdf},
journal = {Science},
number = {5278},
pages = {1220--1221},
title = {{American Association for the Advancement of Science}},
volume = {178},
year = {2014}
}
@article{Wang2013,
author = {Wang, Huaixiao and Liu, Jianyong and Zhi, Jun and Fu, Chengqun},
file = {:home/chiroptera/Dropbox/mendeley/Wang et al. - 2013 - The Improvement of Quantum Genetic Algorithm and Its Application on Function Optimization.pdf:pdf},
number = {1},
title = {{The Improvement of Quantum Genetic Algorithm and Its Application on Function Optimization}},
volume = {2013},
year = {2013}
}
@article{Horn2004,
abstract = {We describe results of a novel algorithm for grammar induction from a large corpus. The ADIOS (Automatic DIstillation of Structure) algorithm searches for significant patterns, chosen according to context dependent statistical criteria, and builds a hierarchy of such patterns according to a set of rules leading to structured generalization. The corpus is thus generalized into a context free grammar (CFG), composed of patterns, equivalence classes and words of the initial lexicon. We have evaluated our method both on corpora generated by CFG and on natural language ones. The performance of ADIOS is judged by searching for both good recall (acceptance of correct novel sentences) and good precision (production of correct novel sentences). The results are very encouraging.},
author = {Horn, David and Solan, Zach and Ruppin, Eytan and Edelman, Shimon},
file = {:home/chiroptera/Dropbox/mendeley/Horn et al. - 2004 - Unsupervised language acquisition syntax from plain corpus.pdf:pdf},
journal = {\ldots on Human Language},
title = {{Unsupervised language acquisition: syntax from plain corpus}},
url = {http://horn.tau.ac.il/~horn/publications/newcastle.pdf},
year = {2004}
}
